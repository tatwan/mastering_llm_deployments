{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Local Model Serving with FastAPI\n",
                "\n",
                "**Module 04 | Notebook 1 of 4**\n",
                "\n",
                "Learn to create production-ready REST APIs for your ML models using FastAPI.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Create a FastAPI application for model serving\n",
                "2. Implement prediction endpoints\n",
                "3. Handle input validation with Pydantic\n",
                "4. Test your API locally\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch fastapi uvicorn pydantic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Why FastAPI?\n",
                "\n",
                "### REST API Serving Pattern\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     HTTP Request      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ   FastAPI   ‚îÇ\n",
                "‚îÇ  (Browser,  ‚îÇ     {\"text\": \"...\"}   ‚îÇ   Server    ‚îÇ\n",
                "‚îÇ   Mobile)   ‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ\n",
                "       ‚Üë                                     ‚ñº\n",
                "       ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "       ‚îÇ      HTTP Response           ‚îÇ    Model    ‚îÇ\n",
                "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  Inference  ‚îÇ\n",
                "             {\"label\": \"POSITIVE\"}    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "### FastAPI Advantages\n",
                "\n",
                "| Feature | Benefit |\n",
                "|---------|--------|\n",
                "| **Automatic docs** | Swagger UI out of the box |\n",
                "| **Type hints** | Automatic validation |\n",
                "| **Async support** | High concurrency |\n",
                "| **Fast** | One of the fastest Python frameworks |\n",
                "| **Modern** | Native Python 3.6+ features |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded: distilbert-base-uncased-finetuned-sst-2-english\n",
                        "Labels: {0: 'NEGATIVE', 1: 'POSITIVE'}\n"
                    ]
                }
            ],
            "source": [
                "# Load model and tokenizer\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
                "model.eval()\n",
                "\n",
                "print(f\"Model loaded: {model_name}\")\n",
                "print(f\"Labels: {model.config.id2label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test prediction: {'label': 'POSITIVE', 'confidence': 0.9998781681060791, 'probabilities': {'NEGATIVE': 0.00012178818724351004, 'POSITIVE': 0.9998781681060791}}\n"
                    ]
                }
            ],
            "source": [
                "# Test prediction function\n",
                "def predict(text: str) -> dict:\n",
                "    \"\"\"Run inference on input text.\"\"\"\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors=\"pt\",\n",
                "        truncation=True,\n",
                "        max_length=512\n",
                "    ).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    \n",
                "    probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                "    pred_idx = probs.argmax().item()\n",
                "    \n",
                "    return {\n",
                "        \"label\": model.config.id2label[pred_idx],\n",
                "        \"confidence\": probs[pred_idx].item(),\n",
                "        \"probabilities\": {\n",
                "            model.config.id2label[i]: probs[i].item() \n",
                "            for i in range(len(probs))\n",
                "        }\n",
                "    }\n",
                "\n",
                "# Test\n",
                "result = predict(\"This movie was fantastic!\")\n",
                "print(f\"Test prediction: {result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Create the FastAPI Application\n",
                "\n",
                "Here's the complete FastAPI application code. In a production setting, you would save this to a file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "FastAPI Application Code:\n",
                        "============================================================\n",
                        "\n",
                        "from fastapi import FastAPI, HTTPException\n",
                        "from pydantic import BaseModel, Field\n",
                        "from typing import Dict, List, Optional\n",
                        "import torch\n",
                        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                        "\n",
                        "# Initialize app\n",
                        "app = FastAPI(\n",
                        "    title=\"Sentiment Analysis API\",\n",
                        "    description=\"A REST API for sentiment classification using DistilBERT\",\n",
                        "    version=\"1.0.0\"\n",
                        ")\n",
                        "\n",
                        "# Load model at startup\n",
                        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "model = model.to(device)\n",
                        "model.eval()\n",
                        "\n",
                        "# Request/Response schemas\n",
                        "class PredictionRequest(BaseModel):\n",
                        "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to classify\")\n",
                        "    \n",
                        "    class Config:\n",
                        "        json_schema_extra = {\n",
                        "            \"example\": {\"text\": \"This movie was absolutely fantastic!\"}\n",
                        "        }\n",
                        "\n",
                        "class PredictionResponse(BaseModel):\n",
                        "    label: str\n",
                        "    confidence: float\n",
                        "    probabilities: Dict[str, float]\n",
                        "\n",
                        "class BatchRequest(BaseModel):\n",
                        "    texts: List[str] = Field(..., max_length=100)\n",
                        "\n",
                        "class HealthResponse(BaseModel):\n",
                        "    status: str\n",
                        "    model: str\n",
                        "    device: str\n",
                        "\n",
                        "# Endpoints\n",
                        "@app.get(\"/health\", response_model=HealthResponse)\n",
                        "def health_check():\n",
                        "    \"\"\"Check if the API is running and model is loaded.\"\"\"\n",
                        "    return {\n",
                        "        \"status\": \"healthy\",\n",
                        "        \"model\": model_name,\n",
                        "        \"device\": str(device)\n",
                        "    }\n",
                        "\n",
                        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
                        "def predict_sentiment(request: PredictionRequest):\n",
                        "    \"\"\"Predict sentiment for a single text.\"\"\"\n",
                        "    try:\n",
                        "        inputs = tokenizer(\n",
                        "            request.text,\n",
                        "            return_tensors=\"pt\",\n",
                        "            truncation=True,\n",
                        "            max_length=512\n",
                        "        ).to(device)\n",
                        "        \n",
                        "        with torch.no_grad():\n",
                        "            outputs = model(**inputs)\n",
                        "        \n",
                        "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                        "        pred_idx = probs.argmax().item()\n",
                        "        \n",
                        "        return {\n",
                        "            \"label\": model.config.id2label[pred_idx],\n",
                        "            \"confidence\": probs[pred_idx].item(),\n",
                        "            \"probabilities\": {\n",
                        "                model.config.id2label[i]: probs[i].item() \n",
                        "                for i in range(len(probs))\n",
                        "            }\n",
                        "        }\n",
                        "    except Exception as e:\n",
                        "        raise HTTPException(status_code=500, detail=str(e))\n",
                        "\n",
                        "@app.post(\"/predict/batch\", response_model=List[PredictionResponse])\n",
                        "def predict_batch(request: BatchRequest):\n",
                        "    \"\"\"Predict sentiment for multiple texts.\"\"\"\n",
                        "    results = []\n",
                        "    for text in request.texts:\n",
                        "        req = PredictionRequest(text=text)\n",
                        "        results.append(predict_sentiment(req))\n",
                        "    return results\n",
                        "\n",
                        "if __name__ == \"__main__\":\n",
                        "    import uvicorn\n",
                        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# FastAPI application code\n",
                "app_code = '''\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel, Field\n",
                "from typing import Dict, List, Optional\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "\n",
                "# Initialize app\n",
                "app = FastAPI(\n",
                "    title=\"Sentiment Analysis API\",\n",
                "    description=\"A REST API for sentiment classification using DistilBERT\",\n",
                "    version=\"1.0.0\"\n",
                ")\n",
                "\n",
                "# Load model at startup\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)\n",
                "model.eval()\n",
                "\n",
                "# Request/Response schemas\n",
                "class PredictionRequest(BaseModel):\n",
                "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to classify\")\n",
                "    \n",
                "    class Config:\n",
                "        json_schema_extra = {\n",
                "            \"example\": {\"text\": \"This movie was absolutely fantastic!\"}\n",
                "        }\n",
                "\n",
                "class PredictionResponse(BaseModel):\n",
                "    label: str\n",
                "    confidence: float\n",
                "    probabilities: Dict[str, float]\n",
                "\n",
                "class BatchRequest(BaseModel):\n",
                "    texts: List[str] = Field(..., max_length=100)\n",
                "\n",
                "class HealthResponse(BaseModel):\n",
                "    status: str\n",
                "    model: str\n",
                "    device: str\n",
                "\n",
                "# Endpoints\n",
                "@app.get(\"/health\", response_model=HealthResponse)\n",
                "def health_check():\n",
                "    \"\"\"Check if the API is running and model is loaded.\"\"\"\n",
                "    return {\n",
                "        \"status\": \"healthy\",\n",
                "        \"model\": model_name,\n",
                "        \"device\": str(device)\n",
                "    }\n",
                "\n",
                "@app.post(\"/predict\", response_model=PredictionResponse)\n",
                "def predict_sentiment(request: PredictionRequest):\n",
                "    \"\"\"Predict sentiment for a single text.\"\"\"\n",
                "    try:\n",
                "        inputs = tokenizer(\n",
                "            request.text,\n",
                "            return_tensors=\"pt\",\n",
                "            truncation=True,\n",
                "            max_length=512\n",
                "        ).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "        \n",
                "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                "        pred_idx = probs.argmax().item()\n",
                "        \n",
                "        return {\n",
                "            \"label\": model.config.id2label[pred_idx],\n",
                "            \"confidence\": probs[pred_idx].item(),\n",
                "            \"probabilities\": {\n",
                "                model.config.id2label[i]: probs[i].item() \n",
                "                for i in range(len(probs))\n",
                "            }\n",
                "        }\n",
                "    except Exception as e:\n",
                "        raise HTTPException(status_code=500, detail=str(e))\n",
                "\n",
                "@app.post(\"/predict/batch\", response_model=List[PredictionResponse])\n",
                "def predict_batch(request: BatchRequest):\n",
                "    \"\"\"Predict sentiment for multiple texts.\"\"\"\n",
                "    results = []\n",
                "    for text in request.texts:\n",
                "        req = PredictionRequest(text=text)\n",
                "        results.append(predict_sentiment(req))\n",
                "    return results\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    import uvicorn\n",
                "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
                "'''\n",
                "\n",
                "# Display the code\n",
                "print(\"FastAPI Application Code:\")\n",
                "print(\"=\" * 60)\n",
                "print(app_code)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Application saved to app.py\n",
                        "\n",
                        "To run the server:\n",
                        "  python app.py\n",
                        "  OR\n",
                        "  uvicorn app:app --reload --host 0.0.0.0 --port 8000\n"
                    ]
                }
            ],
            "source": [
                "# Save to file\n",
                "with open(\"./app.py\", \"w\") as f:\n",
                "    f.write(app_code)\n",
                "\n",
                "print(\"‚úÖ Application saved to app.py\")\n",
                "print(\"\\nTo run the server:\")\n",
                "print(\"  python app.py\")\n",
                "print(\"  OR\")\n",
                "print(\"  uvicorn app:app --reload --host 0.0.0.0 --port 8000\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !python app.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding the Application\n",
                "\n",
                "### Request/Response Models (Pydantic)\n",
                "\n",
                "```python\n",
                "class PredictionRequest(BaseModel):\n",
                "    text: str = Field(..., min_length=1, max_length=5000)\n",
                "```\n",
                "\n",
                "This provides:\n",
                "- **Automatic validation** (text must be 1-5000 characters)\n",
                "- **Documentation** (shown in Swagger UI)\n",
                "- **Type hints** for IDE support\n",
                "\n",
                "### Endpoints\n",
                "\n",
                "| Endpoint | Method | Description |\n",
                "|----------|--------|-------------|\n",
                "| `/health` | GET | Check API status |\n",
                "| `/predict` | POST | Single text prediction |\n",
                "| `/predict/batch` | POST | Batch predictions |\n",
                "| `/docs` | GET | Swagger UI (automatic) |\n",
                "| `/redoc` | GET | ReDoc UI (automatic) |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Testing the API\n",
                "\n",
                "Once the server is running, you can test it using `requests`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Client Test Code:\n",
                        "============================================================\n",
                        "\n",
                        "import requests\n",
                        "\n",
                        "BASE_URL = \"http://localhost:8000\"\n",
                        "\n",
                        "# Health check\n",
                        "response = requests.get(f\"{BASE_URL}/health\")\n",
                        "print(\"Health:\", response.json())\n",
                        "\n",
                        "# Single prediction\n",
                        "response = requests.post(\n",
                        "    f\"{BASE_URL}/predict\",\n",
                        "    json={\"text\": \"This movie was fantastic!\"}\n",
                        ")\n",
                        "print(\"Prediction:\", response.json())\n",
                        "\n",
                        "# Batch prediction\n",
                        "response = requests.post(\n",
                        "    f\"{BASE_URL}/predict/batch\",\n",
                        "    json={\n",
                        "        \"texts\": [\n",
                        "            \"I love this product!\",\n",
                        "            \"Terrible experience, never again.\",\n",
                        "            \"It was okay.\"\n",
                        "        ]\n",
                        "    }\n",
                        ")\n",
                        "print(\"Batch:\", response.json())\n",
                        "\n",
                        "\n",
                        "‚úÖ Client code saved to test_client.py\n"
                    ]
                }
            ],
            "source": [
                "# Example client code (run when server is active)\n",
                "client_code = '''\n",
                "import requests\n",
                "\n",
                "BASE_URL = \"http://localhost:8000\"\n",
                "\n",
                "# Health check\n",
                "response = requests.get(f\"{BASE_URL}/health\")\n",
                "print(\"Health:\", response.json())\n",
                "\n",
                "# Single prediction\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict\",\n",
                "    json={\"text\": \"This movie was fantastic!\"}\n",
                ")\n",
                "print(\"Prediction:\", response.json())\n",
                "\n",
                "# Batch prediction\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict/batch\",\n",
                "    json={\n",
                "        \"texts\": [\n",
                "            \"I love this product!\",\n",
                "            \"Terrible experience, never again.\",\n",
                "            \"It was okay.\"\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "print(\"Batch:\", response.json())\n",
                "'''\n",
                "\n",
                "print(\"Client Test Code:\")\n",
                "print(\"=\" * 60)\n",
                "print(client_code)\n",
                "\n",
                "# Save client code\n",
                "with open(\"./test_client.py\", \"w\") as f:\n",
                "    f.write(client_code)\n",
                "print(\"\\n‚úÖ Client code saved to test_client.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Running the Server\n",
                "\n",
                "### ‚ö†Ô∏è Environment-Specific Instructions\n",
                "\n",
                "The way you run the FastAPI server depends on your environment:\n",
                "\n",
                "| Environment | How to Run Server | How to Test |\n",
                "|-------------|-------------------|-------------|\n",
                "| **Colab/Jupyter** | Background subprocess (see below) | Run code in next cell |\n",
                "| **Local (2 terminals)** | `python app.py` | `python test_client.py` |\n",
                "| **Production** | `uvicorn app:app --host 0.0.0.0 --port 8000` | HTTP client or curl |\n",
                "\n",
                "---\n",
                "\n",
                "### Running in Google Colab (Notebook Environment)\n",
                "\n",
                "In Colab, we can't open a second terminal, so we start the server as a **background process** using `subprocess`. This allows the notebook to continue executing while the server runs in the background.\n",
                "\n",
                "> **Note:** This is a workaround for learning/demo purposes. In production, you would run the server as a standalone process or container."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Server started!\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# START SERVER IN BACKGROUND (Colab/Jupyter Only)\n",
                "# ============================================================\n",
                "# In a notebook environment, we can't run the server in a separate \n",
                "# terminal. Instead, we use subprocess to run it in the background.\n",
                "#\n",
                "# PRODUCTION ALTERNATIVE:\n",
                "# - Open a terminal and run: python app.py\n",
                "# - Or use: uvicorn app:app --reload --host 0.0.0.0 --port 8000\n",
                "# - Or deploy with Docker/Kubernetes (see Module 04 notebooks)\n",
                "# ============================================================\n",
                "\n",
                "import subprocess\n",
                "import time\n",
                "\n",
                "# Start the FastAPI server as a background process\n",
                "print(\"Starting FastAPI server in background...\")\n",
                "server_process = subprocess.Popen(\n",
                "    [\"python\", \"app.py\"],\n",
                "    stdout=subprocess.PIPE,\n",
                "    stderr=subprocess.PIPE\n",
                ")\n",
                "\n",
                "# Wait for the server to start and model to load\n",
                "# This takes ~10-20 seconds due to model initialization\n",
                "print(\"Waiting for model to load (this may take 15-20 seconds)...\")\n",
                "time.sleep(20)\n",
                "\n",
                "print(\"‚úÖ Server should be running on http://localhost:8000\")\n",
                "print(\"   - API docs: http://localhost:8000/docs\")\n",
                "print(\"   - Health check: http://localhost:8000/health\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Testing the API\n",
                "\n",
                "Now that the server is running in the background, we can send HTTP requests to it directly from this notebook.\n",
                "\n",
                "> **In Production:** You would typically test using:\n",
                "> - `curl` commands from terminal\n",
                "> - A separate test script (`python test_client.py`)\n",
                "> - Automated tests with `pytest` and `httpx`\n",
                "> - API testing tools like Postman or Insomnia"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Health: {'status': 'healthy', 'model': 'distilbert-base-uncased-finetuned-sst-2-english', 'device': 'cuda'}\n",
                        "Prediction: {'label': 'POSITIVE', 'confidence': 0.9998781681060791, 'probabilities': {'NEGATIVE': 0.00012178818724351004, 'POSITIVE': 0.9998781681060791}}\n"
                    ]
                }
            ],
            "source": [
                "import requests\n",
                "\n",
                "BASE_URL = \"http://localhost:8000\"\n",
                "\n",
                "# Health check\n",
                "response = requests.get(f\"{BASE_URL}/health\")\n",
                "print(\"Health:\", response.json())\n",
                "\n",
                "# Prediction\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict\",\n",
                "    json={\"text\": \"This movie was fantastic!\"}\n",
                ")\n",
                "print(\"Prediction:\", response.json())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Health: {'status': 'healthy', 'model': 'distilbert-base-uncased-finetuned-sst-2-english', 'device': 'cuda'}\n",
                        "Prediction: {'label': 'POSITIVE', 'confidence': 0.9998781681060791, 'probabilities': {'NEGATIVE': 0.00012178818724351004, 'POSITIVE': 0.9998781681060791}}\n",
                        "Batch: [{'label': 'POSITIVE', 'confidence': 0.9998855590820312, 'probabilities': {'NEGATIVE': 0.00011442836694186553, 'POSITIVE': 0.9998855590820312}}, {'label': 'NEGATIVE', 'confidence': 0.9902605414390564, 'probabilities': {'NEGATIVE': 0.9902605414390564, 'POSITIVE': 0.009739442728459835}}, {'label': 'POSITIVE', 'confidence': 0.9998270869255066, 'probabilities': {'NEGATIVE': 0.00017293228302150965, 'POSITIVE': 0.9998270869255066}}]\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# TEST THE API\n",
                "# ============================================================\n",
                "# This code sends requests to our running FastAPI server.\n",
                "# In production, this would be in a separate test_client.py file.\n",
                "# ============================================================\n",
                "\n",
                "import requests\n",
                "\n",
                "BASE_URL = \"http://localhost:8000\"\n",
                "\n",
                "# Health check\n",
                "print(\"1. Health Check:\")\n",
                "print(\"-\" * 40)\n",
                "response = requests.get(f\"{BASE_URL}/health\")\n",
                "print(response.json())\n",
                "\n",
                "# Single prediction\n",
                "print(\"\\n2. Single Prediction:\")\n",
                "print(\"-\" * 40)\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict\",\n",
                "    json={\"text\": \"This movie was fantastic!\"}\n",
                ")\n",
                "print(response.json())\n",
                "\n",
                "# Batch prediction\n",
                "print(\"\\n3. Batch Prediction:\")\n",
                "print(\"-\" * 40)\n",
                "response = requests.post(\n",
                "    f\"{BASE_URL}/predict/batch\",\n",
                "    json={\n",
                "        \"texts\": [\n",
                "            \"I love this product!\",\n",
                "            \"Terrible experience, never again.\",\n",
                "            \"It was okay.\"\n",
                "        ]\n",
                "    }\n",
                ")\n",
                "for i, result in enumerate(response.json()):\n",
                "    print(f\"  Text {i+1}: {result['label']} ({result['confidence']:.2%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Production Best Practices\n",
                "\n",
                "### 1. Model Loading\n",
                "- Load model ONCE at startup, not per request\n",
                "- Use `@app.on_event(\"startup\")` for initialization\n",
                "\n",
                "### 2. Error Handling\n",
                "- Use try/except and HTTPException\n",
                "- Return meaningful error messages\n",
                "\n",
                "### 3. Logging\n",
                "```python\n",
                "import logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)\n",
                "```\n",
                "\n",
                "### 4. Rate Limiting\n",
                "```python\n",
                "from fastapi_limiter import FastAPILimiter\n",
                "```\n",
                "\n",
                "### 5. CORS (for web clients)\n",
                "```python\n",
                "...\n",
                "### 7. Health Checks\n",
                "- Include model status, memory usage, GPU utilization\n",
                "- Kubernetes/Docker can use for readiness probes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "### Challenge: Add New Endpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Extend the API with these features:\n",
                "\n",
                "# 1. Add a `/tokenize` endpoint that returns token information\n",
                "#    - Input: {\"text\": \"...\"}\n",
                "#    - Output: {\"tokens\": [...], \"token_ids\": [...], \"num_tokens\": N}\n",
                "\n",
                "# 2. Add model info endpoint `/model/info`\n",
                "#    - Output: {\"name\": \"...\", \"parameters\": N, \"vocab_size\": N}\n",
                "\n",
                "# 3. Add request timing middleware\n",
                "#    - Log request duration for each call\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Key Takeaways\n",
                "\n",
                "1. **FastAPI** provides automatic docs, validation, and async support\n",
                "2. **Pydantic models** define request/response schemas with validation\n",
                "3. **Load models once** at startup for efficiency\n",
                "4. **Health endpoints** are essential for production monitoring\n",
                "5. **Batch endpoints** improve throughput for multiple requests\n",
                "\n",
                "---\n",
                "\n",
                "## ‚û°Ô∏è Next Steps\n",
                "\n",
                "Continue to `02_gradio_ui.ipynb` for interactive web interfaces!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
