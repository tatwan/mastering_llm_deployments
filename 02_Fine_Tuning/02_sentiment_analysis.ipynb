{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentiment Analysis with IMDB\n",
                "\n",
                "**Module 02 | Notebook 2 of 3**\n",
                "\n",
                "In this notebook, we'll build a complete sentiment analysis system using the IMDB movie review dataset.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Handle large text datasets efficiently\n",
                "2. Implement proper train/validation splits\n",
                "3. Use evaluation metrics (accuracy, F1, precision, recall)\n",
                "4. Analyze model performance and errors\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate scikit-learn matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSequenceClassification,\n",
                "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
                ")\n",
                "from datasets import load_dataset\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load and Explore the Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load IMDB dataset\n",
                "dataset = load_dataset(\"imdb\")\n",
                "\n",
                "print(\"Dataset structure:\")\n",
                "print(dataset)\n",
                "print(f\"\\nTrain examples: {len(dataset['train']):,}\")\n",
                "print(f\"Test examples: {len(dataset['test']):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the data\n",
                "print(\"Label distribution:\")\n",
                "train_labels = dataset['train']['label']\n",
                "pos_count = sum(train_labels)\n",
                "neg_count = len(train_labels) - pos_count\n",
                "print(f\"  Positive: {pos_count:,} ({pos_count/len(train_labels):.1%})\")\n",
                "print(f\"  Negative: {neg_count:,} ({neg_count/len(train_labels):.1%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Why check text length?\n",
                "BERT-based models have a maximum token limit (usually 512). If texts are too long, they get truncated, meaning the model might miss the most important part of the review if it's at the end.\n",
                "We need to know how many reviews will be affected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze text lengths\n",
                "text_lengths = [len(text.split()) for text in dataset['train']['text']]\n",
                "\n",
                "print(\"\\nText length statistics (words):\")\n",
                "print(f\"  Min: {min(text_lengths)}\")\n",
                "print(f\"  Max: {max(text_lengths)}\")\n",
                "print(f\"  Mean: {np.mean(text_lengths):.0f}\")\n",
                "print(f\"  Median: {np.median(text_lengths):.0f}\")\n",
                "\n",
                "# Plot distribution\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
                "plt.axvline(x=256, color='r', linestyle='--', label='256 tokens (typical limit)')\n",
                "plt.xlabel('Number of Words')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Distribution of Review Lengths')\n",
                "plt.legend()\n",
                "plt.xlim(0, 1000)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample reviews\n",
                "print(\"Sample Positive Review:\")\n",
                "print(\"-\" * 50)\n",
                "pos_example = dataset['train'].filter(lambda x: x['label'] == 1)[0]\n",
                "print(pos_example['text'][:500] + \"...\")\n",
                "\n",
                "print(\"\\nSample Negative Review:\")\n",
                "print(\"-\" * 50)\n",
                "neg_example = dataset['train'].filter(lambda x: x['label'] == 0)[0]\n",
                "print(neg_example['text'][:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Data Preparation\n",
                "\n",
                "For faster training in this demo, we'll use a subset. In production, use the full dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create subsets for faster training (increase for better results)\n",
                "train_size = 2000\n",
                "test_size = 500\n",
                "\n",
                "# Balanced sampling\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "test_data = dataset['test'].shuffle(seed=42).select(range(test_size))\n",
                "\n",
                "# Create validation split from training data\n",
                "train_val_split = train_data.train_test_split(test_size=0.1, seed=42)\n",
                "train_dataset = train_val_split['train']\n",
                "val_dataset = train_val_split['test']\n",
                "\n",
                "print(f\"Training samples: {len(train_dataset)}\")\n",
                "print(f\"Validation samples: {len(val_dataset)}\")\n",
                "print(f\"Test samples: {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        padding=True,\n",
                "        truncation=True,\n",
                "        max_length=256  # IMDB reviews can be long\n",
                "    )\n",
                "\n",
                "# Tokenize all datasets\n",
                "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
                "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
                "test_tokenized = test_data.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"Tokenization complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    model_name,\n",
                "    num_labels=2,\n",
                "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
                "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
                ")\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> \u26a0\ufe0f **OOM Error?** Reduce `per_device_train_batch_size` to 8 or 4 in the training arguments below."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding Metrics\n",
                "\n",
                "When evaluating a classifier, accuracy isn't enough.\n",
                "\n",
                "*   **Accuracy**: Overall correct % (Good for balanced data).\n",
                "*   **Precision**: \"Quality\" - When it predicts Positive, how often is it right?\n",
                "*   **Recall**: \"Quantity\" - Of all actual Positives, how many did it find?\n",
                "*   **F1 Score**: Harmonic mean of Precision and Recall. Use this if you want a balance.\n",
                "\n",
                "Example: In Spam detection, you want high **Precision** (don't mark real email as spam). In Fraud detection, you want high **Recall** (catch all fraud, even if you flag some safe transactions)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define comprehensive metrics\n",
                "accuracy = evaluate.load(\"accuracy\")\n",
                "f1 = evaluate.load(\"f1\")\n",
                "precision = evaluate.load(\"precision\")\n",
                "recall = evaluate.load(\"recall\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    preds = np.argmax(predictions, axis=1) # argmax converts probabilities to class IDs (0 or 1)\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy.compute(predictions=preds, references=labels)['accuracy'],\n",
                "        'f1': f1.compute(predictions=preds, references=labels)['f1'],\n",
                "        'precision': precision.compute(predictions=preds, references=labels)['precision'],\n",
                "        'recall': recall.compute(predictions=preds, references=labels)['recall']\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./imdb_sentiment_model\",\n",
                "    num_train_epochs=3,              # \ud83d\udcda Full passes through training data\n",
                "    per_device_train_batch_size=16,  # \ud83d\udcbe Lower to 8 or 4 if you get OOM errors\n",
                "    per_device_eval_batch_size=32,\n",
                "    learning_rate=2e-5,              # \ud83c\udfaf Sweet spot for BERT models (try 1e-5 to 5e-5)\n",
                "    weight_decay=0.01,               # \ud83d\udee1\ufe0f Prevents overfitting (L2 regularization)\n",
                "    warmup_ratio=0.1,                # \ud83d\udd25 Gradual LR ramp-up for first 10% of training\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1\",\n",
                "    logging_steps=25,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "# Data collator\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "\n",
                "# Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_tokenized,\n",
                "    eval_dataset=val_tokenized,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udcc8 Is My Model Learning?\n",
                "\n",
                "The training table above shows loss per epoch, but let's visualize it to catch **overfitting** early.\n",
                "\n",
                "**Reading the chart:**\n",
                "- \u2705 **Good**: Both training and validation loss go down together\n",
                "- \u26a0\ufe0f **Overfitting**: Training loss goes down, but validation loss goes UP\n",
                "- \ud83d\udca1 **Early stopping**: Stop training when validation loss stops improving for 2-3 epochs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history to check for overfitting\n",
                "history = trainer.state.log_history\n",
                "\n",
                "# Extract validation losses per epoch\n",
                "eval_losses = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
                "epochs = range(1, len(eval_losses) + 1)\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(epochs, eval_losses, 'b-o', label='Validation Loss', linewidth=2, markersize=8)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Is My Model Learning? (Lower is Better)')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xticks(epochs)\n",
                "plt.show()\n",
                "\n",
                "# Quick overfitting check\n",
                "if len(eval_losses) > 1 and eval_losses[-1] > eval_losses[-2]:\n",
                "    print(\"\u26a0\ufe0f Validation loss increased in last epoch - watch for overfitting!\")\n",
                "else:\n",
                "    print(\"\u2705 Model is learning well!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluation and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "test_results = trainer.evaluate(test_tokenized)\n",
                "\n",
                "print(\"Test Set Results:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Accuracy:  {test_results['eval_accuracy']:.2%}\")\n",
                "print(f\"F1 Score:  {test_results['eval_f1']:.2%}\")\n",
                "print(f\"Precision: {test_results['eval_precision']:.2%}\")\n",
                "print(f\"Recall:    {test_results['eval_recall']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> \ud83d\udcca **Are these scores good?**\n",
                "> - Random guessing = 50%\n",
                "> - Keyword-based baseline = ~70-75%\n",
                "> - Our fine-tuned DistilBERT = ~85-88% \u2713\n",
                "> \n",
                "> **IMDB benchmark**: State-of-the-art models reach ~95%+, but they use the full 25k training set and larger architectures."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> \ud83d\udd27 **Troubleshooting Poor Results:**\n",
                "> - **Accuracy stuck at ~50%?** \u2192 Learning rate too high (try 2e-5) or data issue\n",
                "> - **Training loss not decreasing?** \u2192 Learning rate too low (try 5e-5)\n",
                "> - **Great train accuracy, poor test accuracy?** \u2192 Overfitting! Add more data or reduce epochs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Confusion Matrix\n",
                "The confusion matrix tells us **where** the errors are happening.\n",
                "\n",
                "*   **Top-Left & Bottom-Right**: Correct predictions (Diagonal).\n",
                "*   **Top-Right (False Positive)**: Model said Positive, but it was Negative.\n",
                "*   **Bottom-Left (False Negative)**: Model said Negative, but it was Positive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions for confusion matrix\n",
                "predictions = trainer.predict(test_tokenized)\n",
                "preds = np.argmax(predictions.predictions, axis=1)\n",
                "true_labels = predictions.label_ids\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(true_labels, preds)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Negative', 'Positive'],\n",
                "            yticklabels=['Negative', 'Positive'])\n",
                "plt.ylabel('True Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(true_labels, preds, target_names=['Negative', 'Positive']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Error Analysis\n",
                "\n",
                "Understanding where the model fails helps improve it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find misclassified examples\n",
                "test_texts = test_data['text']\n",
                "misclassified_indices = np.where(preds != true_labels)[0]\n",
                "\n",
                "print(f\"Misclassified: {len(misclassified_indices)} out of {len(true_labels)} ({len(misclassified_indices)/len(true_labels):.1%})\")\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"Sample Misclassified Reviews:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for idx in misclassified_indices[:3]:\n",
                "    idx = int(idx)  # Convert numpy.int64 to Python int\n",
                "    true_label = \"Positive\" if true_labels[idx] == 1 else \"Negative\"\n",
                "    pred_label = \"Positive\" if preds[idx] == 1 else \"Negative\"\n",
                "    \n",
                "    print(f\"\\nTrue: {true_label} | Predicted: {pred_label}\")\n",
                "    print(f\"Text: {test_texts[idx][:300]}...\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Interactive Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Create inference pipeline\n",
                "sentiment_classifier = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "# Test examples\n",
                "test_reviews = [\n",
                "    \"This movie is a masterpiece. The acting, direction, and story are all perfect.\",\n",
                "    \"Absolute garbage. Waste of 2 hours of my life. Avoid at all costs.\",\n",
                "    \"It was okay. Some good moments but overall forgettable.\",\n",
                "    \"Not as good as the original, but still entertaining enough.\"\n",
                "]\n",
                "\n",
                "print(\"Live Predictions:\")\n",
                "print(\"=\" * 60)\n",
                "for review in test_reviews:\n",
                "    result = sentiment_classifier(review)[0]\n",
                "    emoji = \"\ud83d\ude0a\" if result['label'] == \"POSITIVE\" else \"\ud83d\ude20\"\n",
                "    print(f\"{emoji} {result['label']} ({result['score']:.2%})\")\n",
                "    print(f\"   {review[:60]}...\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udfaf Student Challenge\n",
                "\n",
                "### Challenge: Experimentation\n",
                "\n",
                "Machine Learning is experimental science. Don't just guess\u2014test!\n",
                "Run the following code block to test different **Learning Rates** and see how they impact the F1 Score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Experiment Loop\n",
                "learning_rates = [1e-5, 5e-5, 1e-4] # Too small, Good, Too big\n",
                "\n",
                "print(\"Experiment Results:\")\n",
                "print(\"-------------------\")\n",
                "\n",
                "for lr in learning_rates:\n",
                "    print(f\"\\nTraining with LR: {lr}...\")\n",
                "    \n",
                "    # 1. Update Training Args with new LR\n",
                "    # current_args = TrainingArguments(..., learning_rate=lr, ...)\n",
                "    \n",
                "    # 2. Re-init Trainer\n",
                "    # current_trainer = Trainer(..., args=current_args)\n",
                "    \n",
                "    # 3. Train & Evaluate\n",
                "    # current_trainer.train()\n",
                "    # metrics = current_trainer.evaluate()\n",
                "    \n",
                "    # print(f\"  -> F1 Score: {metrics['eval_f1']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Data exploration** is crucial before training\n",
                "2. **Balanced datasets** lead to better model performance\n",
                "3. **Multiple metrics** (accuracy, F1, precision, recall) give complete picture\n",
                "4. **Error analysis** helps identify model weaknesses\n",
                "5. **Confusion matrices** visualize classification errors\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to `03_summarization.ipynb` for sequence-to-sequence tasks!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}