{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentiment Analysis with IMDB\n",
                "\n",
                "**Module 02 | Notebook 2 of 3**\n",
                "\n",
                "In this notebook, we'll build a complete sentiment analysis system using the IMDB movie review dataset.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Handle large text datasets efficiently\n",
                "2. Implement proper train/validation splits\n",
                "3. Use evaluation metrics (accuracy, F1, precision, recall)\n",
                "4. Analyze model performance and errors\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate scikit-learn matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSequenceClassification,\n",
                "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
                ")\n",
                "from datasets import load_dataset\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load and Explore the Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load IMDB dataset\n",
                "dataset = load_dataset(\"imdb\")\n",
                "\n",
                "print(\"Dataset structure:\")\n",
                "print(dataset)\n",
                "print(f\"\\nTrain examples: {len(dataset['train']):,}\")\n",
                "print(f\"Test examples: {len(dataset['test']):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the data\n",
                "print(\"Label distribution:\")\n",
                "train_labels = dataset['train']['label']\n",
                "pos_count = sum(train_labels)\n",
                "neg_count = len(train_labels) - pos_count\n",
                "print(f\"  Positive: {pos_count:,} ({pos_count/len(train_labels):.1%})\")\n",
                "print(f\"  Negative: {neg_count:,} ({neg_count/len(train_labels):.1%})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze text lengths\n",
                "text_lengths = [len(text.split()) for text in dataset['train']['text']]\n",
                "\n",
                "print(\"\\nText length statistics (words):\")\n",
                "print(f\"  Min: {min(text_lengths)}\")\n",
                "print(f\"  Max: {max(text_lengths)}\")\n",
                "print(f\"  Mean: {np.mean(text_lengths):.0f}\")\n",
                "print(f\"  Median: {np.median(text_lengths):.0f}\")\n",
                "\n",
                "# Plot distribution\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
                "plt.axvline(x=256, color='r', linestyle='--', label='256 tokens (typical limit)')\n",
                "plt.xlabel('Number of Words')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Distribution of Review Lengths')\n",
                "plt.legend()\n",
                "plt.xlim(0, 1000)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample reviews\n",
                "print(\"Sample Positive Review:\")\n",
                "print(\"-\" * 50)\n",
                "pos_example = dataset['train'].filter(lambda x: x['label'] == 1)[0]\n",
                "print(pos_example['text'][:500] + \"...\")\n",
                "\n",
                "print(\"\\nSample Negative Review:\")\n",
                "print(\"-\" * 50)\n",
                "neg_example = dataset['train'].filter(lambda x: x['label'] == 0)[0]\n",
                "print(neg_example['text'][:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Data Preparation\n",
                "\n",
                "For faster training in this demo, we'll use a subset. In production, use the full dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create subsets for faster training (increase for better results)\n",
                "train_size = 2000\n",
                "test_size = 500\n",
                "\n",
                "# Balanced sampling\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "test_data = dataset['test'].shuffle(seed=42).select(range(test_size))\n",
                "\n",
                "# Create validation split from training data\n",
                "train_val_split = train_data.train_test_split(test_size=0.1, seed=42)\n",
                "train_dataset = train_val_split['train']\n",
                "val_dataset = train_val_split['test']\n",
                "\n",
                "print(f\"Training samples: {len(train_dataset)}\")\n",
                "print(f\"Validation samples: {len(val_dataset)}\")\n",
                "print(f\"Test samples: {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        padding=True,\n",
                "        truncation=True,\n",
                "        max_length=256  # IMDB reviews can be long\n",
                "    )\n",
                "\n",
                "# Tokenize all datasets\n",
                "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
                "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
                "test_tokenized = test_data.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"Tokenization complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    model_name,\n",
                "    num_labels=2,\n",
                "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
                "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
                ")\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define comprehensive metrics\n",
                "accuracy = evaluate.load(\"accuracy\")\n",
                "f1 = evaluate.load(\"f1\")\n",
                "precision = evaluate.load(\"precision\")\n",
                "recall = evaluate.load(\"recall\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    preds = np.argmax(predictions, axis=1)\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy.compute(predictions=preds, references=labels)['accuracy'],\n",
                "        'f1': f1.compute(predictions=preds, references=labels)['f1'],\n",
                "        'precision': precision.compute(predictions=preds, references=labels)['precision'],\n",
                "        'recall': recall.compute(predictions=preds, references=labels)['recall']\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./imdb_sentiment_model\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=32,\n",
                "    learning_rate=2e-5,\n",
                "    weight_decay=0.01,\n",
                "    warmup_ratio=0.1,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1\",\n",
                "    logging_steps=25,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "# Data collator\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "\n",
                "# Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_tokenized,\n",
                "    eval_dataset=val_tokenized,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluation and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "test_results = trainer.evaluate(test_tokenized)\n",
                "\n",
                "print(\"Test Set Results:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Accuracy:  {test_results['eval_accuracy']:.2%}\")\n",
                "print(f\"F1 Score:  {test_results['eval_f1']:.2%}\")\n",
                "print(f\"Precision: {test_results['eval_precision']:.2%}\")\n",
                "print(f\"Recall:    {test_results['eval_recall']:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions for confusion matrix\n",
                "predictions = trainer.predict(test_tokenized)\n",
                "preds = np.argmax(predictions.predictions, axis=1)\n",
                "true_labels = predictions.label_ids\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(true_labels, preds)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Negative', 'Positive'],\n",
                "            yticklabels=['Negative', 'Positive'])\n",
                "plt.ylabel('True Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(true_labels, preds, target_names=['Negative', 'Positive']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Error Analysis\n",
                "\n",
                "Understanding where the model fails helps improve it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find misclassified examples\n",
                "test_texts = test_data['text']\n",
                "misclassified_indices = np.where(preds != true_labels)[0]\n",
                "\n",
                "print(f\"Misclassified: {len(misclassified_indices)} out of {len(true_labels)} ({len(misclassified_indices)/len(true_labels):.1%})\")\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"Sample Misclassified Reviews:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for idx in misclassified_indices[:3]:\n",
                "    true_label = \"Positive\" if true_labels[idx] == 1 else \"Negative\"\n",
                "    pred_label = \"Positive\" if preds[idx] == 1 else \"Negative\"\n",
                "    \n",
                "    print(f\"\\nTrue: {true_label} | Predicted: {pred_label}\")\n",
                "    print(f\"Text: {test_texts[idx][:300]}...\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Interactive Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Create inference pipeline\n",
                "sentiment_classifier = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "# Test examples\n",
                "test_reviews = [\n",
                "    \"This movie is a masterpiece. The acting, direction, and story are all perfect.\",\n",
                "    \"Absolute garbage. Waste of 2 hours of my life. Avoid at all costs.\",\n",
                "    \"It was okay. Some good moments but overall forgettable.\",\n",
                "    \"Not as good as the original, but still entertaining enough.\"\n",
                "]\n",
                "\n",
                "print(\"Live Predictions:\")\n",
                "print(\"=\" * 60)\n",
                "for review in test_reviews:\n",
                "    result = sentiment_classifier(review)[0]\n",
                "    emoji = \"ðŸ˜Š\" if result['label'] == \"POSITIVE\" else \"ðŸ˜ \"\n",
                "    print(f\"{emoji} {result['label']} ({result['score']:.2%})\")\n",
                "    print(f\"   {review[:60]}...\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Student Challenge\n",
                "\n",
                "### Challenge: Improve Model Performance\n",
                "\n",
                "Try these strategies to improve the model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Experiment with these improvements:\n",
                "\n",
                "# 1. Use more training data (increase train_size)\n",
                "# 2. Try a different base model (e.g., \"bert-base-uncased\", \"roberta-base\")\n",
                "# 3. Adjust hyperparameters (learning rate, batch size, epochs)\n",
                "# 4. Increase max_length for longer reviews\n",
                "\n",
                "# Track your improvements:\n",
                "# | Change | Accuracy | F1 |\n",
                "# |--------|----------|----|\n",
                "# | Baseline | ... | ... |\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Data exploration** is crucial before training\n",
                "2. **Balanced datasets** lead to better model performance\n",
                "3. **Multiple metrics** (accuracy, F1, precision, recall) give complete picture\n",
                "4. **Error analysis** helps identify model weaknesses\n",
                "5. **Confusion matrices** visualize classification errors\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to `03_summarization.ipynb` for sequence-to-sequence tasks!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
