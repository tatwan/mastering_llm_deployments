{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning a Language Model for Healthcare Question Answering\n",
                "\n",
                "**Module 02 | Notebook 4 of 4**\n",
                "\n",
                "In this notebook, we will fine-tune a specialized language model (`TinyLlama`) to answer medical questions using the `MedQuad` dataset.\n",
                "\n",
                "## Learning Objectives\n",
                "1.  Understand the difference between **Context (RAG)** vs **Fine-Tuning**.\n",
                "2.  Learn about **PEFT (Parameter-Efficient Fine-Tuning)** and **LoRA**.\n",
                "3.  Fine-tune a model on a custom dataset.\n",
                "4.  Export the model for local use.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üö® CRITICAL: Medical AI Ethics - READ THIS FIRST\n",
                "\n",
                "> ‚ö†Ô∏è **CAUTION: This notebook is for EDUCATIONAL PURPOSES ONLY**\n",
                "\n",
                "### ‚ùå DO NOT Use This Model For:\n",
                "- Real medical diagnosis or treatment decisions\n",
                "- Emergency health situations\n",
                "- Any life or health decisions\n",
                "- Patient-facing applications without regulatory approval\n",
                "\n",
                "### ‚öñÔ∏è Legal Reality:\n",
                "- Medical AI requires **FDA approval** (or equivalent in your country)\n",
                "- Clinical validation studies are mandatory\n",
                "- Professional liability insurance is required\n",
                "- Violations can result in legal consequences\n",
                "\n",
                "### üéì Why We Use Medical Data:\n",
                "We use healthcare data because it:\n",
                "- Demonstrates domain adaptation effectively\n",
                "- Is publicly available (MedQuad dataset)\n",
                "- Shows how AI learns specialized terminology\n",
                "\n",
                "**The skills you learn here apply to SAFER domains:**\n",
                "- Creative writing assistants\n",
                "- Customer service bots\n",
                "- Code documentation helpers\n",
                "- Product recommendation systems\n",
                "\n",
                "> ‚ö†Ô∏è **If you're building real medical AI, stop here and consult with healthcare regulatory experts first.**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What You'll Build Today\n",
                "\n",
                "By the end of this notebook, you will have:\n",
                "- ‚úÖ A custom medical question-answering model\n",
                "- ‚úÖ Understanding of when to use fine-tuning vs RAG\n",
                "- ‚úÖ Hands-on experience with LoRA (Low-Rank Adaptation)\n",
                "- ‚úÖ A model that uses only ~2% of the original parameters to learn!\n",
                "\n",
                "**Estimated Time:** 45-60 minutes (depending on hardware)\n",
                "**Prerequisites:** Basic Python, understanding of neural networks\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚è±Ô∏è Before You Start - What to Expect\n",
                "\n",
                "### Time Estimates by Hardware\n",
                "\n",
                "| Hardware | Training Time | Memory Needed |\n",
                "|----------|--------------|---------------|\n",
                "| NVIDIA GPU (RTX 3060+) | ~10-15 min | 8 GB VRAM |\n",
                "| Apple M1/M2/M3 | ~30-45 min | 16 GB RAM |\n",
                "| CPU Only | ~60-90 min | 16 GB RAM |\n",
                "\n",
                "### System Requirements\n",
                "- **Disk Space:** ~5 GB (for model downloads and checkpoints)\n",
                "- **RAM:** 16 GB recommended (8 GB minimum)\n",
                "- **GPU:** Optional but speeds up training 4-6x\n",
                "\n",
                "### üéØ Reality Check\n",
                "\n",
                "> üí° **We use only 500 examples for demo speed.**\n",
                ">\n",
                "> - Real production models use 5,000-50,000+ examples\n",
                "> - You're learning the **PROCESS**, not building production-ready AI\n",
                "> - Quality comes from data size AND data quality\n",
                "\n",
                "### ‚úÖ What Success Looks Like\n",
                "\n",
                "After completing this notebook, your model will:\n",
                "- Give basic medical-sounding answers\n",
                "- Use appropriate medical terminology\n",
                "- Follow the Q&A format you trained it on\n",
                "\n",
                "**It won't be perfect - that's OK!** With 500 examples, expect decent but not expert-level responses.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. When to use What? (RAG vs. Fine-Tuning)\n",
                "\n",
                "Before we start, it's important to know *when* to fine-tune.\n",
                "\n",
                "| Feature | **RAG (Retrieval-Augmented Gen)** | **Fine-Tuning** |\n",
                "| :--- | :--- | :--- |\n",
                "| **Analogy** | Giving the model an open textbook during the exam. | Sending the model to medical school for 4 years. |\n",
                "| **Goal** | Add new *knowledge* (facts, data). | Change *behavior*, style, or learn specialized jargon. |\n",
                "| **Pros** | Cheaper, easier to update facts. | Better performance on specific tasks, faster inference (no retrieval). |\n",
                "| **Cons** | Limited context window. | Expensive to train, hard to update facts (requires re-training). |\n",
                "\n",
                "**In this notebook**, we are doing **Fine-Tuning** to teach the model how to *act* like a medical assistant and understand medical terminology, not necessarily to memorize every drug interaction (RAG would be better for that)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üåç Real-World Examples\n",
                "\n",
                "**When to use RAG:**\n",
                "- A company chatbot that needs to answer questions about constantly changing product documentation\n",
                "- A legal assistant that references current laws and regulations\n",
                "- A customer service bot with access to your latest knowledge base\n",
                "\n",
                "**When to use Fine-Tuning:**\n",
                "- Teaching a model to write in a specific tone (e.g., Shakespearean English)\n",
                "- Making a model understand medical/legal jargon and respond appropriately\n",
                "- Creating a coding assistant that follows your company's specific style guide\n",
                "- Teaching a model to structure outputs in a particular format (e.g., always JSON)\n",
                "\n",
                "**üí° Pro Tip:** Many production systems use BOTH! They fine-tune for style/behavior and use RAG for facts.\n",
                "\n",
                "### üìö Think of it this way:\n",
                "\n",
                "**RAG = Open Book Exam**\n",
                "- You (the model) can look up answers in provided documents\n",
                "- You don't need to memorize everything\n",
                "- If the documents are updated, you automatically have new information\n",
                "\n",
                "**Fine-Tuning = Closed Book Exam (After Studying)**\n",
                "- You've internalized the patterns and style\n",
                "- You respond faster (no need to search documents)\n",
                "- But updating your knowledge requires studying again (retraining)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate peft trl bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, \n",
                "    AutoTokenizer, \n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Device setup\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "if torch.backends.mps.is_available():\n",
                "    device = \"mps\"  # For Mac users\n",
                "\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Load the Dataset\n",
                "\n",
                "We will use `MedQuad` from generic sources. It contains pairs of `Question` and `Answer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "dataset_name = \"keivalya/MedQuad-MedicalQnADataset\"\n",
                "dataset = load_dataset(dataset_name, split=\"train\")\n",
                "\n",
                "# Use a small subset for demonstration (Top 500 examples)\n",
                "dataset = dataset.select(range(500))\n",
                "\n",
                "print(f\"Training on {len(dataset)} examples\")\n",
                "print(\"Sample:\", dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìä Understanding Our Dataset\n",
                "\n",
                "Let's explore what we're working with:\n",
                "\n",
                "**MedQuad Dataset:**\n",
                "- Contains medical questions and expert answers\n",
                "- Sourced from trusted health organizations (NIH, CDC, etc.)\n",
                "- Format: {Question, Answer} pairs\n",
                "\n",
                "**Why only 500 examples?**\n",
                "- Full dataset has 16,000+ examples\n",
                "- For learning purposes, 500 is enough to see results quickly\n",
                "- In production, you'd use the full dataset (takes 2-4 hours to train)\n",
                "\n",
                "### üíé Dataset Quality Matters!\n",
                "\n",
                "> üí° **Dataset size for this demo: 500 examples**\n",
                "> - ‚úÖ For learning the process: Enough!\n",
                "> - ‚ùå For production use: Need 5,000-50,000+\n",
                "\n",
                "**What makes good training data?**\n",
                "- ‚úÖ Consistent format (all Q&A pairs formatted the same)\n",
                "- ‚úÖ Accurate information (wrong answers = model learns wrong things)\n",
                "- ‚úÖ Diverse examples (different types of questions)\n",
                "- ‚úÖ Clean text (no weird characters, proper grammar)\n",
                "\n",
                "**Red Flags in Training Data:**\n",
                "- ‚ùå Contradictory answers to similar questions\n",
                "- ‚ùå Very short answers (< 10 words)\n",
                "- ‚ùå Copy-pasted repetitive text\n",
                "- ‚ùå Mixed languages or formats\n",
                "\n",
                "**For This Notebook:**\n",
                "- MedQuad is pre-cleaned ‚úÖ\n",
                "- Only 500 examples = expect decent but not perfect results\n",
                "- **In real projects: Spend 50% of time on data quality!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üéØ Why Data Formatting is Critical\n",
                "\n",
                "The model doesn't \"know\" what's a question vs an answer. We need to teach it!\n",
                "\n",
                "**Bad formatting:**\n",
                "```\n",
                "What are the symptoms of diabetes? Increased thirst, frequent urination...\n",
                "```\n",
                "The model sees one continuous text blob - it doesn't know where the question ends!\n",
                "\n",
                "**Good formatting:**\n",
                "```\n",
                "### Question:\n",
                "What are the symptoms of diabetes?\n",
                "\n",
                "### Answer:\n",
                "Increased thirst, frequent urination...\n",
                "```\n",
                "Now the model learns the pattern:\n",
                "1. Text after \"### Question:\" = what the user asks\n",
                "2. Text after \"### Answer:\" = what I should respond\n",
                "\n",
                "**This is called \"prompt formatting\" - it's one of the most important parts of fine-tuning!**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting\n",
                "To train a chat model, we format the data clearly so the model knows what is the input and what is the output.\n",
                "\n",
                "```\n",
                "### Question:\n",
                "{User's Question}\n",
                "\n",
                "### Answer:\n",
                "{Model's Answer}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def formatting_func(example):\n",
                "    text = f\"\"\"### Question:\n",
                "{example['Question']}\n",
                "\n",
                "### Answer:\n",
                "{example['Answer']}\"\"\"\n",
                "    return text\n",
                "\n",
                "print(formatting_func(dataset[0]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5 Understanding Quantization (Memory Optimization)\n",
                "\n",
                "Before we load our model, let's understand how we'll fit a billion-parameter model on your laptop!\n",
                "\n",
                "### What is Quantization?\n",
                "\n",
                "**The Problem:**\n",
                "- Modern LLMs are HUGE. TinyLlama (our \"small\" model) has 1.1 BILLION parameters\n",
                "- Each parameter is typically stored as a 32-bit float = 4 bytes\n",
                "- 1.1B √ó 4 bytes = 4.4 GB just for the model weights!\n",
                "\n",
                "**The Solution: Quantization**\n",
                "- Store numbers in fewer bits (4-bit or 8-bit instead of 32-bit)\n",
                "- **4-bit quantization:** 1.1B √ó 0.5 bytes ‚âà 550 MB (8√ó smaller!)\n",
                "\n",
                "### The Trade-off\n",
                "```\n",
                "Higher Precision ‚Üí More Memory ‚Üí Better Quality (slightly)\n",
                "Lower Precision ‚Üí Less Memory ‚Üí Faster Training ‚Üí Tiny quality loss\n",
                "```\n",
                "**For learning and experimentation, 4-bit is perfect!**\n",
                "\n",
                "### Hardware-Specific Notes:\n",
                "- **NVIDIA GPU:** We use `bitsandbytes` library for 4-bit quantization\n",
                "- **Mac (M1/M2/M3):** We use float16 (16-bit) instead\n",
                "  - Why? The `bitsandbytes` library doesn't support Mac GPUs yet\n",
                "  - Good news: TinyLlama is small enough that float16 works fine!\n",
                "- **CPU Only:** We also use float16, but training will be slower\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Model Setup (with Conditional Quantization)\n",
                "\n",
                "We use **TinyLlama-1.1B**. It's small enough to run on most laptops.\n",
                "\n",
                "### Hardware Note\n",
                "*   **NVIDIA GPU**: We can use **4-bit quantization** to save massive memory.\n",
                "*   **Mac (M1/M2/M3)**: 4-bit quantization (`bitsandbytes`) is not natively supported. We will load the model in `float16` instead. TinyLlama is small (2GB), so this works fine!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "# Determine optimized settings based on hardware\n",
                "if device == \"cuda\":\n",
                "    # Quantization Config (NVIDIA only)\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "    )\n",
                "    model_kwargs = {\"quantization_config\": bnb_config}\n",
                "else:\n",
                "    # Mac/CPU: Load in half-precision (float16) to save RAM\n",
                "    bnb_config = None\n",
                "    model_kwargs = {\"torch_dtype\": torch.float16}\n",
                "\n",
                "print(f\"Loading model with config: {model_kwargs}\")\n",
                "\n",
                "# Load Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    device_map=\"auto\" if device == \"cuda\" else None, # MPS/CPU mapping handled manually or by defaults\n",
                "    trust_remote_code=True,\n",
                "    **model_kwargs\n",
                ")\n",
                "\n",
                "# For Mac MPS specifically, we explicit move if needed, but 'auto' usually avoids MPS for some models unless explicit\n",
                "if device == \"mps\":\n",
                "    model.to(\"mps\")\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéì LoRA for Complete Beginners\n",
                "\n",
                "Before we dive into the technical details, let's understand LoRA with a simple analogy:\n",
                "\n",
                "### Think of Your Brain Learning Guitar\n",
                "\n",
                "You already know how to use your hands (pre-trained model = general knowledge).\n",
                "\n",
                "**Full Fine-Tuning = Relearn how to use your hands from scratch**\n",
                "- Forget everything, start over\n",
                "- Takes months\n",
                "- Very expensive\n",
                "\n",
                "**LoRA = Just learn the guitar-specific finger movements**\n",
                "- Keep all your hand knowledge\n",
                "- Only learn the NEW muscle memory\n",
                "- Takes weeks, much cheaper\n",
                "\n",
                "### In AI Terms:\n",
                "- TinyLlama has **1.1 BILLION** parameters (all its knowledge)\n",
                "- Full fine-tuning = update all 1.1B parameters (slow, expensive)\n",
                "- LoRA = freeze those 1.1B, add **~20M new \"adapter\" parameters** (fast, cheap)\n",
                "- You're only training **~2% of the model!**\n",
                "\n",
                "**The Magic:** Those 20M parameters are enough to teach specialized behavior.\n",
                "\n",
                "### Visual:\n",
                "```\n",
                "Full Model:     [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 1.1B parameters (FROZEN ‚ùÑÔ∏è)\n",
                "LoRA Adapters:  [‚ñà‚ñà] 20M parameters (LEARNING üî•)\n",
                "Result:         Medical-specialized model!\n",
                "```\n",
                "\n",
                "Now let's look at the technical details..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Setting up LoRA (Low-Rank Adaptation)\n",
                "\n",
                "### The Valid Analogy\n",
                "Imagine you want to customize your car (Pre-trained Model).\n",
                "*   **Full Fine-Tuning**: Rebuilding the entire engine. Powerful, but expensive and slow.\n",
                "*   **LoRA**: Adding a \"Turbocharger\" plugin. You don't touch the engine; you just add a small, focused part that modifies the performance.\n",
                "\n",
                "### Parameters\n",
                "*   **`r` (Rank)**: The size of the \"plugin\". Bigger = smarter but slower. (Common: 8, 16)\n",
                "*   **`target_modules`**: Where to attach the plugin. In a Transformer, we usually attach to the Attention layers (`q_proj`, `v_proj`)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üî¨ How LoRA Actually Works (Technical)\n",
                "\n",
                "Let's understand what's happening under the hood:\n",
                "\n",
                "**Traditional Fine-Tuning:**\n",
                "```\n",
                "Original Model: 1.1 Billion parameters\n",
                "Fine-Tuning: Update ALL 1.1 Billion parameters\n",
                "Memory needed: ~20 GB\n",
                "Time: Many hours\n",
                "```\n",
                "**LoRA Fine-Tuning:**\n",
                "```\n",
                "Original Model: 1.1 Billion parameters (FROZEN ‚ùÑÔ∏è)\n",
                "LoRA Adapters: ~2-20 Million parameters (TRAINABLE üî•)\n",
                "Memory needed: ~4 GB\n",
                "Time: Much faster!\n",
                "```\n",
                "**The Math (Simplified):**\n",
                "- In a transformer, we have weight matrices like `W` (e.g., 4096 √ó 4096)\n",
                "- LoRA adds two small matrices: `A` (4096 √ó r) and `B` (r √ó 4096)\n",
                "- Instead of updating `W`, we learn `A` and `B`\n",
                "- Final output: `W¬∑x + B¬∑(A¬∑x)` ‚âà Updated model behavior\n",
                "\n",
                "**Why does this work?**\n",
                "- The \"important\" changes to a model often live in a lower-dimensional space\n",
                "- `r` (rank) controls this dimension - usually 8 or 16 is enough!\n",
                "\n",
                "### Visualizing Parameter Count\n",
                "```\n",
                "Full Fine-Tuning: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% parameters\n",
                "LoRA (r=16):      [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 1.8% parameters\n",
                "```\n",
                "You're only training ~20 million out of 1.1 billion parameters!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üéõÔ∏è Understanding LoRA Parameters\n",
                "\n",
                "Let's break down each parameter in our config:\n",
                "\n",
                "**`r=16` (Rank)**\n",
                "- The \"size\" of our adaptation\n",
                "- Higher = More powerful but slower and more memory\n",
                "- Typical values: 8, 16, 32, 64\n",
                "- **Our choice (16):** Good balance for most tasks\n",
                "\n",
                "**`lora_alpha=32` (Scaling Factor)**\n",
                "- Controls how much influence LoRA has\n",
                "- Rule of thumb: `alpha = 2 √ó r`\n",
                "- **Our choice (32):** Standard scaling for r=16\n",
                "\n",
                "**`target_modules` (Where to apply LoRA)**\n",
                "```\n",
                "Transformer Layer:\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ   Attention    ‚îÇ ‚Üê We target these!\n",
                "‚îÇ - q_proj     ‚úì ‚îÇ (Query, Key, Value, Output)\n",
                "‚îÇ - k_proj     ‚úì ‚îÇ\n",
                "‚îÇ - v_proj     ‚úì ‚îÇ\n",
                "‚îÇ - o_proj     ‚úì ‚îÇ\n",
                "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "‚îÇ  Feed Forward  ‚îÇ ‚Üê We skip these\n",
                "‚îÇ - gate_proj    ‚îÇ (To save memory)\n",
                "‚îÇ - up_proj      ‚îÇ\n",
                "‚îÇ - down_proj    ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "- **Why attention layers?** They control how the model \"understands\" relationships\n",
                "- **Pro tip:** For maximum quality, target FFN layers too (but uses more memory)\n",
                "\n",
                "**`lora_dropout=0.05`**\n",
                "- Randomly \"turns off\" 5% of LoRA neurons during training\n",
                "- Prevents overfitting (memorizing instead of learning)\n",
                "- **Our choice (0.05):** Conservative, works well for small datasets\n",
                "\n",
                "**`task_type=\"CAUSAL_LM\"`**\n",
                "- Tells LoRA we're doing language generation (not classification)\n",
                "- CAUSAL_LM = predict next word (like GPT)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if bnb_config: # If using quantization\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16, \n",
                "    lora_alpha=32, \n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèãÔ∏è Understanding Training Parameters\n",
                "\n",
                "Let's demystify each training parameter:\n",
                "\n",
                "### Core Training Settings\n",
                "\n",
                "**`num_train_epochs=1`**\n",
                "- An \"epoch\" = one complete pass through the entire dataset\n",
                "- 1 epoch on 500 examples = Model sees each Q&A pair once\n",
                "- **Why just 1?** For demo purposes. Production uses 3-5 epochs.\n",
                "\n",
                "**`per_device_train_batch_size=2`**\n",
                "- How many examples to process at once\n",
                "- Larger = faster training but more memory\n",
                "- **Our choice (2):** Conservative for laptops with 8-16 GB RAM\n",
                "- If you have a beefy GPU: try 4 or 8\n",
                "\n",
                "**`gradient_accumulation_steps=4`**\n",
                "- This is clever! We simulate a larger batch size without using more memory\n",
                "- Effective batch size = 2 √ó 4 = 8\n",
                "- **How it works:**\n",
                "```\n",
                "Step 1: Process 2 examples, calculate gradients (don't update yet)\n",
                "Step 2: Process 2 more, accumulate gradients\n",
                "Step 3: Process 2 more, accumulate gradients\n",
                "Step 4: Process 2 more, accumulate gradients\n",
                "NOW: Update the model with accumulated gradients from 8 examples\n",
                "```\n",
                "**`learning_rate=2e-4` (0.0002)**\n",
                "- How big of a \"step\" to take when updating parameters\n",
                "- Too high ‚Üí Model explodes üí• or doesn't learn\n",
                "- Too low ‚Üí Training takes forever üêå\n",
                "- **Our choice (2e-4):** Standard for LoRA fine-tuning\n",
                "\n",
                "### Advanced Settings\n",
                "\n",
                "**`bf16=True` (Brain Float 16)**\n",
                "- Use 16-bit precision instead of 32-bit for faster training\n",
                "- \"Brain Float\" = special 16-bit format optimized for AI training\n",
                "- Saves memory and speeds up computation\n",
                "\n",
                "**`max_length=512`**\n",
                "- Maximum tokens (words/subwords) in a training example\n",
                "- Longer sequences = more context but more memory\n",
                "- Medical Q&A rarely exceeds 512 tokens\n",
                "\n",
                "**`packing=False`**\n",
                "- Could we pack multiple short examples into one sequence?\n",
                "- No - we want clean question-answer separation\n",
                "\n",
                "### üìä What to Expect:\n",
                "```\n",
                "Training on 500 examples:\n",
                "- With GPU: ~10-15 minutes\n",
                "- With CPU: ~45-60 minutes\n",
                "- You'll see loss decreasing (good!) - aim for < 1.0\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = SFTConfig(\n",
                "    output_dir=\"./tinyllama-medical\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=2, # Keep low for standard laptops\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=10,\n",
                "    bf16=True,  # Changed from fp16=True\n",
                "    dataset_text_field=\"text\",\n",
                "    max_length=512,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=lora_config,\n",
                "    formatting_func=formatting_func,\n",
                "    args=training_args,\n",
                "    processing_class=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìà Understanding the Training Output\n",
                "\n",
                "While training, you'll see output like this:\n",
                "```\n",
                "| Step | Training Loss |\n",
                "| ---- | ------------- |\n",
                "| 10   | 2.456         |\n",
                "| 20   | 1.892         |\n",
                "| 30   | 1.423         |\n",
                "| 40   | 1.156         |\n",
                "```\n",
                "**What does this mean?**\n",
                "- **Loss** = How \"wrong\" the model's predictions are\n",
                "- Lower loss = Better predictions\n",
                "- Starting loss (~2-3) = Model is guessing randomly\n",
                "- Good final loss (~0.8-1.2) = Model is learning the patterns!\n",
                "\n",
                "**‚ö†Ô∏è Warning Signs:**\n",
                "- Loss increasing ‚Üí Learning rate too high\n",
                "- Loss stuck at same value ‚Üí Learning rate too low or data too small\n",
                "- Loss drops to nearly 0 ‚Üí Model is memorizing (overfitting)\n",
                "\n",
                "**üí° What's Actually Happening During Training:**\n",
                "```\n",
                "For each Q&A pair:\n",
                "1. Model reads the question\n",
                "2. Tries to predict the answer word-by-word\n",
                "3. Compares prediction to actual answer\n",
                "4. Calculates \"how wrong\" it was (loss)\n",
                "5. Updates the LoRA adapters to do better next time\n",
                "6. Repeat!\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üíæ Saving a Training Checkpoint\n",
                "\n",
                "Training takes 15-60 minutes - you don't want to lose it! Let's save a checkpoint.\n",
                "\n",
                "**Why save checkpoints?**\n",
                "- If your computer crashes or restarts, reload from here\n",
                "- Good practice for long training runs\n",
                "- Allows you to experiment with different inference settings\n",
                "\n",
                "**To reload later:**\n",
                "```python\n",
                "from peft import PeftModel\n",
                "model = PeftModel.from_pretrained(base_model, \"./tinyllama-medical-checkpoint\")\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üíæ Save checkpoint (in case something goes wrong later)\n",
                "import os\n",
                "checkpoint_dir = \"./tinyllama-medical-checkpoint\"\n",
                "os.makedirs(checkpoint_dir, exist_ok=True)\n",
                "\n",
                "print(\"üíæ Saving training checkpoint...\")\n",
                "trainer.model.save_pretrained(checkpoint_dir)\n",
                "tokenizer.save_pretrained(checkpoint_dir)\n",
                "print(f\"‚úÖ Checkpoint saved to {checkpoint_dir}\")\n",
                "print(\"If your computer crashes, you can reload from here!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the adapter (the plugin)\n",
                "trainer.model.save_pretrained(\"./tinyllama-medical-adapter\")\n",
                "tokenizer.save_pretrained(\"./tinyllama-medical-adapter\")\n",
                "print(\"Adapter saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìä Did My Fine-Tuning Actually Work?\n",
                "\n",
                "Before we move to testing, let's understand how to evaluate if fine-tuning was successful!\n",
                "\n",
                "### How to Know If It Worked:\n",
                "\n",
                "1. **Training Loss Decreased** - You should have seen loss go from ~2-3 down to ~0.8-1.2\n",
                "2. **Coherent Outputs** - Model generates complete, sensible sentences\n",
                "3. **Domain Knowledge** - Model uses medical terminology appropriately\n",
                "4. **Format Compliance** - Model follows the Q&A format we trained it on\n",
                "\n",
                "### Quality Evaluation Checklist\n",
                "\n",
                "When testing your model, mentally check these boxes:\n",
                "\n",
                "- [ ] **Coherent:** Answers are complete sentences, not gibberish\n",
                "- [ ] **Medical terminology:** Uses appropriate medical terms\n",
                "- [ ] **Professional tone:** Maintains helpful, professional voice\n",
                "- [ ] **On-topic:** Stays on topic when asked medical questions\n",
                "- [ ] **Appropriate uncertainty:** Acknowledges limitations (\"consult a doctor\")\n",
                "\n",
                "### üìä Scoring Guide:\n",
                "\n",
                "| Score | Interpretation |\n",
                "|-------|----------------|\n",
                "| 5/5 ‚úÖ | Excellent! Fine-tuning worked well |\n",
                "| 3-4/5 ‚ö†Ô∏è | Decent, might need more epochs or data |\n",
                "| 0-2/5 ‚ùå | Something went wrong - check troubleshooting |\n",
                "\n",
                "### Common Issues Quick Reference:\n",
                "\n",
                "| Symptom | Likely Cause | Fix |\n",
                "|---------|--------------|-----|\n",
                "| Gibberish output | Learning rate too high or overtrained | Lower `learning_rate` to `1e-4` |\n",
                "| Repeats the question | Needs more training steps | Increase `num_train_epochs` |\n",
                "| Too generic | Dataset too small | Expected with 500 examples |\n",
                "| Very short answers | Model undertrained | Train longer or use more data |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Put model in evaluation mode and disable gradient checkpointing\n",
                "model.eval()\n",
                "model.config.use_cache = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üé≤ Understanding Text Generation Parameters\n",
                "\n",
                "When we ask our model a question, we need to control HOW it generates the answer:\n",
                "\n",
                "**`max_new_tokens=100`**\n",
                "- Maximum number of tokens (words/subwords) to generate\n",
                "- Think of it as \"answer length limit\"\n",
                "- 100 tokens ‚âà 75-80 words\n",
                "\n",
                "**`do_sample=True`**\n",
                "- Should the model be creative or deterministic?\n",
                "- **True:** Model picks from top probable words (varied, interesting)\n",
                "- **False:** Always picks THE most probable word (boring, repetitive)\n",
                "\n",
                "**`temperature=0.7`**\n",
                "- Controls randomness (only matters if do_sample=True)\n",
                "- Range: 0.0 to 2.0\n",
                "```\n",
                "Temperature 0.1 ‚Üí Very focused, conservative\n",
                "Temperature 0.7 ‚Üí Balanced (our choice)\n",
                "Temperature 1.0 ‚Üí More creative\n",
                "Temperature 2.0 ‚Üí Wild, incoherent\n",
                "```\n",
                "**Visual Example:**\n",
                "```\n",
                "Question: \"What helps a headache?\"\n",
                "Temperature 0.1: \"Take aspirin and rest.\" (boring but safe)\n",
                "Temperature 0.7: \"Try aspirin or ibuprofen. Rest in a dark room may help.\"\n",
                "Temperature 1.5: \"Aspirin, meditation, cucumber slices, purple thoughts...\"\n",
                "```\n",
                "**For Medical Applications:**\n",
                "- Keep temperature low (0.3-0.7) for accuracy\n",
                "- Higher temperature risks hallucinations (making up facts)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask(question):\n",
                "    prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        **inputs, \n",
                "        max_new_tokens=100, \n",
                "        do_sample=True, \n",
                "        temperature=0.7\n",
                "    )\n",
                "    \n",
                "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "\n",
                "ask(\"What are the symptoms of a cold?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß™ Test Your Fine-Tuned Model\n",
                "\n",
                "Let's compare the model's performance on different types of questions:\n",
                "\n",
                "**Test 1: Medical Knowledge (From Training)**\n",
                "```python\n",
                "ask(\"What are the symptoms of diabetes?\")\n",
                "# Expected: Should give medically accurate symptoms\n",
                "```\n",
                "**Test 2: General Medical (Not in Training)**\n",
                "```python\n",
                "ask(\"How can I prevent the flu?\")\n",
                "# Expected: Reasonable medical advice, even if not trained on this exact question\n",
                "```\n",
                "**Test 3: Off-Topic Question**\n",
                "```python\n",
                "ask(\"What's the capital of France?\")\n",
                "# Interesting: Model might still answer, or might try to frame as medical\n",
                "```\n",
                "### üìä Evaluating Quality\n",
                "\n",
                "**Good Signs ‚úÖ:**\n",
                "- Answers are medically relevant\n",
                "- Uses appropriate medical terminology\n",
                "- Maintains professional tone\n",
                "- Acknowledges uncertainty when appropriate\n",
                "\n",
                "**Bad Signs ‚ùå:**\n",
                "- Makes up fake medical facts\n",
                "- Uses inappropriate casual language\n",
                "- Gives dangerous medical advice\n",
                "- Refuses to answer simple questions\n",
                "\n",
                "**Remember:** This model is NOT ready for real medical advice! It's a learning exercise.\n",
                "\n",
                "### üî¨ Advanced: Compare to Base Model\n",
                "\n",
                "Want to see the difference fine-tuning made? Load the base model and compare:\n",
                "```python\n",
                "# Load base model (not fine-tuned)\n",
                "base_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
                "# Ask the same question to both models\n",
                "# See the difference!\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Optional: Local Deployment with Ollama (GGUF Format)\n",
                "\n",
                "After fine-tuning, you may want to run your model locally without cloud dependencies. **Ollama** is a popular tool for running LLMs locally, and it uses the **GGUF** format.\n",
                "\n",
                "### What is GGUF?\n",
                "\n",
                "**GGUF (GPT-Generated Unified Format)** is a file format designed for efficient local inference of large language models. It was created by the [llama.cpp](https://github.com/ggerganov/llama.cpp) project.\n",
                "\n",
                "**Why GGUF?**\n",
                "- **CPU + GPU Inference:** Runs efficiently on CPUs and GPUs alike\n",
                "- **Quantization Built-In:** Supports various quantization levels (4-bit, 8-bit, etc.) for memory efficiency\n",
                "- **Single File:** Everything needed for inference is in one file\n",
                "- **Popular Ecosystem:** Used by Ollama, llama.cpp, GPT4All, and more\n",
                "\n",
                "**Think of it as:**\n",
                "- PyTorch format = Developer-friendly, for training and research\n",
                "- GGUF format = Production-friendly, optimized for local deployment\n",
                "\n",
                "### The Conversion Pipeline\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ  LoRA Adapter   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Merged Model   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    GGUF File    ‚îÇ\n",
                "‚îÇ  (Small, ~MB)   ‚îÇ     ‚îÇ  (Full, ~2GB)   ‚îÇ     ‚îÇ  (Quantized)    ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "                              ‚îÇ\n",
                "                              ‚ñº\n",
                "                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "                        ‚îÇ     Ollama      ‚îÇ\n",
                "                        ‚îÇ  Local Server   ‚îÇ\n",
                "                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: Merge LoRA Adapters with Base Model\n",
                "\n",
                "First, we need to merge our LoRA adapters back into the base model to create a standalone model.\n",
                "\n",
                "> **Why merge?** LoRA adapters are small \"plugins\" that modify the base model. To use with Ollama, we need a complete, standalone model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import PeftModel\n",
                "\n",
                "# Path to your saved LoRA adapter\n",
                "adapter_path = \"./tinyllama-medical-adapter\"\n",
                "merged_output_dir = \"./tinyllama-medical-merged\"\n",
                "\n",
                "# Load base model WITHOUT quantization for merging\n",
                "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\" if device == \"cuda\" else None,\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "\n",
                "# Load and merge the LoRA weights\n",
                "merged_model = PeftModel.from_pretrained(base_model_for_merge, adapter_path)\n",
                "merged_model = merged_model.merge_and_unload()  # This combines LoRA weights with base model\n",
                "\n",
                "# Save the merged model\n",
                "merged_model.save_pretrained(merged_output_dir)\n",
                "tokenizer.save_pretrained(merged_output_dir)\n",
                "\n",
                "print(f\"‚úÖ Merged model saved to {merged_output_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Convert to GGUF Format\n",
                "\n",
                "Now we need to convert the merged model to GGUF format using `llama.cpp`. This step is typically done in a terminal:\n",
                "\n",
                "**Install llama.cpp:**\n",
                "```bash\n",
                "git clone https://github.com/ggerganov/llama.cpp.git\n",
                "cd llama.cpp\n",
                "pip install -r requirements.txt\n",
                "```\n",
                "\n",
                "**Convert to GGUF:**\n",
                "```bash\n",
                "python convert_hf_to_gguf.py /path/to/tinyllama-medical-merged \\\n",
                "  --outfile tinyllama-medical-qa.gguf \\\n",
                "  --outtype f16\n",
                "```\n",
                "\n",
                "### Optional: Quantize for Smaller Size\n",
                "\n",
                "Quantization reduces file size and speeds up inference:\n",
                "\n",
                "```bash\n",
                "# Build llama.cpp first\n",
                "cd llama.cpp\n",
                "mkdir build && cd build\n",
                "cmake ..\n",
                "cmake --build . --config Release\n",
                "\n",
                "# Quantize to 4-bit (Q4_K_M is recommended for balance of speed/quality)\n",
                "./bin/llama-quantize tinyllama-medical-qa.gguf \\\n",
                "  tinyllama-medical-qa-q4.gguf Q4_K_M\n",
                "```\n",
                "\n",
                "**Quantization Options:**\n",
                "| Type | Size Reduction | Quality | Use Case |\n",
                "| :--- | :--- | :--- | :--- |\n",
                "| F16 | None | Best | When size doesn't matter |\n",
                "| Q8_0 | ~50% | Excellent | Good balance |\n",
                "| Q4_K_M | ~75% | Very Good | **Recommended** |\n",
                "| Q4_0 | ~75% | Good | Faster, slightly lower quality |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Set Up Ollama\n",
                "\n",
                "**Install Ollama:**\n",
                "```bash\n",
                "# On macOS/Linux:\n",
                "curl -fsSL https://ollama.com/install.sh | sh\n",
                "\n",
                "# On Windows: Download from https://ollama.com/download\n",
                "```\n",
                "\n",
                "**Create a Modelfile:**\n",
                "\n",
                "Create a file named `Modelfile` (no extension) with the following content:\n",
                "\n",
                "```\n",
                "FROM ./tinyllama-medical-qa-q4.gguf\n",
                "\n",
                "TEMPLATE \"\"\"### Question:\n",
                "{{ .Prompt }}\n",
                "\n",
                "### Answer:\n",
                "\"\"\"\n",
                "\n",
                "SYSTEM \"You are a helpful medical assistant trained to answer healthcare questions.\"\n",
                "\n",
                "PARAMETER temperature 0.7\n",
                "PARAMETER top_p 0.9\n",
                "PARAMETER stop \"### Question:\"\n",
                "```\n",
                "\n",
                "> **Note:** The `TEMPLATE` must match the format we used during training! This ensures the model receives prompts in the same structure it learned."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Create and Run in Ollama\n",
                "\n",
                "```bash\n",
                "# Start Ollama server (if not already running)\n",
                "ollama serve &\n",
                "\n",
                "# Create the model in Ollama\n",
                "ollama create tinyllama-medical-qa -f Modelfile\n",
                "\n",
                "# Run the model interactively\n",
                "ollama run tinyllama-medical-qa\n",
                "```\n",
                "\n",
                "**Test your model:**\n",
                "```bash\n",
                "ollama run tinyllama-medical-qa \"What are the symptoms of diabetes?\"\n",
                "```\n",
                "\n",
                "### Using from Python (API)\n",
                "\n",
                "Once your model is in Ollama, you can also use it via API:\n",
                "\n",
                "```python\n",
                "import requests\n",
                "\n",
                "response = requests.post('http://localhost:11434/api/generate', json={\n",
                "    'model': 'tinyllama-medical-qa',\n",
                "    'prompt': 'What are the symptoms of the flu?',\n",
                "    'stream': False\n",
                "})\n",
                "\n",
                "print(response.json()['response'])\n",
                "```\n",
                "\n",
                "### üéâ Congratulations!\n",
                "\n",
                "You now have a fine-tuned model running locally with Ollama! This means:\n",
                "- ‚úÖ No cloud API costs\n",
                "- ‚úÖ Complete privacy (data never leaves your machine)\n",
                "- ‚úÖ Works offline\n",
                "- ‚úÖ Can be integrated into any local application\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Student Challenge: Build an Empathetic Mental Health Bot\n",
                "\n",
                "### The Goal\n",
                "Medical data is factual and clinical. But what if we want a bot that's warm and comforting?\n",
                "\n",
                "### Why This Matters\n",
                "- Tone and style matter in AI applications\n",
                "- Same model, different fine-tuning = completely different personality\n",
                "- This demonstrates fine-tuning for BEHAVIOR, not just knowledge\n",
                "\n",
                "### Your Mission (Step-by-Step)\n",
                "\n",
                "**STEP 1: Create Your Dataset**\n",
                "```python\n",
                "from datasets import Dataset\n",
                "\n",
                "mental_health_data = [\n",
                "    {\"question\": \"I feel sad.\", \"answer\": \"I'm sorry to hear that. It's okay to feel down sometimes. Do you want to talk about it?\"},\n",
                "    {\"question\": \"I am anxious.\", \"answer\": \"Take a deep breath. Anxiety is tough, but you are not alone. Let's focus on the present moment.\"},\n",
                "    {\"question\": \"Nobody likes me.\", \"answer\": \"That must be a painful thought. I care about you, and I'm sure others do too, even if it's hard to see right now.\"},\n",
                "    {\"question\": \"I can't sleep.\", \"answer\": \"Trouble sleeping is really difficult. Have you tried any relaxation techniques? I'm here to help.\"},\n",
                "    {\"question\": \"I feel overwhelmed.\", \"answer\": \"It's completely understandable to feel that way. Let's break things down together, one step at a time.\"}\n",
                "]\n",
                "\n",
                "mh_dataset = Dataset.from_list(mental_health_data)\n",
                "print(f\"Created dataset with {len(mh_dataset)} examples\")\n",
                "```\n",
                "**STEP 2: Modify the Formatting Function**\n",
                "\n",
                "The key difference: Add a system message to set the tone!\n",
                "```python\n",
                "def empathetic_format(example):\n",
                "    text = f\"\"\"You are a caring, empathetic friend who listens without judgment.\n",
                "\n",
                "### Question:\n",
                "{example['question']}\n",
                "\n",
                "### Answer:\n",
                "{example['answer']}\"\"\"\n",
                "    return text\n",
                "\n",
                "# Test it:\n",
                "print(empathetic_format(mh_dataset[0]))\n",
                "```\n",
                "**STEP 3: Train the Model**\n",
                "```python\n",
                "# Same LoRA config as before\n",
                "\n",
                "empathetic_training_args = SFTConfig(\n",
                "    output_dir=\"./tinyllama-empathetic\",\n",
                "    num_train_epochs=3, # Note: More epochs for small dataset!\n",
                "    per_device_train_batch_size=1, # Smaller because examples are longer\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=5,\n",
                "    bf16=True,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_length=512,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "empathetic_trainer = SFTTrainer(\n",
                "    model=model, # Start from your medical model or reload base\n",
                "    train_dataset=mh_dataset,\n",
                "    peft_config=lora_config,\n",
                "    formatting_func=empathetic_format,\n",
                "    args=empathetic_training_args,\n",
                "    processing_class=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Training empathetic model...\")\n",
                "empathetic_trainer.train()\n",
                "```\n",
                "**STEP 4: Test Your Creation**\n",
                "```python\n",
                "def ask_empathetic(question):\n",
                "    prompt = f\"\"\"You are a caring, empathetic friend who listens without judgment.\n",
                "\n",
                "### Question:\n",
                "{question}\n",
                "\n",
                "### Answer:\n",
                "\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=150, # Longer for empathetic responses\n",
                "        do_sample=True,\n",
                "        temperature=0.8 # Slightly higher for warmth\n",
                "    )\n",
                "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "\n",
                "# Try it!\n",
                "ask_empathetic(\"I'm worried about my exams.\")\n",
                "ask_empathetic(\"I had a bad day.\")\n",
                "```\n",
                "### ü§î Discussion Questions\n",
                "\n",
                "1. **How does the tone differ from the medical model?**\n",
                "   - Medical: factual, clinical\n",
                "   - Empathetic: warm, validating\n",
                "\n",
                "2. **Could we combine both?**\n",
                "   - Yes! Train on both datasets\n",
                "   - Model would be medically accurate AND empathetic\n",
                "\n",
                "3. **What are the risks?**\n",
                "   - AI should not replace professional mental health care\n",
                "   - Could give harmful advice if not carefully supervised\n",
                "   - Important to include disclaimers\n",
                "\n",
                "### üåü Extension Ideas\n",
                "\n",
                "1. **Add more training data:**\n",
                "   - Find mental health conversation datasets\n",
                "   - Create 100+ examples for better quality\n",
                "\n",
                "2. **Add safety guardrails:**\n",
                "   - Train model to suggest professional help for serious issues\n",
                "   - Add crisis hotline information to responses\n",
                "\n",
                "3. **Combine with RAG:**\n",
                "   - Fine-tune for empathetic tone\n",
                "   - Use RAG to pull from verified mental health resources\n",
                "\n",
                "4. **Multi-turn conversations:**\n",
                "   - Most mental health chats are multi-turn\n",
                "   - Challenge: Modify the format to include conversation history\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîç Quick Diagnosis Flowchart\n",
                "\n",
                "Use this to quickly identify what's wrong:\n",
                "\n",
                "```\n",
                "Problem: Model outputs gibberish or nonsense?\n",
                "‚îÇ\n",
                "‚îî‚îÄ Is loss very low (<0.3)?\n",
                "   ‚îú‚îÄ YES ‚Üí Overfitting! Train fewer epochs or get more data\n",
                "   ‚îî‚îÄ NO ‚Üí Is loss very high (>2.0)?\n",
                "      ‚îú‚îÄ YES ‚Üí Didn't train enough or learning rate wrong\n",
                "      ‚îî‚îÄ NO ‚Üí Check data formatting (print formatting_func output)\n",
                "\n",
                "Problem: Model just repeats the question?\n",
                "‚îÇ\n",
                "‚îî‚îÄ Check: Does your formatting function include \"### Answer:\" marker?\n",
                "   ‚îú‚îÄ NO ‚Üí Fix formatting function\n",
                "   ‚îî‚îÄ YES ‚Üí Train for more steps (increase num_train_epochs)\n",
                "\n",
                "Problem: \"Out of Memory\" error?\n",
                "‚îÇ\n",
                "‚îî‚îÄ Reduce per_device_train_batch_size to 1\n",
                "   ‚îî‚îÄ Still failing? ‚Üí Reduce r=8 in LoRA config\n",
                "      ‚îî‚îÄ Still failing? ‚Üí Reduce max_length=256\n",
                "\n",
                "Problem: Training is too slow (>60min)?\n",
                "‚îÇ\n",
                "‚îî‚îÄ For testing: Use only 100 examples instead of 500\n",
                "   dataset = dataset.select(range(100))\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Troubleshooting Common Issues\n",
                "\n",
                "### Issue 1: \"CUDA Out of Memory\"\n",
                "\n",
                "**Problem:** GPU runs out of RAM\n",
                "\n",
                "**Solutions:**\n",
                "```python\n",
                "# Option A: Reduce batch size\n",
                "per_device_train_batch_size=1 # Down from 2\n",
                "\n",
                "# Option B: Reduce LoRA rank\n",
                "r=8 # Down from 16\n",
                "\n",
                "# Option C: Reduce max_length\n",
                "max_length=256 # Down from 512\n",
                "\n",
                "# Option D: Enable gradient checkpointing\n",
                "model.gradient_checkpointing_enable()\n",
                "```\n",
                "### Issue 2: Model Outputs Gibberish\n",
                "\n",
                "**Problem:** Loss decreased but outputs are nonsensical\n",
                "\n",
                "**Likely Causes:**\n",
                "- Learning rate too high\n",
                "- Trained for too many epochs (overfit)\n",
                "- Bad data formatting\n",
                "\n",
                "**Solutions:**\n",
                "```python\n",
                "# Lower learning rate\n",
                "learning_rate=1e-4 # Down from 2e-4\n",
                "\n",
                "# Fewer epochs\n",
                "num_train_epochs=1 # Down from 3\n",
                "\n",
                "# Check your formatting function output manually\n",
                "print(formatting_func(dataset[0]))\n",
                "```\n",
                "### Issue 3: Model Just Repeats the Question\n",
                "\n",
                "**Problem:** Output looks like:\n",
                "```\n",
                "### Question:\n",
                "What is diabetes?\n",
                "### Answer:\n",
                "What is diabetes?\n",
                "```\n",
                "**Cause:** Model hasn't learned where answers should go\n",
                "\n",
                "**Solutions:**\n",
                "- Check formatting function includes \"### Answer:\" marker\n",
                "- Train for more steps\n",
                "- Increase dataset size\n",
                "\n",
                "### Issue 4: Training is Extremely Slow\n",
                "\n",
                "**Speed Benchmarks:**\n",
                "- **GPU (NVIDIA):** 10-15 minutes for 500 examples\n",
                "- **Mac M1/M2:** 30-45 minutes\n",
                "- **CPU Only:** 60-120 minutes\n",
                "\n",
                "**Speed-up Tips:**\n",
                "```python\n",
                "# 1. Reduce number of examples for testing\n",
                "dataset = dataset.select(range(100)) # Just 100 examples\n",
                "\n",
                "# 2. Increase batch size (if memory allows)\n",
                "per_device_train_batch_size=4\n",
                "\n",
                "# 3. Reduce gradient accumulation steps\n",
                "gradient_accumulation_steps=2\n",
                "\n",
                "# 4. Use fewer logging steps\n",
                "logging_steps=50 # Log less frequently\n",
                "```\n",
                "### Issue 5: \"ImportError: bitsandbytes not found\"\n",
                "\n",
                "**On Mac:** This is expected! Use this instead:\n",
                "```python\n",
                "# For Mac, use this loading code:\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "model.to(\"mps\") # Move to Mac GPU\n",
                "```\n",
                "### Issue 6: Model Refuses to Follow Fine-Tuning\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "1.  **LoRA** allows us to fine-tune significantly faster by freezing the main model.\n",
                "2.  **Quantization** is great for NVIDIA GPUs, but smaller models (1B) run fine on Mac/CPU in `float16`.\n",
                "3.  **Data Formatting** is critical in teaching the model *how* to speak (e.g., Q&A format)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}