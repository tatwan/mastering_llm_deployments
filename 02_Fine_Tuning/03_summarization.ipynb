{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text Summarization with DialogSum\n",
                "\n",
                "**Module 02 | Notebook 3 of 3**\n",
                "\n",
                "In this notebook, we'll fine-tune a sequence-to-sequence model for dialogue summarization.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Work with encoder-decoder models (T5, BART)\n",
                "2. Prepare data for seq2seq tasks\n",
                "3. Use ROUGE metrics for evaluation\n",
                "4. Generate summaries from dialogue\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate rouge-score nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
                "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
                "    DataCollatorForSeq2Seq\n",
                ")\n",
                "from datasets import load_dataset\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import nltk\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Download NLTK data for sentence tokenization\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding Sequence-to-Sequence Models\n",
                "\n",
                "### Why T5 (Encoder-Decoder)?\n",
                "Unlike BERT (Encoder-only, good for classification) or GPT (Decoder-only, good for generation), **T5 is an Encoder-Decoder model**.\n",
                "\n",
                "1.  **Encoder**: Reads the entire input text (dialogue) and understands the context.\n",
                "2.  **Decoder**: Generates the output text (summary) word by word, attending to the encoder's understanding.\n",
                "\n",
                "This makes it perfect for tasks where the input and output are different texts (Summarization, Translation).\n",
                "\n",
                "### How Summarization Works\n",
                "\n",
                "```\n",
                "Input (Dialogue):\n",
                "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "\u2502 Person A: Hi, how are you?                  \u2502\n",
                "\u2502 Person B: I'm great! Just got promoted.     \u2502\n",
                "\u2502 Person A: Congratulations! That's amazing!  \u2502\n",
                "\u2502 Person B: Thanks! Let's celebrate tonight.  \u2502\n",
                "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                    \u2502\n",
                "                    \u25bc\n",
                "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "            \u2502    ENCODER    \u2502  (Understands input)\n",
                "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                    \u2502\n",
                "                    \u25bc\n",
                "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "            \u2502    DECODER    \u2502  (Generates output)\n",
                "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                    \u2502\n",
                "                    \u25bc\n",
                "Output (Summary):\n",
                "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "\u2502 Person B got promoted and they plan to      \u2502\n",
                "\u2502 celebrate tonight.                          \u2502\n",
                "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "### Popular Summarization Models\n",
                "\n",
                "| Model | Parameters | Best For |\n",
                "|-------|------------|----------|\n",
                "| T5-small | 60M | Quick experiments |\n",
                "| T5-base | 220M | Good balance |\n",
                "| BART-base | 140M | News articles |\n",
                "| FLAN-T5 | Various | Instruction-tuned |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load the DialogSum Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DialogSum dataset\n",
                "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
                "\n",
                "print(\"Dataset structure:\")\n",
                "print(dataset)\n",
                "print(f\"\\nTrain examples: {len(dataset['train']):,}\")\n",
                "print(f\"Validation examples: {len(dataset['validation']):,}\")\n",
                "print(f\"Test examples: {len(dataset['test']):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the data\n",
                "print(\"Sample dialogue and summary:\")\n",
                "print(\"=\" * 60)\n",
                "example = dataset['train'][0]\n",
                "\n",
                "print(\"DIALOGUE:\")\n",
                "print(example['dialogue'])\n",
                "print(\"\\nSUMMARY:\")\n",
                "print(example['summary'])\n",
                "print(\"\\nTOPIC:\")\n",
                "print(example.get('topic', 'N/A'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze lengths\n",
                "dialogue_lengths = [len(d.split()) for d in dataset['train']['dialogue']]\n",
                "summary_lengths = [len(s.split()) for s in dataset['train']['summary']]\n",
                "\n",
                "print(\"Length Statistics (words):\")\n",
                "print(f\"\\nDialogues:\")\n",
                "print(f\"  Mean: {np.mean(dialogue_lengths):.0f}\")\n",
                "print(f\"  Max: {max(dialogue_lengths)}\")\n",
                "\n",
                "print(f\"\\nSummaries:\")\n",
                "print(f\"  Mean: {np.mean(summary_lengths):.0f}\")\n",
                "print(f\"  Max: {max(summary_lengths)}\")\n",
                "\n",
                "# Compression ratio\n",
                "ratios = [s/d for s, d in zip(summary_lengths, dialogue_lengths) if d > 0]\n",
                "print(f\"\\nCompression ratio: {np.mean(ratios):.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Data Preparation for Seq2Seq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model and tokenizer\n",
                "model_name = \"t5-small\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# T5 uses a prefix for summarization\n",
                "prefix = \"summarize: \"\n",
                "\n",
                "# Tokenization parameters\n",
                "max_input_length = 512\n",
                "max_target_length = 128"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    \"\"\"Tokenize inputs and targets for seq2seq training.\"\"\"\n",
                "    # Add prefix to inputs\n",
                "    inputs = [prefix + doc for doc in examples['dialogue']]\n",
                "    \n",
                "    # Tokenize inputs\n",
                "    model_inputs = tokenizer(\n",
                "        inputs,\n",
                "        max_length=max_input_length,\n",
                "        truncation=True,\n",
                "        padding=True\n",
                "    )\n",
                "    \n",
                "    # Tokenize targets (summaries)\n",
                "    labels = tokenizer(\n",
                "        examples['summary'],\n",
                "        max_length=max_target_length,\n",
                "        truncation=True,\n",
                "        padding=True\n",
                "    )\n",
                "    \n",
                "    model_inputs['labels'] = labels['input_ids']\n",
                "    \n",
                "    return model_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use smaller subsets for faster training\n",
                "train_size = 1000\n",
                "val_size = 200\n",
                "\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "val_data = dataset['validation'].shuffle(seed=42).select(range(val_size))\n",
                "\n",
                "# Tokenize\n",
                "train_tokenized = train_data.map(preprocess_function, batched=True)\n",
                "val_tokenized = val_data.map(preprocess_function, batched=True)\n",
                "\n",
                "print(f\"Training samples: {len(train_tokenized)}\")\n",
                "print(f\"Validation samples: {len(val_tokenized)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding ROUGE Metrics\n",
                "\n",
                "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures summary quality by checking overlaps.\n",
                "\n",
                "### Visual Example\n",
                "**Reference**: \"The **cat** is **on** the **mat**.\"\n",
                "**Generated**: \"The **cat** is **on**.\"\n",
                "\n",
                "1.  **ROUGE-1 (Unigrams)**: Matches single words.\n",
                "    *   Matches: \"The\", \"cat\", \"is\", \"on\" (4 words).\n",
                "    *   Score: 4/6 (Recall)\n",
                "\n",
                "2.  **ROUGE-2 (Bigrams)**: Matches pairs.\n",
                "    *   Matches: \"The cat\", \"cat is\", \"is on\" (3 pairs).\n",
                "    *   Score: 3/5 (Recall)\n",
                "\n",
                "3.  **ROUGE-L**: Longest Common Subsequence (Structure).\n",
                "    *   Checks for the longest sequence of words that appear in both."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load ROUGE metric\n",
                "rouge = evaluate.load(\"rouge\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    \"\"\"\n",
                "    Compute ROUGE metrics for summarization evaluation.\n",
                "    \n",
                "    Note on Token Handling:\n",
                "    -----------------------\n",
                "    When using Seq2SeqTrainer with predict_with_generate=True, the predictions \n",
                "    and labels arrays come as numpy int64 values. The HuggingFace Fast Tokenizer \n",
                "    (Rust backend) can throw an OverflowError when decoding these values directly.\n",
                "    \n",
                "    This happens because:\n",
                "    1. Padding tokens are marked as -100 (a convention in HuggingFace for ignore_index)\n",
                "    2. Numpy int64 values may not convert cleanly to Rust's integer types\n",
                "    3. Token IDs outside the valid vocabulary range cause decoding issues\n",
                "    \n",
                "    The fix below explicitly:\n",
                "    - Converts each token to a Python int using int(tok)\n",
                "    - Validates tokens are within [0, vocab_size) range\n",
                "    - Replaces invalid tokens (including -100) with pad_token_id\n",
                "    \"\"\"\n",
                "    predictions, labels = eval_pred\n",
                "    \n",
                "    # Get pad token id and vocab size for validation\n",
                "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
                "    vocab_size = tokenizer.vocab_size\n",
                "    \n",
                "    # Clean predictions: convert to Python int and replace invalid values\n",
                "    predictions_clean = []\n",
                "    for row in predictions:\n",
                "        clean_row = []\n",
                "        for tok in row:\n",
                "            tok_int = int(tok)  # Explicitly convert to Python int\n",
                "            if tok_int < 0 or tok_int >= vocab_size:\n",
                "                clean_row.append(pad_id)\n",
                "            else:\n",
                "                clean_row.append(tok_int)\n",
                "        predictions_clean.append(clean_row)\n",
                "    \n",
                "    decoded_preds = tokenizer.batch_decode(predictions_clean, skip_special_tokens=True)\n",
                "    \n",
                "    # Clean labels: convert to Python int and replace -100 padding\n",
                "    labels_clean = []\n",
                "    for row in labels:\n",
                "        clean_row = []\n",
                "        for tok in row:\n",
                "            tok_int = int(tok)  # Explicitly convert to Python int\n",
                "            if tok_int < 0 or tok_int >= vocab_size:\n",
                "                clean_row.append(pad_id)\n",
                "            else:\n",
                "                clean_row.append(tok_int)\n",
                "        labels_clean.append(clean_row)\n",
                "    \n",
                "    decoded_labels = tokenizer.batch_decode(labels_clean, skip_special_tokens=True)\n",
                "    \n",
                "    # Strip whitespace\n",
                "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
                "    decoded_labels = [label.strip() for label in decoded_labels]\n",
                "    \n",
                "    # Compute ROUGE scores\n",
                "    result = rouge.compute(\n",
                "        predictions=decoded_preds,\n",
                "        references=decoded_labels,\n",
                "        use_stemmer=True\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        'rouge1': result['rouge1'],\n",
                "        'rouge2': result['rouge2'],\n",
                "        'rougeL': result['rougeL']\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Model Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> \u26a0\ufe0f **Memory Warning**: T5 training uses ~6-8 GB GPU memory. If you get an **OOM (Out of Memory) Error**:\n",
                "> - Reduce `per_device_train_batch_size` from 8 \u2192 4 \u2192 2\n",
                "> - Reduce `train_size` to 500 in the data preparation step above\n",
                "> - T5-small is already the lightest option; avoid T5-base on free Colab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seq2Seq training arguments\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=\"./summarization_model\",\n",
                "    num_train_epochs=3,              # \ud83d\udcda Full passes through training data\n",
                "    per_device_train_batch_size=8,   # \ud83d\udcbe Reduce to 4 or 2 if OOM (T5 is memory-hungry)\n",
                "    per_device_eval_batch_size=8,\n",
                "    learning_rate=3e-5,              # \ud83c\udfaf T5 likes slightly higher LR (1e-4 to 3e-5)\n",
                "    weight_decay=0.01,               # \ud83d\udee1\ufe0f Helps prevent overfitting\n",
                "    warmup_ratio=0.1,                # \ud83d\udd25 Gradual LR ramp-up (10% of training steps)\n",
                "    \n",
                "    # Generation during evaluation\n",
                "    predict_with_generate=True,      # \ud83d\udd2e Actually generate text during eval (slower but needed for ROUGE)\n",
                "    generation_max_length=max_target_length,\n",
                "    \n",
                "    # Evaluation\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"rougeL\",\n",
                "    \n",
                "    # Logging\n",
                "    logging_steps=50,\n",
                "    \n",
                "    # Performance\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "# Data collator for seq2seq\n",
                "data_collator = DataCollatorForSeq2Seq(\n",
                "    tokenizer=tokenizer,\n",
                "    model=model\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_tokenized,\n",
                "    eval_dataset=val_tokenized,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train\n",
                "print(\"Starting training...\")\n",
                "print(\"=\" * 50)\n",
                "trainer.train()\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "eval_results = trainer.evaluate()\n",
                "\n",
                "print(\"Evaluation Results:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"ROUGE-1: {eval_results['eval_rouge1']:.2%}\")\n",
                "print(f\"ROUGE-2: {eval_results['eval_rouge2']:.2%}\")\n",
                "print(f\"ROUGE-L: {eval_results['eval_rougeL']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> \ud83d\udcca **Are these ROUGE scores good?**\n",
                "> - ROUGE-1 ~30-40% is typical for abstractive summarization\n",
                "> - ROUGE-L ~25-35% means decent structural similarity\n",
                "> - Our DialogSum scores: ROUGE-1 ~40%, ROUGE-L ~35% \u2713\n",
                "> \n",
                "> **Important**: ROUGE isn't perfect! A summary can be great but score low if it uses different words than the reference. Human evaluation is often needed for final quality assessment."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> \ud83d\udd27 **Troubleshooting:**\n",
                "> - **Summaries are too short/repetitive?** \u2192 Increase `min_length`, try `num_beams=4`\n",
                "> - **Summaries don't make sense?** \u2192 Check training data quality or add more examples\n",
                "> - **ROUGE dropping after epoch 2?** \u2192 Overfitting! Reduce epochs to 2 or add more training data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Generate Summaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Create summarization pipeline\n",
                "summarizer = pipeline(\n",
                "    \"summarization\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on examples from the test set\n",
                "test_examples = dataset['test'].select(range(3))\n",
                "\n",
                "print(\"Generated Summaries:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for i, example in enumerate(test_examples):\n",
                "    dialogue = example['dialogue']\n",
                "    reference = example['summary']\n",
                "    \n",
                "    # Generate summary\n",
                "    generated = summarizer(\n",
                "        prefix + dialogue,\n",
                "        max_length=128,\n",
                "        min_length=20,\n",
                "        do_sample=False\n",
                "    )[0]['summary_text']\n",
                "    \n",
                "    print(f\"\\n--- Example {i+1} ---\")\n",
                "    print(f\"\\nDIALOGUE (truncated):\")\n",
                "    print(dialogue[:300] + \"...\")\n",
                "    print(f\"\\nREFERENCE SUMMARY:\")\n",
                "    print(reference)\n",
                "    print(f\"\\nGENERATED SUMMARY:\")\n",
                "    print(generated)\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on custom dialogue\n",
                "custom_dialogue = \"\"\"\n",
                "#Person1#: Hey Sarah, have you finished the project report?\n",
                "#Person2#: Almost done! Just need to add the conclusion section.\n",
                "#Person1#: Great! The client meeting is tomorrow at 2 PM.\n",
                "#Person2#: I'll have it ready by noon so you can review it.\n",
                "#Person1#: Perfect. Also, can you include the budget projections?\n",
                "#Person2#: Already added those. I also updated the timeline chart.\n",
                "#Person1#: You're a lifesaver! Thanks so much.\n",
                "#Person2#: No problem! See you tomorrow.\n",
                "\"\"\"\n",
                "\n",
                "summary = summarizer(\n",
                "    prefix + custom_dialogue,\n",
                "    max_length=100,\n",
                "    min_length=20,\n",
                "    do_sample=False\n",
                ")[0]['summary_text']\n",
                "\n",
                "print(\"Custom Dialogue Summarization:\")\n",
                "print(\"=\" * 50)\n",
                "print(\"\\nINPUT:\")\n",
                "print(custom_dialogue)\n",
                "print(\"\\nSUMMARY:\")\n",
                "print(summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Generation Parameters\n",
                "\n",
                "You can control how summaries are generated:\n",
                "\n",
                "| Parameter | Effect |\n",
                "|-----------|--------|\n",
                "| `num_beams` | Example: 4. The model keeps the 4 best paths at each step. Better quality, but slower. |\n",
                "| `temperature` | Randomness. Low (0.1) = repetitive/safe. High (0.9) = creative/risky. |\n",
                "| `do_sample` | True = allows multiple different outputs. False = always same output. |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_dialogue = dataset['test'][0]['dialogue']\n",
                "\n",
                "# Different generation strategies\n",
                "print(\"Generation Strategies Comparison:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Greedy (default)\n",
                "greedy = summarizer(prefix + test_dialogue, max_length=100, do_sample=False)[0]['summary_text']\n",
                "print(f\"\\nGreedy: {greedy}\")\n",
                "\n",
                "# Beam search\n",
                "beam = summarizer(prefix + test_dialogue, max_length=100, num_beams=4, do_sample=False)[0]['summary_text']\n",
                "print(f\"\\nBeam Search (4): {beam}\")\n",
                "\n",
                "# Sampling\n",
                "sampled = summarizer(prefix + test_dialogue, max_length=100, do_sample=True, top_k=50, temperature=0.7)[0]['summary_text']\n",
                "print(f\"\\nSampling (temp=0.7): {sampled}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udfaf Student Challenge\n",
                "\n",
                "### Challenge: Fine-tune on News Articles (CNN/DailyMail)\n",
                "\n",
                "We trained on dialogue. Now, try adapting the code for news summarization using the famous CNN/DailyMail dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Student Solution\n",
                "\n",
                "# 1. Load the dataset (It's large, so we just take a tiny slice for the challenge)\n",
                "# cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:100]\")\n",
                "\n",
                "# 2. Inspect the columns - Note: They are different from DialogSum!\n",
                "# print(cnn_dataset.column_names) \n",
                "# Expected: ['article', 'highlights', 'id']\n",
                "\n",
                "# 3. Create a NEW preprocess function\n",
                "# def preprocess_news(examples):\n",
                "#     inputs = [prefix + doc for doc in examples['article']] # Use 'article' column\n",
                "#     ...\n",
                "#     labels = tokenizer(examples['highlights'], ...) # Use 'highlights' column\n",
                "#     return model_inputs\n",
                "\n",
                "# 4. Apply mapping and Train\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Seq2Seq models** have separate encoder and decoder components\n",
                "2. **T5 uses prefixes** (\"summarize:\", \"translate:\") to specify tasks\n",
                "3. **ROUGE metrics** measure n-gram overlap between generated and reference summaries\n",
                "4. **Generation parameters** control output quality and diversity\n",
                "5. **Beam search** usually produces better results than greedy decoding\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to Module 03: **Model Optimization**\n",
                "- `03_Model_Optimization/01_intro_to_optimization.ipynb`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}