{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text Summarization with DialogSum\n",
                "\n",
                "**Module 02 | Notebook 3 of 3**\n",
                "\n",
                "In this notebook, we'll fine-tune a sequence-to-sequence model for dialogue summarization.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Work with encoder-decoder models (T5, BART)\n",
                "2. Prepare data for seq2seq tasks\n",
                "3. Use ROUGE metrics for evaluation\n",
                "4. Generate summaries from dialogue\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate rouge-score nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
                "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
                "    DataCollatorForSeq2Seq\n",
                ")\n",
                "from datasets import load_dataset\n",
                "import evaluate\n",
                "import numpy as np\n",
                "import nltk\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Download NLTK data for sentence tokenization\n",
                "nltk.download('punkt', quiet=True)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding Sequence-to-Sequence Models\n",
                "\n",
                "### How Summarization Works\n",
                "\n",
                "```\n",
                "Input (Dialogue):\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚ Person A: Hi, how are you?                  â”‚\n",
                "â”‚ Person B: I'm great! Just got promoted.     â”‚\n",
                "â”‚ Person A: Congratulations! That's amazing!  â”‚\n",
                "â”‚ Person B: Thanks! Let's celebrate tonight.  â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                    â”‚\n",
                "                    â–¼\n",
                "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "            â”‚    ENCODER    â”‚  (Understands input)\n",
                "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                    â”‚\n",
                "                    â–¼\n",
                "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "            â”‚    DECODER    â”‚  (Generates output)\n",
                "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                    â”‚\n",
                "                    â–¼\n",
                "Output (Summary):\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚ Person B got promoted and they plan to      â”‚\n",
                "â”‚ celebrate tonight.                          â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "### Popular Summarization Models\n",
                "\n",
                "| Model | Parameters | Best For |\n",
                "|-------|------------|----------|\n",
                "| T5-small | 60M | Quick experiments |\n",
                "| T5-base | 220M | Good balance |\n",
                "| BART-base | 140M | News articles |\n",
                "| FLAN-T5 | Various | Instruction-tuned |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Load the DialogSum Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DialogSum dataset\n",
                "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
                "\n",
                "print(\"Dataset structure:\")\n",
                "print(dataset)\n",
                "print(f\"\\nTrain examples: {len(dataset['train'])}\")\n",
                "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
                "print(f\"Test examples: {len(dataset['test'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the data\n",
                "print(\"Sample dialogue and summary:\")\n",
                "print(\"=\" * 60)\n",
                "example = dataset['train'][0]\n",
                "\n",
                "print(\"DIALOGUE:\")\n",
                "print(example['dialogue'])\n",
                "print(\"\\nSUMMARY:\")\n",
                "print(example['summary'])\n",
                "print(\"\\nTOPIC:\")\n",
                "print(example.get('topic', 'N/A'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze lengths\n",
                "dialogue_lengths = [len(d.split()) for d in dataset['train']['dialogue']]\n",
                "summary_lengths = [len(s.split()) for s in dataset['train']['summary']]\n",
                "\n",
                "print(\"Length Statistics (words):\")\n",
                "print(f\"\\nDialogues:\")\n",
                "print(f\"  Mean: {np.mean(dialogue_lengths):.0f}\")\n",
                "print(f\"  Max: {max(dialogue_lengths)}\")\n",
                "\n",
                "print(f\"\\nSummaries:\")\n",
                "print(f\"  Mean: {np.mean(summary_lengths):.0f}\")\n",
                "print(f\"  Max: {max(summary_lengths)}\")\n",
                "\n",
                "# Compression ratio\n",
                "ratios = [s/d for s, d in zip(summary_lengths, dialogue_lengths) if d > 0]\n",
                "print(f\"\\nCompression ratio: {np.mean(ratios):.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Data Preparation for Seq2Seq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model and tokenizer\n",
                "model_name = \"t5-small\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# T5 uses a prefix for summarization\n",
                "prefix = \"summarize: \"\n",
                "\n",
                "# Tokenization parameters\n",
                "max_input_length = 512\n",
                "max_target_length = 128"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    \"\"\"Tokenize inputs and targets for seq2seq training.\"\"\"\n",
                "    # Add prefix to inputs\n",
                "    inputs = [prefix + doc for doc in examples['dialogue']]\n",
                "    \n",
                "    # Tokenize inputs\n",
                "    model_inputs = tokenizer(\n",
                "        inputs,\n",
                "        max_length=max_input_length,\n",
                "        truncation=True,\n",
                "        padding=True\n",
                "    )\n",
                "    \n",
                "    # Tokenize targets (summaries)\n",
                "    labels = tokenizer(\n",
                "        examples['summary'],\n",
                "        max_length=max_target_length,\n",
                "        truncation=True,\n",
                "        padding=True\n",
                "    )\n",
                "    \n",
                "    model_inputs['labels'] = labels['input_ids']\n",
                "    \n",
                "    return model_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use smaller subsets for faster training\n",
                "train_size = 1000\n",
                "val_size = 200\n",
                "\n",
                "train_data = dataset['train'].shuffle(seed=42).select(range(train_size))\n",
                "val_data = dataset['validation'].shuffle(seed=42).select(range(val_size))\n",
                "\n",
                "# Tokenize\n",
                "train_tokenized = train_data.map(preprocess_function, batched=True)\n",
                "val_tokenized = val_data.map(preprocess_function, batched=True)\n",
                "\n",
                "print(f\"Training samples: {len(train_tokenized)}\")\n",
                "print(f\"Validation samples: {len(val_tokenized)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding ROUGE Metrics\n",
                "\n",
                "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures summary quality:\n",
                "\n",
                "| Metric | What it Measures |\n",
                "|--------|------------------|\n",
                "| ROUGE-1 | Unigram (word) overlap |\n",
                "| ROUGE-2 | Bigram (2-word) overlap |\n",
                "| ROUGE-L | Longest common subsequence |\n",
                "\n",
                "Higher scores = better summaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load ROUGE metric\n",
                "rouge = evaluate.load(\"rouge\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    \n",
                "    # Decode predictions\n",
                "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
                "    \n",
                "    # Replace -100 in labels (padding)\n",
                "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "    \n",
                "    # Add newlines for ROUGE calculation\n",
                "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
                "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
                "    \n",
                "    # Compute ROUGE scores\n",
                "    result = rouge.compute(\n",
                "        predictions=decoded_preds,\n",
                "        references=decoded_labels,\n",
                "        use_stemmer=True\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        'rouge1': result['rouge1'],\n",
                "        'rouge2': result['rouge2'],\n",
                "        'rougeL': result['rougeL']\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seq2Seq training arguments\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=\"./summarization_model\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=8,\n",
                "    per_device_eval_batch_size=8,\n",
                "    learning_rate=3e-5,\n",
                "    weight_decay=0.01,\n",
                "    warmup_ratio=0.1,\n",
                "    \n",
                "    # Generation during evaluation\n",
                "    predict_with_generate=True,\n",
                "    generation_max_length=max_target_length,\n",
                "    \n",
                "    # Evaluation\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"rougeL\",\n",
                "    \n",
                "    # Logging\n",
                "    logging_steps=50,\n",
                "    \n",
                "    # Performance\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "# Data collator for seq2seq\n",
                "data_collator = DataCollatorForSeq2Seq(\n",
                "    tokenizer=tokenizer,\n",
                "    model=model\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_tokenized,\n",
                "    eval_dataset=val_tokenized,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train\n",
                "print(\"Starting training...\")\n",
                "print(\"=\" * 50)\n",
                "trainer.train()\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "eval_results = trainer.evaluate()\n",
                "\n",
                "print(\"Evaluation Results:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"ROUGE-1: {eval_results['eval_rouge1']:.2%}\")\n",
                "print(f\"ROUGE-2: {eval_results['eval_rouge2']:.2%}\")\n",
                "print(f\"ROUGE-L: {eval_results['eval_rougeL']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Generate Summaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Create summarization pipeline\n",
                "summarizer = pipeline(\n",
                "    \"summarization\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on examples from the test set\n",
                "test_examples = dataset['test'].select(range(3))\n",
                "\n",
                "print(\"Generated Summaries:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for i, example in enumerate(test_examples):\n",
                "    dialogue = example['dialogue']\n",
                "    reference = example['summary']\n",
                "    \n",
                "    # Generate summary\n",
                "    generated = summarizer(\n",
                "        prefix + dialogue,\n",
                "        max_length=128,\n",
                "        min_length=20,\n",
                "        do_sample=False\n",
                "    )[0]['summary_text']\n",
                "    \n",
                "    print(f\"\\n--- Example {i+1} ---\")\n",
                "    print(f\"\\nDIALOGUE (truncated):\")\n",
                "    print(dialogue[:300] + \"...\")\n",
                "    print(f\"\\nREFERENCE SUMMARY:\")\n",
                "    print(reference)\n",
                "    print(f\"\\nGENERATED SUMMARY:\")\n",
                "    print(generated)\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on custom dialogue\n",
                "custom_dialogue = \"\"\"\n",
                "#Person1#: Hey Sarah, have you finished the project report?\n",
                "#Person2#: Almost done! Just need to add the conclusion section.\n",
                "#Person1#: Great! The client meeting is tomorrow at 2 PM.\n",
                "#Person2#: I'll have it ready by noon so you can review it.\n",
                "#Person1#: Perfect. Also, can you include the budget projections?\n",
                "#Person2#: Already added those. I also updated the timeline chart.\n",
                "#Person1#: You're a lifesaver! Thanks so much.\n",
                "#Person2#: No problem! See you tomorrow.\n",
                "\"\"\"\n",
                "\n",
                "summary = summarizer(\n",
                "    prefix + custom_dialogue,\n",
                "    max_length=100,\n",
                "    min_length=20,\n",
                "    do_sample=False\n",
                ")[0]['summary_text']\n",
                "\n",
                "print(\"Custom Dialogue Summarization:\")\n",
                "print(\"=\" * 50)\n",
                "print(\"\\nINPUT:\")\n",
                "print(custom_dialogue)\n",
                "print(\"\\nSUMMARY:\")\n",
                "print(summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Generation Parameters\n",
                "\n",
                "You can control how summaries are generated:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_dialogue = dataset['test'][0]['dialogue']\n",
                "\n",
                "# Different generation strategies\n",
                "print(\"Generation Strategies Comparison:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Greedy (default)\n",
                "greedy = summarizer(prefix + test_dialogue, max_length=100, do_sample=False)[0]['summary_text']\n",
                "print(f\"\\nGreedy: {greedy}\")\n",
                "\n",
                "# Beam search\n",
                "beam = summarizer(prefix + test_dialogue, max_length=100, num_beams=4, do_sample=False)[0]['summary_text']\n",
                "print(f\"\\nBeam Search (4): {beam}\")\n",
                "\n",
                "# Sampling\n",
                "sampled = summarizer(prefix + test_dialogue, max_length=100, do_sample=True, top_k=50, temperature=0.7)[0]['summary_text']\n",
                "print(f\"\\nSampling (temp=0.7): {sampled}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generation Parameters Explained\n",
                "\n",
                "| Parameter | Effect |\n",
                "|-----------|--------|\n",
                "| `max_length` | Maximum output tokens |\n",
                "| `min_length` | Minimum output tokens |\n",
                "| `num_beams` | Beam search width (higher = better but slower) |\n",
                "| `do_sample` | Enable random sampling |\n",
                "| `temperature` | Randomness (0=deterministic, 1=default, >1=more random) |\n",
                "| `top_k` | Limit sampling to top K tokens |\n",
                "| `top_p` | Nucleus sampling threshold |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Student Challenge\n",
                "\n",
                "### Challenge: Fine-tune for News Summarization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Adapt this notebook for news summarization\n",
                "# 1. Load the CNN/DailyMail dataset: load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
                "# 2. The columns are 'article' (input) and 'highlights' (summary)\n",
                "# 3. Update the preprocess_function\n",
                "# 4. Train and evaluate\n",
                "# 5. Compare ROUGE scores\n",
                "\n",
                "# Hint: CNN/DailyMail articles are longer, you may need max_input_length=1024\n",
                "\n",
                "# Your solution:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Seq2Seq models** have separate encoder and decoder components\n",
                "2. **T5 uses prefixes** (\"summarize:\", \"translate:\") to specify tasks\n",
                "3. **ROUGE metrics** measure n-gram overlap between generated and reference summaries\n",
                "4. **Generation parameters** control output quality and diversity\n",
                "5. **Beam search** usually produces better results than greedy decoding\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to Module 03: **Model Optimization**\n",
                "- `03_Model_Optimization/01_intro_to_optimization.ipynb`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
