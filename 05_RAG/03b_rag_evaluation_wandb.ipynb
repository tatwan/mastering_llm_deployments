{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "evaluation-intro-title",
            "metadata": {},
            "source": [
                "# RAG Evaluation and Observability with Weights & Biases (W&B)\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "\n",
                "1. **Build** a complete RAG pipeline using LangChain v1.0+\n",
                "2. **Understand** why evaluation is critical for production RAG systems\n",
                "3. **Create** a \"Golden Dataset\" for systematic evaluation\n",
                "4. **Use W&B Weave** to track experiments and enable observability\n",
                "5. **Interpret** LLM-as-a-Judge metrics (Faithfulness, Answer Relevance)\n",
                "6. **Debug** RAG failures using per-question analysis\n",
                "7. **Iterate** on RAG configurations using experiment comparison"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "why-evaluate-rag",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Why Evaluate RAG Systems?\n",
                "\n",
                "Before we dive into building, let's understand **why evaluation matters**.\n",
                "\n",
                "### The Problem: \"It Looks Good\" is Not Enough\n",
                "\n",
                "When you test a chatbot manually, you might ask 5-10 questions and think: *\"The answers seem reasonable!\"* But in production:\n",
                "\n",
                "- You can't manually check thousands of queries\n",
                "- Users will ask questions you never anticipated\n",
                "- Small changes (new documents, different LLM) can break things silently\n",
                "\n",
                "### What Can Go Wrong in RAG?\n",
                "\n",
                "| Failure Type | Description | Example |\n",
                "|--------------|-------------|---------|\n",
                "| **Retrieval Failure** | Wrong documents were fetched | User asks about \"Python\" (the language) but retrieves documents about \"python\" (the snake) |\n",
                "| **Hallucination** | LLM invents information not in documents | LLM confidently states a date that doesn't exist in your PDFs |\n",
                "| **Irrelevant Answer** | Answer is technically correct but doesn't address the question | User asks \"How do I install X?\" and gets \"X was developed in 2020...\" |\n",
                "| **Context Window Overflow** | Too many chunks stuffed into prompt | The LLM gets confused or ignores important context |\n",
                "\n",
                "### The Solution: Systematic Evaluation\n",
                "\n",
                "We need:\n",
                "1. **A benchmark** (Golden Dataset) with known correct answers\n",
                "2. **Automated metrics** that can score answers at scale\n",
                "3. **Observability** to trace what happened at each step\n",
                "4. **Experiment tracking** to compare different configurations\n",
                "\n",
                "> üí° **Key Insight**: Evaluation is not just about quality‚Äîit's about **confidence**. You need to know *when* your system will fail, not just hope it won't."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "observability-intro",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üî≠ Introduction to LLM Observability with W&B\n",
                "\n",
                "### What is Observability?\n",
                "\n",
                "**Observability** is the ability to understand what's happening *inside* your system by examining its *outputs*. For LLM applications, this means:\n",
                "\n",
                "- **Traces**: The complete journey of a request (query ‚Üí retrieval ‚Üí generation ‚Üí response)\n",
                "- **Metrics**: Quantitative measurements (latency, token count, quality scores)\n",
                "- **Logs**: Detailed records of inputs, outputs, and intermediate steps\n",
                "\n",
                "### Why Observability for RAG?\n",
                "\n",
                "RAG systems are **multi-step pipelines**. When something goes wrong, you need to know:\n",
                "\n",
                "```\n",
                "User Query ‚Üí [Embedding] ‚Üí [Retrieval] ‚Üí [Prompt Construction] ‚Üí [LLM Generation] ‚Üí Response\n",
                "     ‚Üì            ‚Üì             ‚Üì                ‚Üì                     ‚Üì              ‚Üì\n",
                "  Logged?      Traced?      What docs?      What prompt?          What output?    Scored?\n",
                "```\n",
                "\n",
                "Without observability, debugging is like finding a needle in a haystack.\n",
                "\n",
                "### W&B Weave for LLM Observability\n",
                "\n",
                "**W&B Weave** is Weights & Biases' toolkit for LLM observability and evaluation. It provides:\n",
                "\n",
                "| Feature | Description |\n",
                "|---------|-------------|\n",
                "| **Tracing** | Automatically capture LLM calls and chain executions |\n",
                "| **Evaluation** | Run LLM-as-a-Judge metrics with custom scorers |\n",
                "| **Datasets** | Store and version your evaluation datasets |\n",
                "| **Dashboard** | Visual interface at wandb.ai to explore results |\n",
                "| **Collaboration** | Share experiments with your team |\n",
                "\n",
                "> üìå **In this notebook**, we use W&B Weave's `@weave.op()` decorator and `weave.Evaluation()` to trace RAG executions and score responses with custom LLM-as-a-Judge metrics."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "install-packages-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 0: Install Required Packages\n",
                "\n",
                "Before we begin, we need to install the required packages. This cell installs:\n",
                "\n",
                "- **`langchain`**: The core LangChain framework\n",
                "- **`langchain-community`**: Community integrations (document loaders, etc.)\n",
                "- **`langchain-openai`**: OpenAI-specific components (embeddings, chat models)\n",
                "- **`langchain-chroma`**: Chroma vector store integration\n",
                "- **`pypdf`**: PDF parsing library\n",
                "- **`weave`**: W&B's LLM observability and evaluation toolkit\n",
                "- **`wandb`**: Weights & Biases core package\n",
                "- **`python-dotenv`**: Environment variable management\n",
                "\n",
                "> ‚ö†Ô∏è **Important**: After running this cell, you may need to **restart the kernel** to ensure all packages are properly loaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50791208",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "# !pip install -q langchain langchain-community langchain-openai langchain-chroma pypdf python-dotenv\n",
                "\n",
                "print(\"‚úÖ LangChain packages installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d83dec03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install W&B Weave for LLM evaluation and observability\n",
                "!pip install -q weave wandb\n",
                "\n",
                "print(\"‚úÖ W&B Weave installed - restart kernel if this is your first time\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b741c31",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Environment Setup\n",
                "\n",
                "We need to configure our API keys to authenticate with OpenAI and W&B. This notebook supports both:\n",
                "\n",
                "- **Google Colab**: Uses `google.colab.userdata` to securely access keys stored in Colab Secrets\n",
                "- **Local Execution**: Uses `python-dotenv` to load keys from a `.env` file\n",
                "\n",
                "### Setting Up Your API Keys\n",
                "\n",
                "**For local development**, create a `.env` file in this directory with:\n",
                "```\n",
                "OPENAI_API_KEY=your-openai-key-here\n",
                "WANDB_API_KEY=your-wandb-key-here\n",
                "```\n",
                "\n",
                "**For Colab**, add your keys to Colab Secrets with the names `OPENAI_API_KEY` and `WANDB_API_KEY`.\n",
                "\n",
                "> üí° **Get your W&B API key** at: https://wandb.ai/authorize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f587032c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Configuration\n",
                "MODEL = \"gpt-4o-mini\"  # The LLM model to use\n",
                "db_name = \"vector_db\"  # Directory name for the vector store\n",
                "WANDB_PROJECT = \"rag-evaluation\"  # W&B project name\n",
                "\n",
                "# Option 1: Set your API key directly (for Colab)\n",
                "#from google.colab import userdata\n",
                "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "#os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
                "\n",
                "# Option 2\n",
                "# Load environment variables from .env file\n",
                "# from dotenv import load_dotenv\n",
                "# load_dotenv()\n",
                "\n",
                "# Verify API keys are set\n",
                "if os.environ.get(\"OPENAI_API_KEY\"):\n",
                "    print(\"‚úÖ OPENAI_API_KEY loaded successfully\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not found. Please set it in your .env file or environment.\")\n",
                "\n",
                "if os.environ.get(\"WANDB_API_KEY\"):\n",
                "    print(\"‚úÖ WANDB_API_KEY loaded successfully\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Warning: WANDB_API_KEY not found. Get yours at https://wandb.ai/authorize\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "weave-init",
            "metadata": {},
            "outputs": [],
            "source": [
                "import weave\n",
                "import wandb\n",
                "import pandas as pd\n",
                "\n",
                "# Initialize W&B Weave - this enables tracing and evaluation\n",
                "weave.init(WANDB_PROJECT)\n",
                "\n",
                "print(f\"‚úÖ W&B Weave initialized with project: {WANDB_PROJECT}\")\n",
                "print(f\"üí° View your experiments at: https://wandb.ai/{wandb.api.default_entity}/{WANDB_PROJECT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "architecture-diagram",
            "metadata": {},
            "source": [
                "### RAG Chain Architecture\n",
                "\n",
                "Here's the complete flow of our RAG system:\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ  RAG Chain Flow                                     ‚îÇ\n",
                "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "‚îÇ                                                     ‚îÇ\n",
                "‚îÇ  1. User Question + Chat History                    ‚îÇ\n",
                "‚îÇ     ‚Üì                                               ‚îÇ\n",
                "‚îÇ  2. History-Aware Retriever                         ‚îÇ\n",
                "‚îÇ     (Reformulates question to be standalone)        ‚îÇ\n",
                "‚îÇ     ‚Üì                                               ‚îÇ\n",
                "‚îÇ  3. Vector Store Search (Chroma)                    ‚îÇ\n",
                "‚îÇ     (Finds top-k most similar chunks)               ‚îÇ\n",
                "‚îÇ     ‚Üì                                               ‚îÇ\n",
                "‚îÇ  4. Question-Answer Chain                           ‚îÇ\n",
                "‚îÇ     (LLM generates answer using retrieved context)  ‚îÇ\n",
                "‚îÇ     ‚Üì                                               ‚îÇ\n",
                "‚îÇ  5. Final Response                                  ‚îÇ\n",
                "‚îÇ                                                     ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "**Key Components:**\n",
                "- **History-Aware Retriever**: Handles follow-up questions by reformulating them\n",
                "- **Vector Store**: Stores embeddings and performs semantic search\n",
                "- **Stuff Documents Chain**: \"Stuffs\" all retrieved docs into the LLM prompt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df53314a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Import Dependencies\n",
                "\n",
                "Now we import all the necessary modules from LangChain and other libraries. Here's what each import does:\n",
                "\n",
                "### Document Processing\n",
                "- **`DirectoryLoader`**: Loads multiple files from a directory\n",
                "- **`PyPDFLoader`**: Parses PDF files into text\n",
                "- **`RecursiveCharacterTextSplitter`**: Splits text into chunks while respecting natural boundaries\n",
                "\n",
                "### Embeddings & Vector Store\n",
                "- **`OpenAIEmbeddings`**: Converts text to vector embeddings using OpenAI's models\n",
                "- **`Chroma`**: A fast, open-source vector database\n",
                "\n",
                "### LLM & Chains\n",
                "- **`ChatOpenAI`**: OpenAI's chat models (GPT-4, etc.)\n",
                "- **`create_history_aware_retriever`**: Creates a retriever that understands conversation context\n",
                "- **`create_retrieval_chain`**: Combines retrieval and generation into a single chain\n",
                "- **`create_stuff_documents_chain`**: Creates a chain that \"stuffs\" documents into the prompt\n",
                "\n",
                "### Prompts & Messages\n",
                "- **`ChatPromptTemplate`**: Templates for structured prompts\n",
                "- **`MessagesPlaceholder`**: Placeholder for conversation history\n",
                "- **`HumanMessage` / `AIMessage`**: Message types for chat history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8941dc4",
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "import os\n",
                "import json\n",
                "\n",
                "# Document loading and processing\n",
                "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Embeddings and LLM\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "\n",
                "# Vector store\n",
                "from langchain_chroma import Chroma\n",
                "\n",
                "# Chains for RAG\n",
                "from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain\n",
                "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
                "\n",
                "# Prompts and messages\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e662569",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Load Documents\n",
                "\n",
                "The first step in building a RAG application is loading your documents. We use:\n",
                "\n",
                "- **`glob.glob()`**: To find all PDF files in the `pdfs/` directory and current directory\n",
                "- **`PyPDFLoader`**: To parse each PDF and extract text content\n",
                "\n",
                "### Document Structure\n",
                "\n",
                "Each loaded document contains:\n",
                "- **`page_content`**: The actual text content\n",
                "- **`metadata`**: Information about the document (source file, page number, etc.)\n",
                "\n",
                "> üìÅ **Note**: Place your PDF files in a `pdfs/` subdirectory or in the same directory as this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8da950c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install gdown -q\n",
                "\n",
                "# !gdown 12hCcDOBYO0A3q2eFstMQnNqcCX2WYCZq -O pdfs.zip\n",
                "# !unzip -q pdfs.zip -d pdfs\n",
                "# !rm pdfs.zip\n",
                "# print(\"‚úì PDFs extracted to ./pdfs folder\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "089ba94c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all PDF files in the pdfs/ subdirectory and current directory\n",
                "folders = glob.glob(\"pdfs/*.pdf\") + glob.glob(\"*.pdf\")\n",
                "\n",
                "if not folders:\n",
                "    print(\"‚ö†Ô∏è No PDF files found. Please add PDF files to the 'pdfs/' directory or current directory.\")\n",
                "else:\n",
                "    print(f\"üìÑ Found {len(folders)} PDF file(s)\")\n",
                "\n",
                "# Load all documents\n",
                "documents = []\n",
                "for file_path in folders:\n",
                "    loader = PyPDFLoader(file_path)\n",
                "    docs = loader.load()\n",
                "    for doc in docs:\n",
                "        # Add custom metadata to track source file\n",
                "        doc.metadata[\"source_file\"] = os.path.basename(file_path)\n",
                "        documents.append(doc)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(documents)} pages from {len(folders)} file(s)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24be028a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Split Documents into Chunks\n",
                "\n",
                "LLMs have a **context window limit** (maximum tokens they can process at once). Additionally, for effective retrieval, we want to find *specific* relevant passages, not entire documents.\n",
                "\n",
                "We use **`RecursiveCharacterTextSplitter`** which:\n",
                "- Splits text hierarchically (paragraphs ‚Üí sentences ‚Üí words)\n",
                "- Tries to keep semantically related text together\n",
                "- Creates overlapping chunks to preserve context at boundaries\n",
                "\n",
                "### Key Parameters\n",
                "\n",
                "| Parameter | Value | Description |\n",
                "|-----------|-------|--------------|\n",
                "| `chunk_size` | 1000 | Maximum characters per chunk |\n",
                "| `chunk_overlap` | 200 | Characters shared between adjacent chunks |\n",
                "| `add_start_index` | True | Tracks the position of each chunk in the original document |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c49cc1a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the text splitter\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,       # Max characters per chunk\n",
                "    chunk_overlap=200,     # Overlap between chunks for context continuity\n",
                "    add_start_index=True   # Track position in original document\n",
                ")\n",
                "\n",
                "# Split documents into chunks\n",
                "chunks = text_splitter.split_documents(documents)\n",
                "print(f\"‚úÖ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
                "\n",
                "# Show example chunk\n",
                "if chunks:\n",
                "    print(\"\\nüìù Example Chunk:\")\n",
                "    print(\"-\" * 50)\n",
                "    print(chunks[0].page_content[:300] + \"...\")\n",
                "    print(\"-\" * 50)\n",
                "    print(f\"Metadata: {chunks[0].metadata}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7a04768",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 5: Create Embeddings and Vector Store\n",
                "\n",
                "### What are Embeddings?\n",
                "\n",
                "**Embeddings** are numerical representations (vectors) of text that capture semantic meaning. Texts with similar meanings will have vectors that are close together in the embedding space.\n",
                "\n",
                "### What is a Vector Store?\n",
                "\n",
                "A **Vector Store** is a specialized database optimized for:\n",
                "- Storing high-dimensional vectors\n",
                "- Performing fast similarity searches\n",
                "- Enabling \"semantic search\" (finding text by meaning, not just keywords)\n",
                "\n",
                "### Our Setup\n",
                "\n",
                "- **`OpenAIEmbeddings`**: Uses OpenAI's `text-embedding-3-small` model (fast and cost-effective)\n",
                "- **`Chroma`**: Open-source vector database that persists to disk\n",
                "\n",
                "> üí° **Tip**: The embeddings are stored locally, so subsequent runs will be faster as you won't need to re-embed documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64d4820b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize embedding model\n",
                "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
                "\n",
                "# Clean up existing database if it exists (to ensure fresh data)\n",
                "# NOTE: In production, you would likely load the existing DB mostly.\n",
                "# For this lab, we check if it exists and load it to save time/cost.\n",
                "\n",
                "if os.path.exists(db_name):\n",
                "    # Load existing vector store\n",
                "    vectorstore = Chroma(\n",
                "        persist_directory=db_name, \n",
                "        embedding_function=embeddings\n",
                "    )\n",
                "    print(f\"‚úÖ Loaded existing vector store: {db_name}\")\n",
                "    try:\n",
                "        count = vectorstore._collection.count()\n",
                "        print(f\"üìä Document count: {count}\")\n",
                "    except:\n",
                "        print(\"üìä Could not get document count\")\n",
                "else:\n",
                "    # Create new vector store\n",
                "    print(f\"üÜï Creating new vector store: {db_name}...\")\n",
                "    vectorstore = Chroma.from_documents(\n",
                "        documents=chunks,\n",
                "        embedding=embeddings,\n",
                "        persist_directory=db_name\n",
                "    )\n",
                "    print(f\"‚úÖ Vector store created with {vectorstore._collection.count()} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "228c56ab",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 6: Build the RAG Chain\n",
                "\n",
                "Now we create the complete RAG pipeline using **LangChain Expression Language (LCEL)**. The chain consists of two main components:\n",
                "\n",
                "### 1. History-Aware Retriever\n",
                "\n",
                "This component reformulates the user's question to be **standalone** (understandable without context). \n",
                "\n",
                "**Example:**\n",
                "- Chat history: \"Tell me about SecLM\"\n",
                "- Follow-up: \"What are its main features?\"\n",
                "- Reformulated: \"What are the main features of SecLM?\"\n",
                "\n",
                "### 2. Question-Answer Chain\n",
                "\n",
                "This component:\n",
                "1. Takes the retrieved documents and the question\n",
                "2. \"Stuffs\" the documents into the prompt as context\n",
                "3. Generates a grounded answer using the LLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7655086e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize the LLM\n",
                "llm = ChatOpenAI(temperature=0, model_name=MODEL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d31270eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Create a retriever from the vector store\n",
                "# k=5 means we retrieve the top 5 most relevant chunks\n",
                "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e196646c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Define the contextualization prompt\n",
                "# This prompt helps reformulate questions based on chat history\n",
                "contextualize_q_system_prompt = (\n",
                "    \"Given a chat history and the latest user question \"\n",
                "    \"which might reference context in the chat history, \"\n",
                "    \"formulate a standalone question which can be understood \"\n",
                "    \"without the chat history. Do NOT answer the question, \"\n",
                "    \"just reformulate it if needed and otherwise return it as is.\"\n",
                ")\n",
                "\n",
                "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", contextualize_q_system_prompt),\n",
                "    MessagesPlaceholder(\"chat_history\"),\n",
                "    (\"human\", \"{input}\"),\n",
                "])\n",
                "\n",
                "# Create the history-aware retriever\n",
                "history_aware_retriever = create_history_aware_retriever(\n",
                "    llm, retriever, contextualize_q_prompt\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d569ed2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define the QA prompt\n",
                "# This prompt instructs the LLM how to use the retrieved context\n",
                "qa_system_prompt = (\n",
                "    \"You are an assistant for question-answering tasks. \"\n",
                "    \"Use the following pieces of retrieved context to answer \"\n",
                "    \"the question. If you don't know the answer, just say that you \"\n",
                "    \"don't know. Use three sentences maximum and keep the \"\n",
                "    \"answer concise.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "qa_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", qa_system_prompt),\n",
                "    MessagesPlaceholder(\"chat_history\"),\n",
                "    (\"human\", \"{input}\"),\n",
                "])\n",
                "\n",
                "# Create the question-answer chain\n",
                "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb04d5ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Combine into the final RAG chain\n",
                "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
                "\n",
                "print(\"‚úÖ RAG chain created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f48e93f6",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 7: Test the RAG Chain\n",
                "\n",
                "Let's test our RAG chain with a simple query. The chain will:\n",
                "\n",
                "1. Take the user's question\n",
                "2. Retrieve relevant document chunks from the vector store\n",
                "3. Generate a response based on the retrieved context\n",
                "\n",
                "The response object contains:\n",
                "- **`answer`**: The generated response\n",
                "- **`context`**: The retrieved document chunks used to generate the answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e931695",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a traced RAG function using W&B Weave\n",
                "@weave.op()\n",
                "def rag_query(question: str, chat_history: list = None) -> dict:\n",
                "    \"\"\"Execute a RAG query with W&B tracing.\"\"\"\n",
                "    if chat_history is None:\n",
                "        chat_history = []\n",
                "    \n",
                "    response = rag_chain.invoke({\n",
                "        \"input\": question,\n",
                "        \"chat_history\": chat_history\n",
                "    })\n",
                "    \n",
                "    return {\n",
                "        \"answer\": response[\"answer\"],\n",
                "        \"context\": [doc.page_content for doc in response[\"context\"]]\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ RAG query function with W&B tracing created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test-rag",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the RAG chain\n",
                "query = \"What is the main topic of these documents?\"\n",
                "response = rag_query(query)\n",
                "\n",
                "print(\"‚ùì Question:\", query)\n",
                "print(\"\\nüí¨ Answer:\", response[\"answer\"])\n",
                "print(\"\\n‚úÖ This query was traced in W&B! Check your dashboard.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0c9af49",
            "metadata": {},
            "source": [
                "## üìù Step 8: Create Golden Evaluation Dataset\n",
                "\n",
                "### üß† Educational Context: The \"Golden Dataset\"\n",
                "\n",
                "To scientifically evaluate a RAG system, we cannot just \"eyeball\" a few answers. We need a **benchmark**‚Äîoften called a \"Golden Dataset\" or \"Ground Truth\" set.\n",
                "\n",
                "#### What makes a good evaluation dataset?\n",
                "1.  **Diversity**: Questions should cover different topics within your documents.\n",
                "2.  **Complexity**: Include simple fact lookups (\"What is X?\") and reasoning questions (\"Compare X and Y\").\n",
                "3.  **Ground Truth**: You must provide the *ideal* answer. The LLM Judge will compare the RAG system's output against this reference.\n",
                "\n",
                "**Measurement Goals**:\n",
                "*   **Retrieval Quality**: Did the system find the right page in the PDF?\n",
                "*   **Generation Quality**: Did the LLM answer accurately based on that page?\n",
                "\n",
                "üëá **Action**: The code below creates a list of dictionaries, where each item has a `question` and a `ground_truth` answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5fbee472",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Golden dataset: Questions with ground truth answers\n",
                "# Based on your PDFs: Word Embeddings, BPE, NMT, MTEB\n",
                "eval_data = [\n",
                "    {\n",
                "        \"question\": \"What is Byte Pair Encoding (BPE)?\",\n",
                "        \"ground_truth\": \"BPE is a data compression technique that iteratively replaces the most frequent pair of bytes/symbols with a new symbol. It's used for subword tokenization in NLP tasks like machine translation.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What complexity result did Kozma and Voderholzer prove about optimal pair encoding?\",\n",
                "        \"ground_truth\": \"They proved that optimal pair encoding is APX-complete, meaning it's unlikely to admit a polynomial-time approximation scheme unless P=NP.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the distributional hypothesis in NLP?\",\n",
                "        \"ground_truth\": \"Words that appear in similar contexts tend to have similar meanings. This principle, suggested by Harris (1954), underlies modern word embeddings.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the Vector Space Model (VSM)?\",\n",
                "        \"ground_truth\": \"The VSM represents words and documents as vectors in high-dimensional space, enabling mathematical operations like cosine similarity for information retrieval. Generally attributed to Salton (1975).\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"Who introduced the GloVe word embedding model and when?\",\n",
                "        \"ground_truth\": \"GloVe (Global Vectors for Word Representation) was introduced by Pennington et al. in 2014.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the main contribution of Neural Network Language Models (NNLMs)?\",\n",
                "        \"ground_truth\": \"NNLMs, pioneered by Bengio et al. (2003), reframed language modeling as unsupervised learning and introduced embedding layers that project words into dense vector spaces.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What benchmark is used to evaluate text embedding models across multiple languages?\",\n",
                "        \"ground_truth\": \"The Massive Text Embedding Benchmark (MTEB) evaluates embedding models across multiple languages and diverse NLP tasks.\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the key advantage of subword tokenization in neural machine translation?\",\n",
                "        \"ground_truth\": \"Subword tokenization (like BPE) enables open-vocabulary translation, handling rare words and achieving better compression while maintaining translation quality.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "eval_df = pd.DataFrame(eval_data)\n",
                "print(f\"‚úÖ Golden dataset ready: {len(eval_df)} evaluation questions\")\n",
                "print(f\"üìÑ Covering: Word Embeddings, BPE, NMT, Vector Models\")\n",
                "eval_df[[\"question\"]].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34cc8b27",
            "metadata": {},
            "source": [
                "## üîç Step 9: Run RAG Inference\n",
                "\n",
                "Now we need to generate answers using our RAG pipeline for each question in the golden dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1873c559",
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "print(\"üîç Running RAG evaluation inference...\\n\")\n",
                "\n",
                "for idx, row in eval_df.iterrows():\n",
                "    try:\n",
                "        # Use the traced RAG function\n",
                "        response = rag_query(row[\"question\"])\n",
                "        \n",
                "        results.append({\n",
                "            \"question\": row[\"question\"],\n",
                "            \"ground_truth\": row[\"ground_truth\"],\n",
                "            \"answer\": response[\"answer\"],\n",
                "            \"contexts\": response[\"context\"]  # Required for faithfulness metric\n",
                "        })\n",
                "        \n",
                "        print(f\"  ‚úì Q{idx+1}: {row['question'][:70]}...\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚úó Q{idx+1} failed: {e}\")\n",
                "        continue\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(f\"\\n‚úÖ Inference complete: {len(results_df)}/{len(eval_df)} questions answered\")\n",
                "results_df[[\"question\", \"answer\"]].head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics-reference",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìè RAG Evaluation Metrics Reference\n",
                "\n",
                "Before we run evaluation, let's understand the metrics we'll use.\n",
                "\n",
                "### How LLM-as-a-Judge Works\n",
                "\n",
                "Traditional metrics like **BLEU** or **ROUGE** compare word overlap. But for conversational AI:\n",
                "- \"The capital of France is Paris\" ‚â† \"Paris is the capital city of France\" (different words, same meaning!)\n",
                "\n",
                "**LLM-as-a-Judge** uses a powerful LLM (like GPT-4) to evaluate responses semantically.\n",
                "\n",
                "### Metrics We Use\n",
                "\n",
                "| Metric | Question the Judge Asks | Score Range | What It Measures |\n",
                "|--------|------------------------|-------------|-------------------|\n",
                "| **Faithfulness** | \"Is the answer supported *only* by the retrieved context?\" | 1-5 | Anti-hallucination: did the LLM make things up? |\n",
                "| **Answer Relevance** | \"Does the answer actually address the user's question?\" | 1-5 | Is the response on-topic and helpful? |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5ed071b4",
            "metadata": {},
            "source": [
                "## üìä Step 10: W&B Weave LLM-as-a-Judge Evaluation\n",
                "\n",
                "Now we'll create custom scorer functions for W&B Weave evaluation. These scorers use GPT-4o-mini as the judge to evaluate:\n",
                "\n",
                "1. **Faithfulness**: Is the answer based solely on the retrieved context?\n",
                "2. **Answer Relevance**: Does the answer address the user's question?\n",
                "\n",
                "Unlike MLflow's built-in metrics, W&B Weave uses custom scorer functions that you define."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scorer-definitions",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "\n",
                "# Initialize OpenAI client for LLM-as-Judge\n",
                "client = OpenAI()\n",
                "\n",
                "def llm_judge(prompt: str) -> dict:\n",
                "    \"\"\"Call GPT-4o-mini to judge a response.\"\"\"\n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-4o-mini\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": \"You are an expert evaluator. Provide a score from 1-5 and a brief justification.\"},\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ],\n",
                "        temperature=0\n",
                "    )\n",
                "    return response.choices[0].message.content\n",
                "\n",
                "@weave.op()\n",
                "def faithfulness_scorer(question: str, answer: str, contexts: list) -> dict:\n",
                "    \"\"\"\n",
                "    Evaluates if the answer is faithful to the retrieved context.\n",
                "    Score 5 = fully supported by context, no hallucination\n",
                "    Score 1 = significant hallucination, answer not in context\n",
                "    \"\"\"\n",
                "    context_text = \"\\n\\n\".join(contexts) if contexts else \"No context provided\"\n",
                "    \n",
                "    prompt = f\"\"\"Evaluate FAITHFULNESS: Is the answer supported ONLY by the provided context?\n",
                "\n",
                "CONTEXT:\n",
                "{context_text[:3000]}  # Truncate for token limits\n",
                "\n",
                "QUESTION: {question}\n",
                "\n",
                "ANSWER: {answer}\n",
                "\n",
                "Score from 1-5:\n",
                "- 5: Answer is completely supported by context, no external information used\n",
                "- 4: Answer is mostly supported, minor additions that are reasonable\n",
                "- 3: Answer is partially supported, some unsupported claims\n",
                "- 2: Answer has significant unsupported claims\n",
                "- 1: Answer is mostly hallucinated, not in context\n",
                "\n",
                "Respond with JSON: {{\"score\": <1-5>, \"justification\": \"<brief reason>\"}}\"\"\"\n",
                "    \n",
                "    result = llm_judge(prompt)\n",
                "    try:\n",
                "        # Parse JSON response\n",
                "        parsed = json.loads(result)\n",
                "        return {\"faithfulness_score\": parsed.get(\"score\", 3), \"faithfulness_reason\": parsed.get(\"justification\", \"\")}\n",
                "    except:\n",
                "        # Fallback if JSON parsing fails\n",
                "        return {\"faithfulness_score\": 3, \"faithfulness_reason\": result}\n",
                "\n",
                "@weave.op()\n",
                "def relevance_scorer(question: str, answer: str) -> dict:\n",
                "    \"\"\"\n",
                "    Evaluates if the answer is relevant to the question.\n",
                "    Score 5 = directly answers the question\n",
                "    Score 1 = completely off-topic\n",
                "    \"\"\"\n",
                "    prompt = f\"\"\"Evaluate RELEVANCE: Does the answer directly address the question?\n",
                "\n",
                "QUESTION: {question}\n",
                "\n",
                "ANSWER: {answer}\n",
                "\n",
                "Score from 1-5:\n",
                "- 5: Answer directly and completely addresses the question\n",
                "- 4: Answer mostly addresses the question with minor gaps\n",
                "- 3: Answer partially addresses the question\n",
                "- 2: Answer barely addresses the question\n",
                "- 1: Answer is completely off-topic\n",
                "\n",
                "Respond with JSON: {{\"score\": <1-5>, \"justification\": \"<brief reason>\"}}\"\"\"\n",
                "    \n",
                "    result = llm_judge(prompt)\n",
                "    try:\n",
                "        parsed = json.loads(result)\n",
                "        return {\"relevance_score\": parsed.get(\"score\", 3), \"relevance_reason\": parsed.get(\"justification\", \"\")}\n",
                "    except:\n",
                "        return {\"relevance_score\": 3, \"relevance_reason\": result}\n",
                "\n",
                "print(\"‚úÖ LLM-as-Judge scorers defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-evaluation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation on all results\n",
                "print(\"üîç Running LLM-as-a-Judge evaluation...\\n\")\n",
                "\n",
                "evaluation_results = []\n",
                "\n",
                "for idx, row in results_df.iterrows():\n",
                "    print(f\"  Evaluating Q{idx+1}: {row['question'][:50]}...\")\n",
                "    \n",
                "    # Get faithfulness score\n",
                "    faith_result = faithfulness_scorer(\n",
                "        question=row['question'],\n",
                "        answer=row['answer'],\n",
                "        contexts=row['contexts']\n",
                "    )\n",
                "    \n",
                "    # Get relevance score\n",
                "    rel_result = relevance_scorer(\n",
                "        question=row['question'],\n",
                "        answer=row['answer']\n",
                "    )\n",
                "    \n",
                "    evaluation_results.append({\n",
                "        \"question\": row['question'],\n",
                "        \"ground_truth\": row['ground_truth'],\n",
                "        \"answer\": row['answer'],\n",
                "        \"faithfulness\": faith_result['faithfulness_score'],\n",
                "        \"faithfulness_reason\": faith_result['faithfulness_reason'],\n",
                "        \"relevance\": rel_result['relevance_score'],\n",
                "        \"relevance_reason\": rel_result['relevance_reason']\n",
                "    })\n",
                "\n",
                "eval_results_df = pd.DataFrame(evaluation_results)\n",
                "print(f\"\\n‚úÖ Evaluation complete!\")\n",
                "\n",
                "# Calculate and display aggregate metrics\n",
                "avg_faithfulness = eval_results_df['faithfulness'].mean()\n",
                "avg_relevance = eval_results_df['relevance'].mean()\n",
                "\n",
                "print(f\"\\nüìä AGGREGATE METRICS (1-5 scale, higher is better):\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"  Faithfulness Mean: {avg_faithfulness:.3f}\")\n",
                "print(f\"  Relevance Mean:    {avg_relevance:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "log-to-wandb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Log results to W&B\n",
                "current_config = {\n",
                "    \"chunk_size\": 1000,\n",
                "    \"chunk_overlap\": 200,\n",
                "    \"retrieval_k\": 5,\n",
                "    \"embedding_model\": \"text-embedding-3-small\",\n",
                "    \"llm_model\": MODEL,\n",
                "    \"judge_model\": \"gpt-4o-mini\"\n",
                "}\n",
                "\n",
                "# Log configuration and metrics to W&B\n",
                "wandb.init(project=WANDB_PROJECT, name=\"RAG_Baseline_v1\", config=current_config, reinit=True)\n",
                "\n",
                "# Log aggregate metrics\n",
                "wandb.log({\n",
                "    \"faithfulness_mean\": avg_faithfulness,\n",
                "    \"relevance_mean\": avg_relevance,\n",
                "    \"num_questions\": len(eval_results_df)\n",
                "})\n",
                "\n",
                "# Log the evaluation table\n",
                "eval_table = wandb.Table(dataframe=eval_results_df)\n",
                "wandb.log({\"evaluation_results\": eval_table})\n",
                "\n",
                "# Log the golden dataset\n",
                "golden_table = wandb.Table(dataframe=eval_df)\n",
                "wandb.log({\"golden_dataset\": golden_table})\n",
                "\n",
                "print(f\"\\nüéâ Results logged to W&B!\")\n",
                "print(f\"üåê View at: {wandb.run.get_url()}\")\n",
                "\n",
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7da16cfd",
            "metadata": {},
            "source": [
                "## üî¨ Step 11: Per-Question Breakdown\n",
                "\n",
                "### Debugging Individual Failures\n",
                "\n",
                "Averages hide details. To improve your system, you must look at **individual failures**.\n",
                "\n",
                "#### How to interpret this table:\n",
                "1.  **Low Faithfulness, High Relevance**: The model gave a good-sounding answer, but it wasn't in the document! This is dangerous (Hallucination).\n",
                "2.  **High Faithfulness, Low Relevance**: The model quoted the document perfectly, but it didn't answer the user's question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da78262b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display per-question breakdown\n",
                "print(\"üìã PER-QUESTION PERFORMANCE:\\n\")\n",
                "\n",
                "breakdown = eval_results_df[[\"question\", \"faithfulness\", \"relevance\"]].copy()\n",
                "breakdown.columns = [\"Question\", \"Faithfulness\", \"Relevance\"]\n",
                "\n",
                "# Highlight low-scoring questions (< 3.5)\n",
                "styled = breakdown.style.map(\n",
                "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x < 3.5 else '',\n",
                "    subset=[\"Faithfulness\", \"Relevance\"]\n",
                ")\n",
                "\n",
                "styled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "failure-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify and analyze low-scoring questions\n",
                "THRESHOLD = 3.5  # Scores below this are concerning\n",
                "\n",
                "print(\"üîç FAILURE ANALYSIS:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Find questions with low faithfulness\n",
                "low_faith = eval_results_df[eval_results_df['faithfulness'] < THRESHOLD]\n",
                "\n",
                "if len(low_faith) > 0:\n",
                "    print(f\"\\n‚ö† Found {len(low_faith)} question(s) with Faithfulness < {THRESHOLD}:\")\n",
                "    for idx, row in low_faith.iterrows():\n",
                "        print(f\"\\n  Q: {row['question'][:70]}...\")\n",
                "        print(f\"  Score: {row['faithfulness']}\")\n",
                "        print(f\"  Reason: {row['faithfulness_reason'][:100]}...\")\n",
                "else:\n",
                "    print(\"‚úÖ No low-faithfulness questions found!\")\n",
                "\n",
                "# Find questions with low relevance\n",
                "low_rel = eval_results_df[eval_results_df['relevance'] < THRESHOLD]\n",
                "\n",
                "if len(low_rel) > 0:\n",
                "    print(f\"\\n‚ö† Found {len(low_rel)} question(s) with Relevance < {THRESHOLD}:\")\n",
                "    for idx, row in low_rel.iterrows():\n",
                "        print(f\"\\n  Q: {row['question'][:70]}...\")\n",
                "        print(f\"  Score: {row['relevance']}\")\n",
                "        print(f\"  Reason: {row['relevance_reason'][:100]}...\")\n",
                "else:\n",
                "    print(\"‚úÖ No low-relevance questions found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "student-challenge",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Student Challenge\n",
                "\n",
                "Now it's your turn to experiment!\n",
                "\n",
                "### Challenge 1: Tune the Chunking Strategy\n",
                "\n",
                "**Hypothesis**: Smaller chunks might capture specific details better.\n",
                "\n",
                "**Task**:\n",
                "1. Go back to **Step 4** and change `chunk_size` from 1000 to 500\n",
                "2. Delete the `vector_db` folder to force re-indexing\n",
                "3. Re-run the entire pipeline\n",
                "4. Compare results in W&B dashboard\n",
                "\n",
                "### Challenge 2: Expand the Golden Dataset\n",
                "\n",
                "**Task**: Add 3 new questions to the evaluation dataset:\n",
                "\n",
                "1. One **factual question** (\"When was X published?\")\n",
                "2. One **reasoning question** (\"Compare X and Y\")\n",
                "3. One **out-of-scope question** (something NOT in your documents)\n",
                "\n",
                "### Challenge 3: Try Different Prompts\n",
                "\n",
                "**Task**: Modify the `qa_system_prompt` to be more strict about hallucination:\n",
                "- Add: \"If the information is not in the context, say 'I don't know'\"\n",
                "- Re-run evaluation and compare faithfulness scores"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "next-steps",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìö Next Steps & Resources\n",
                "\n",
                "### Further Reading\n",
                "\n",
                "| Resource | Description | Link |\n",
                "|----------|-------------|------|\n",
                "| **W&B Weave Docs** | Complete guide to LLM evaluation | [docs.wandb.ai/weave](https://docs.wandb.ai/weave) |\n",
                "| **W&B Evaluations Guide** | In-depth tutorial on evaluations | [docs.wandb.ai/weave/guides](https://docs.wandb.ai/weave/guides/core-types/evaluations) |\n",
                "| **RAGAS Framework** | Alternative evaluation framework | [ragas.io](https://docs.ragas.io/) |\n",
                "| **LangSmith** | LangChain's observability platform | [docs.smith.langchain.com](https://docs.smith.langchain.com/) |\n",
                "\n",
                "---\n",
                "\n",
                "## üèÅ Summary\n",
                "\n",
                "In this notebook, you learned:\n",
                "\n",
                "- ‚úÖ RAG systems need **systematic evaluation**, not just manual testing\n",
                "- ‚úÖ **Golden Datasets** provide the ground truth for benchmarking\n",
                "- ‚úÖ **LLM-as-a-Judge** enables semantic evaluation at scale\n",
                "- ‚úÖ **W&B Weave** provides tracing, evaluation, and dashboards\n",
                "- ‚úÖ **Faithfulness** measures hallucination, **Relevance** measures answer quality\n",
                "- ‚úÖ Per-question analysis helps you **debug specific failures**\n",
                "- ‚úÖ Experiment tracking in W&B helps you **iterate on configurations**\n",
                "\n",
                "**Remember**: A RAG system is only as good as your ability to measure and improve it!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
