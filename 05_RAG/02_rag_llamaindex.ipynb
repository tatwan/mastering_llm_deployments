{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG Application with LlamaIndex\n",
                "\n",
                "## Introduction\n",
                "\n",
                "**LlamaIndex** is a data framework for your LLM applications. It is designed to easily connect custom data sources to your LLM. While LangChain is a general-purpose framework for building LLM applications (chains, agents, etc.), LlamaIndex optimizes for **indexing** and **retrieving** data.\n",
                "\n",
                "### Key Differences form LangChain\n",
                "- **Simpler Interface**: LlamaIndex often requires less code to get a RAG system up and running.\n",
                "- **Data-Centric**: It focuses heavily on the \"Index\" partâ€”structuring your data for optimal retrieval.\n",
                "\n",
                "In this notebook, we will build a RAG application to chat with our PDF documents using LlamaIndex."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies\n",
                "\n",
                "We need to install `llama-index`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "25c34368",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install llama-index\n",
                "!uv pip install -q llama-index"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8891a785",
            "metadata": {},
            "source": [
                "## Step 2: Setup Environment\n",
                "\n",
                "LlamaIndex uses OpenAI by default. We need to make sure our API key is set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a3bc452",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… OPENAI_API_KEY loaded\n"
                    ]
                }
            ],
            "source": [
                "# Option 1: Set your API key directly (for Colab)\n",
                "#from google.colab import userdata\n",
                "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "\n",
                "# Option 2: Load from .env file\n",
                "# import os\n",
                "# from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
                "    print(\"âš ï¸ Warning: OPENAI_API_KEY not found. Please set it in your .env file.\")\n",
                "else:\n",
                "    print(\"âœ… OPENAI_API_KEY loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c79ae7bc",
            "metadata": {},
            "source": [
                "## Step 3: Load Data\n",
                "\n",
                "LlamaIndex makes data loading very simple with `SimpleDirectoryReader`. It automatically figures out how to parse files in a directory (PDFs, text files, etc.)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6a484b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install gdown -q\n",
                "\n",
                "# !gdown 12hCcDOBYO0A3q2eFstMQnNqcCX2WYCZq -O pdfs.zip\n",
                "# !unzip -q pdfs.zip -d pdfs\n",
                "# !rm pdfs.zip\n",
                "# print(\"âœ“ PDFs extracted to ./pdfs folder\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "19fd1506",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Loaded 126 document pages\n"
                    ]
                }
            ],
            "source": [
                "from llama_index.core import SimpleDirectoryReader\n",
                "\n",
                "# Load data from the 'pdfs' directory\n",
                "reader = SimpleDirectoryReader(input_dir=\"pdfs\")\n",
                "documents = reader.load_data()\n",
                "\n",
                "print(f\"âœ… Loaded {len(documents)} document pages\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "91d3f954",
            "metadata": {},
            "source": [
                "## Step 4: Create Index\n",
                "\n",
                "Now we create a `VectorStoreIndex` from our documents. This effectively:\n",
                "1. Chunks the documents.\n",
                "2. Embeds the chunks using OpenAI embeddings.\n",
                "3. Stores them in an in-memory vector store (by default).\n",
                "\n",
                "> **Note**: Since this is in-memory, if you restart the kernel, the index is lost and needs to be recreated (incurring embedding costs again). We will see how to persist it later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "2279ba84",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-12-09 13:57:24,078 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
                        "2025-12-09 13:57:27,500 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
                        "2025-12-09 13:57:29,657 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Index created\n"
                    ]
                }
            ],
            "source": [
                "from llama_index.core import VectorStoreIndex\n",
                "\n",
                "# Create the index (this happens in one line!)\n",
                "index = VectorStoreIndex.from_documents(documents)\n",
                "\n",
                "print(\"âœ… Index created\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb424f98",
            "metadata": {},
            "source": [
                "## Step 5: Querying\n",
                "\n",
                "To ask questions, we create a \"Query Engine\" from our index. This engine handles the retrieval and LLM generation loop for us."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "6d4009c2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-12-09 13:57:33,048 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
                        "2025-12-09 13:57:35,020 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Response:\n",
                        "Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of symbols with a single, unused symbol. It is commonly used for subword tokenization in natural language processing tasks to create a token dictionary of a specified size.\n"
                    ]
                }
            ],
            "source": [
                "# Create a query engine\n",
                "query_engine = index.as_query_engine()\n",
                "\n",
                "# Ask a question\n",
                "response = query_engine.query(\"What is Byte Pair Encoding?\")\n",
                "\n",
                "print(\"Response:\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "625f4f9d",
            "metadata": {},
            "source": [
                "### Inspecting the Source\n",
                "\n",
                "We can check which documents were used to generate the answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "1362e178",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------------------------------------\n",
                        "Score: 0.8890902737390015\n",
                        "Source File: 2411.08671v1.pdf\n",
                        "Content: arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "LÂ´ aszlÂ´ o Kozma and Johannes Voderholzer\n",
                        "Institut fÂ¨ ur Informatik, Freie UniversitÂ¨ at Berlin, Germany\n",
                        "Abstract\n",
                        "By...\n",
                        "--------------------------------------------------\n",
                        "Score: 0.8741710360039733\n",
                        "Source File: 1508.07909v5.pdf\n",
                        "Content: As an alternative, we pro-\n",
                        "pose a segmentation algorithm based on byte pair\n",
                        "encoding (BPE), which lets us learn a vocabulary\n",
                        "that provides a good compression rate of the text.\n",
                        "3.2 Byte Pair Encoding (...\n"
                    ]
                }
            ],
            "source": [
                "for node in response.source_nodes:\n",
                "    print(\"--------------------------------------------------\")\n",
                "    print(f\"Score: {node.score}\")\n",
                "    print(f\"Source File: {node.metadata.get('file_name')}\")\n",
                "    print(f\"Content: {node.text[:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46fbfb31",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "As you can see, LlamaIndex abstracts away a lot of the complexity (chunking, embedding, retrieval setup) that we explicitly handled in the LangChain version. This makes it a great choice for getting up and running quickly with standard RAG pipelines."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0cb6c7b",
            "metadata": {},
            "source": [
                "## Persisting to Disk\n",
                "\n",
                "By default, LlamaIndex stores the data in-memory. This means if you restart the kernel, you lose the index and have to rebuild it (re-embedding everything, which costs money).\n",
                "\n",
                "We can persist the index to disk to avoid this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "95840c26",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Index persisted to ./vector_db_llama\n"
                    ]
                }
            ],
            "source": [
                "# Persist the index to a directory\n",
                "index.storage_context.persist(persist_dir=\"./vector_db_llama\")\n",
                "\n",
                "print(\"âœ… Index persisted to ./vector_db_llama\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "409b723e",
            "metadata": {},
            "source": [
                "### Loading from Disk\n",
                "\n",
                "To load the index back, we use `StorageContext` and `load_index_from_storage`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "ab0fdcb4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-12-09 13:57:44,245 - INFO - Loading all indices.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Index loaded from disk\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-12-09 13:57:45,367 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
                        "2025-12-09 13:57:46,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Byte Pair Encoding (BPE) is a data compression technique that involves iteratively replacing the most frequent pair of symbols with a new symbol. This process helps in shortening the text by creating a lookup-table of new symbols and the pairs they replace, allowing for the reconstruction of the original text. The goal of BPE is to achieve the best compression within a predefined budget by fixing the number of new symbols upfront.\n"
                    ]
                }
            ],
            "source": [
                "from llama_index.core import StorageContext, load_index_from_storage\n",
                "\n",
                "# Rebuild storage context\n",
                "storage_context = StorageContext.from_defaults(persist_dir=\"./vector_db_llama\")\n",
                "\n",
                "# Load index from the storage context\n",
                "loaded_index = load_index_from_storage(storage_context)\n",
                "print(\"âœ… Index loaded from disk\")\n",
                "\n",
                "# Verify it works\n",
                "query_engine_loaded = loaded_index.as_query_engine()\n",
                "response = query_engine_loaded.query(\"What is Byte Pair Encoding?\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3d77cdc",
            "metadata": {},
            "source": [
                "## ðŸŽ“ Student Challenge\n",
                "\n",
                "Now it's your turn! \n",
                "\n",
                "**Goal**: Create a new RAG system using your *own* PDF documents.\n",
                "\n",
                "1. Create a new folder named `student_pdfs` in this directory.\n",
                "2. Upload 1-2 PDF files into that folder (e.g., a paper, a resume, a manual).\n",
                "3. Use `SimpleDirectoryReader` to load documents from `student_pdfs`.\n",
                "4. Create a new `VectorStoreIndex` from these documents.\n",
                "5. Create a query engine and ask a question about your documents.\n",
                "\n",
                "Write your code in the cells below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f935c94",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load your documents from 'student_pdfs'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a40588e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Create an index\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "97c09dcc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Query your index\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
