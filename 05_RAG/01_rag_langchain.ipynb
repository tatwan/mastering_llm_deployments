{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "004f2133",
            "metadata": {},
            "source": [
                "# RAG Application with LangChain\n",
                "\n",
                "## Introduction to RAG (Retrieval Augmented Generation)\n",
                "\n",
                "**Retrieval Augmented Generation (RAG)** is a powerful technique that enhances Large Language Models (LLMs) by providing them with access to external data sources. This approach addresses a key limitation of LLMs: while they are trained on vast amounts of public data, they lack knowledge of your *private* data (e.g., company documents, personal files, or proprietary databases).\n",
                "\n",
                "### How RAG Works\n",
                "\n",
                "RAG operates through a three-step process:\n",
                "\n",
                "1. **Retrieval**: Search and retrieve relevant information from your data source based on the user's query\n",
                "2. **Augmentation**: Enhance the prompt sent to the LLM with the retrieved context\n",
                "3. **Generation**: Use the LLM to generate a response grounded in the retrieved information\n",
                "\n",
                "### What We'll Build\n",
                "\n",
                "In this notebook, we will build a RAG application that can chat with PDF documents using **LangChain v1.0+**, the latest version of the popular framework for building LLM applications.\n",
                "\n",
                "### Key Concepts Covered\n",
                "\n",
                "| Concept | Description |\n",
                "|---------|-------------|\n",
                "| **Document Loading** | Reading files (PDFs, text, etc.) into a format LangChain can process |\n",
                "| **Splitting** | Breaking documents into smaller, manageable chunks |\n",
                "| **Embeddings** | Converting text into numerical vectors that capture semantic meaning |\n",
                "| **Vector Store** | A database optimized for storing and searching these vectors |\n",
                "| **Retrieval** | Finding the most relevant chunks for a given query |\n",
                "| **Generation** | Using an LLM to synthesize an answer from retrieved context |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "install-packages-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 0: Install Required Packages\n",
                "\n",
                "Before we begin, we need to install the required packages. This cell installs:\n",
                "\n",
                "- **`langchain`**: The core LangChain framework\n",
                "- **`langchain-community`**: Community integrations (document loaders, etc.)\n",
                "- **`langchain-openai`**: OpenAI-specific components (embeddings, chat models)\n",
                "- **`langchain-chroma`**: Chroma vector store integration\n",
                "- **`langgraph`**: Graph-based agent orchestration (required for modern agents in v1.0+)\n",
                "- **`pypdf`**: PDF parsing library\n",
                "- **`gradio`**: Web interface for interactive demos\n",
                "- **`python-dotenv`**: Environment variable management\n",
                "\n",
                "> ‚ö†Ô∏è **Important**: After running this cell, you may need to **restart the kernel** to ensure all packages are properly loaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50791208",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Packages installed! Please restart the kernel if this is your first time running this cell.\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages\n",
                "# Note: The --force-reinstall for numpy and scipy fixes potential binary incompatibility issues\n",
                "# !uv pip install -U -q langchain langchain-community langchain-openai langchain-chroma langgraph pypdf gradio python-dotenv\n",
                "# !uv pip install --force-reinstall -q numpy scipy\n",
                "\n",
                "print(\"‚úÖ Packages installed! Please restart the kernel if this is your first time running this cell.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b741c31",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Environment Setup\n",
                "\n",
                "We need to configure our API keys to authenticate with OpenAI. This notebook supports both:\n",
                "\n",
                "- **Google Colab**: Uses `google.colab.userdata` to securely access keys stored in Colab Secrets\n",
                "- **Local Execution**: Uses `python-dotenv` to load keys from a `.env` file\n",
                "\n",
                "### Setting Up Your API Key\n",
                "\n",
                "**For local development**, create a `.env` file in this directory with:\n",
                "```\n",
                "OPENAI_API_KEY=your-api-key-here\n",
                "```\n",
                "\n",
                "**For Colab**, add your key to Colab Secrets with the name `OPENAI_API_KEY`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f587032c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ OPENAI_API_KEY loaded successfully\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Configuration\n",
                "MODEL = \"gpt-4o-mini\"  # The LLM model to use\n",
                "db_name = \"vector_db\"  # Directory name for the vector store\n",
                "\n",
                "\n",
                "# Option 1: Set your API key directly (for Colab)\n",
                "#from google.colab import userdata\n",
                "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
                "\n",
                "# Option 2:\n",
                "# Load environment variables from .env file\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Verify API key is set\n",
                "if os.environ.get(\"OPENAI_API_KEY\"):\n",
                "    print(\"‚úÖ OPENAI_API_KEY loaded successfully\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not found. Please set it in your .env file or environment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df53314a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Import Dependencies\n",
                "\n",
                "Now we import all the necessary modules from LangChain and other libraries. Here's what each import does:\n",
                "\n",
                "### Document Processing\n",
                "- **`DirectoryLoader`**: Loads multiple files from a directory\n",
                "- **`PyPDFLoader`**: Parses PDF files into text\n",
                "- **`RecursiveCharacterTextSplitter`**: Splits text into chunks while respecting natural boundaries\n",
                "\n",
                "### Embeddings & Vector Store\n",
                "- **`OpenAIEmbeddings`**: Converts text to vector embeddings using OpenAI's models\n",
                "- **`Chroma`**: A fast, open-source vector database\n",
                "\n",
                "### LLM & Chains\n",
                "- **`ChatOpenAI`**: OpenAI's chat models (GPT-4, etc.)\n",
                "- **`create_history_aware_retriever`**: Creates a retriever that understands conversation context\n",
                "- **`create_retrieval_chain`**: Combines retrieval and generation into a single chain\n",
                "- **`create_stuff_documents_chain`**: Creates a chain that \"stuffs\" documents into the prompt\n",
                "\n",
                "### Prompts & Messages\n",
                "- **`ChatPromptTemplate`**: Templates for structured prompts\n",
                "- **`MessagesPlaceholder`**: Placeholder for conversation history\n",
                "- **`HumanMessage` / `AIMessage`**: Message types for chat history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "f8941dc4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ All imports successful!\n"
                    ]
                }
            ],
            "source": [
                "import glob\n",
                "import os\n",
                "\n",
                "# Document loading and processing\n",
                "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Embeddings and LLM\n",
                "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
                "\n",
                "# Vector store\n",
                "from langchain_chroma import Chroma\n",
                "\n",
                "# Chains for RAG\n",
                "from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain\n",
                "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
                "\n",
                "# Prompts and messages\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "\n",
                "# UI\n",
                "import gradio as gr\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dd2d05cd",
            "metadata": {},
            "source": [
                "### üîç Detailed Breakdown of Imports\n",
                "\n",
                "Let's break down exactly what each imported component does:\n",
                "\n",
                "- **`DirectoryLoader, PyPDFLoader`**: `DirectoryLoader` helps us grab all files in a folder. `PyPDFLoader` is the specialist that knows how to read PDF files page by page.\n",
                "- **`RecursiveCharacterTextSplitter`**: This is a \"smart\" splitter. Instead of just chopping text every 1000 characters (which might cut a sentence in half), it tries to split at natural boundaries like paragraphs `\\n\\n` or sentences `\\n` to check context.\n",
                "- **`OpenAIEmbeddings`**: This tool takes text and turns it into a list of numbers (vectors). It uses OpenAI's models to do this translation.\n",
                "- **`Chroma`**: This is our database. It stores the vectors we created with `OpenAIEmbeddings` so we can search them later.\n",
                "- **`create_history_aware_retriever`**: A special chain that takes your follow-up question (e.g., \"How does it work?\") and your chat history, and rewrites it into a full question (e.g., \"How does RAG work?\") so the database can understand it.\n",
                "- **`create_retrieval_chain`**: The manager that coordinates everything: it gets the question, sends it to the retriever, gets documents back, and sends them to the LLM.\n",
                "- **`create_stuff_documents_chain`**: The worker that actually sends the prompt to the LLM. It \"stuffs\" all the retrieved text into the system prompt.\n",
                "- **`ChatPromptTemplate`**: A flexible template builder. It lets us create prompts with placeholders (like `{context}` or `{input}`) that get filled in dynamically.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e662569",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Load Documents\n",
                "\n",
                "The first step in building a RAG application is loading your documents. We use:\n",
                "\n",
                "- **`glob.glob()`**: To find all PDF files in the `pdfs/` directory and current directory\n",
                "- **`PyPDFLoader`**: To parse each PDF and extract text content\n",
                "\n",
                "### Document Structure\n",
                "\n",
                "Each loaded document contains:\n",
                "- **`page_content`**: The actual text content\n",
                "- **`metadata`**: Information about the document (source file, page number, etc.)\n",
                "\n",
                "> üìÅ **Note**: Place your PDF files in a `pdfs/` subdirectory or in the same directory as this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47491596",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install gdown -q\n",
                "\n",
                "# !gdown 12hCcDOBYO0A3q2eFstMQnNqcCX2WYCZq -O pdfs.zip\n",
                "# !unzip -q pdfs.zip -d pdfs\n",
                "# !rm pdfs.zip\n",
                "# print(\"‚úì PDFs extracted to ./pdfs folder\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "089ba94c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÑ Found 9 PDF file(s)\n",
                        "‚úÖ Loaded 126 pages from 9 file(s)\n"
                    ]
                }
            ],
            "source": [
                "# Find all PDF files in the pdfs/ subdirectory and current directory\n",
                "folders = glob.glob(\"pdfs/*.pdf\") + glob.glob(\"*.pdf\")\n",
                "\n",
                "if not folders:\n",
                "    print(\"‚ö†Ô∏è No PDF files found. Please add PDF files to the 'pdfs/' directory or current directory.\")\n",
                "else:\n",
                "    print(f\"üìÑ Found {len(folders)} PDF file(s)\")\n",
                "\n",
                "# Load all documents\n",
                "documents = []\n",
                "for file_path in folders:\n",
                "    loader = PyPDFLoader(file_path)\n",
                "    docs = loader.load()\n",
                "    for doc in docs:\n",
                "        # Add custom metadata to track source file\n",
                "        doc.metadata[\"source_file\"] = os.path.basename(file_path)\n",
                "        documents.append(doc)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(documents)} pages from {len(folders)} file(s)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "0f8ddf60",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First document metadata:\n",
                        "pdfs/1901.09069v2.pdf\n",
                        "11\n",
                        "1901.09069v2.pdf\n"
                    ]
                }
            ],
            "source": [
                "# print first document metadata, such as file name, source, total number of pages, etc.\n",
                "print(\"First document metadata:\")\n",
                "print(documents[0].metadata['source'])\n",
                "print(documents[0].metadata['total_pages'])\n",
                "print(documents[0].metadata['source_file'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "ab9baf75",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word Embeddings: A Survey\n",
                        "Felipe Almeida Geraldo Xex ¬¥eo‚àó\n",
                        "Computer and Systems Engineering Program (PESC-COPPE)\n",
                        "Federal University of Rio de Janeiro\n",
                        "Rio de Janeiro, Brazil\n",
                        "{falmeida,xexeo}@cos.ufrj.br\n",
                        "Abstract\n",
                        "This work lists and describes the main re-\n",
                        "cent strategies for building Ô¨Åxed-length,\n",
                        "dense and distributed representations for\n",
                        "words, based on the distributional hypoth-\n",
                        "esis. These representations are now com-\n",
                        "monly called word embeddings and, in ad-\n",
                        "dition to encoding surprisingly good syn-\n",
                        "tactic and semantic information, have been\n",
                        "proven useful as extra features in many\n",
                        "downstream NLP tasks.\n",
                        "1 Introduction\n",
                        "The task of representing words and documents is\n",
                        "part and parcel of most, if not all, Natural Lan-\n",
                        "guage Processing (NLP) tasks. In general, it has\n",
                        "been found to be useful to represent them as vec-\n",
                        "tors, which have an appealing, intuitive interpreta-\n",
                        "tion, can be the subject of useful operations (e.g.\n",
                        "addition, subtraction, distance measures, etc) and\n",
                        "lend themselves well to be used in many Machine\n",
                        "Learning (ML) algorithms and strategies.\n",
                        "The Vector Space Model (VSM), generally at-\n",
                        "tributed to Salton (1975) and stemming from the\n",
                        "Information Retrieval (IR) community, is arguably\n",
                        "the most successful and inÔ¨Çuential model to en-\n",
                        "code words and documents as vectors.\n",
                        "Another very important part of natural\n",
                        "language-based solutions is, of course, the study\n",
                        "of language models. A language model is a statis-\n",
                        "tical model of language usage. It focuses mainly\n",
                        "on predicting the next word given a number of\n",
                        "previous words. This is very useful, for instance,\n",
                        "in speech recognition software, where one needs\n",
                        "to correctly decide what is the word said by the\n",
                        "speaker, even when signal quality is poor or there\n",
                        "is a lot of background noise.\n",
                        "These two seemingly independent Ô¨Åelds have\n",
                        "arguably been brought together by recent research\n",
                        "‚àóGeraldo Xex ¬¥eo is also with the Mathematics Institute\n",
                        "(IM-UFRJ), Federal University of Rio de Janeiro\n",
                        "on Neural Network Language Models (NNLMs),\n",
                        "with Bengio et al. (2003)) having developed the\n",
                        "Ô¨Årst1 large-scale language models based on neural\n",
                        "nets.\n",
                        "Their idea was to reframe the problem as an\n",
                        "unsupervised learning problem. A key feature of\n",
                        "this solution is the way raw words vectors are Ô¨Årst\n",
                        "projected onto a so-called embedding layer be-\n",
                        "fore being fed into other layers of the network.\n",
                        "Among other reasons, this was imagined to help\n",
                        "ease the effect of the curse of dimensionality on\n",
                        "language models, and help generalization (Bengio\n",
                        "et al. (2003)).\n",
                        "With time, such word embeddings have\n",
                        "emerged as a topic of research in and of them-\n",
                        "selves, with the realization that they can be used\n",
                        "as standalone features in many NLP tasks (Turian\n",
                        "et al. (2010)) and the fact that they encode surpris-\n",
                        "ingly accurate syntactic and semantic word rela-\n",
                        "tionships (Mikolov et al. (2013a)).\n",
                        "More recently2, other ways of creating embed-\n",
                        "dings have surfaced, which rely not on neural net-\n",
                        "works and embedding layers but on leveraging\n",
                        "word-context matrices to arrive at vector repre-\n",
                        "sentations for words. Among the most inÔ¨Çuential\n",
                        "models we can cite the GloVe model (Pennington\n",
                        "et al. (2014)).\n",
                        "These two types of model have something in\n",
                        "common, namely their reliance on the assump-\n",
                        "tion that words with similar contexts (other words)\n",
                        "have the same meaning. This has been called the\n",
                        "distributional hypothesis, and has been suggested\n",
                        "some time ago by Harris (1954), among others.\n",
                        "This brings us to the deÔ¨Ånition of word embed-\n",
                        "dings we will use in this article, as suggested by\n",
                        "the literature (for instance, Turian et al. (2010);\n",
                        "Blacoe and Lapata (2012); Schnabel et al. (2015)),\n",
                        "1They claim this idea has been put forward before (Mi-\n",
                        "ikkulainen and Dyer (1991)), but not used at scale.\n",
                        "2Their roots, however, date back at least two decades, with\n",
                        "the work of Deerwester et al. (1990).\n",
                        "arXiv:1901.09069v2  [cs.CL]  2 May 2023\n"
                    ]
                }
            ],
            "source": [
                "# show the content of first document\n",
                "print(documents[0].page_content)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "8d6c8ef7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "3856\n"
                    ]
                }
            ],
            "source": [
                "print(len(documents[0].page_content))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "24be028a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Split Documents into Chunks\n",
                "\n",
                "LLMs have a **context window limit** (maximum tokens they can process at once). Additionally, for effective retrieval, we want to find *specific* relevant passages, not entire documents.\n",
                "\n",
                "We use **`RecursiveCharacterTextSplitter`** which:\n",
                "- Splits text hierarchically (paragraphs ‚Üí sentences ‚Üí words)\n",
                "- Tries to keep semantically related text together\n",
                "- Creates overlapping chunks to preserve context at boundaries\n",
                "\n",
                "### Key Parameters\n",
                "\n",
                "| Parameter | Value | Description |\n",
                "|-----------|-------|-------------|\n",
                "| `chunk_size` | 1000 | Maximum characters per chunk |\n",
                "| `chunk_overlap` | 200 | Characters shared between adjacent chunks |\n",
                "| `add_start_index` | True | Tracks the position of each chunk in the original document |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "c49cc1a5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Split 126 pages into 688 chunks\n",
                        "\n",
                        "üìù Example Chunk:\n",
                        "--------------------------------------------------\n",
                        "Word Embeddings: A Survey\n",
                        "Felipe Almeida Geraldo Xex ¬¥eo‚àó\n",
                        "Computer and Systems Engineering Program (PESC-COPPE)\n",
                        "Federal University of Rio de Janeiro\n",
                        "Rio de Janeiro, Brazil\n",
                        "{falmeida,xexeo}@cos.ufrj.br\n",
                        "Abstract\n",
                        "This work lists and describes the main re-\n",
                        "cent strategies for building Ô¨Åxed-length,\n",
                        "dense...\n",
                        "--------------------------------------------------\n",
                        "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-03T00:58:57+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-03T00:58:57+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/1901.09069v2.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': '1901.09069v2.pdf', 'start_index': 0}\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the text splitter\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,       # Max characters per chunk\n",
                "    chunk_overlap=200,     # Overlap between chunks for context continuity\n",
                "    add_start_index=True   # Track position in original document\n",
                ")\n",
                "\n",
                "# Split documents into chunks\n",
                "chunks = text_splitter.split_documents(documents)\n",
                "print(f\"‚úÖ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
                "\n",
                "# Show example chunk\n",
                "if chunks:\n",
                "    print(\"\\nüìù Example Chunk:\")\n",
                "    print(\"-\" * 50)\n",
                "    print(chunks[0].page_content[:300] + \"...\")\n",
                "    print(\"-\" * 50)\n",
                "    print(f\"Metadata: {chunks[0].metadata}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "3dec8c73",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Word Embeddings: A Survey\n",
                        "Felipe Almeida Geraldo Xex ¬¥eo‚àó\n",
                        "Computer and Systems Engineering Program (PESC-COPPE)\n",
                        "Federal University of Rio de Janeiro\n",
                        "Rio de Janeiro, Brazil\n",
                        "{falmeida,xexeo}@cos.ufrj.br\n",
                        "Abstract\n",
                        "This work lists and describes the main re-\n",
                        "cent strategies for building Ô¨Åxed-length,\n",
                        "dense and distributed representations for\n",
                        "words, based on the distributional hypoth-\n",
                        "esis. These representations are now com-\n",
                        "monly called word embeddings and, in ad-\n",
                        "dition to encoding surprisingly good syn-\n",
                        "tactic and semantic information, have been\n",
                        "proven useful as extra features in many\n",
                        "downstream NLP tasks.\n",
                        "1 Introduction\n",
                        "The task of representing words and documents is\n",
                        "part and parcel of most, if not all, Natural Lan-\n",
                        "guage Processing (NLP) tasks. In general, it has\n",
                        "been found to be useful to represent them as vec-\n",
                        "tors, which have an appealing, intuitive interpreta-\n",
                        "tion, can be the subject of useful operations (e.g.\n",
                        "addition, subtraction, distance measures, etc) and\n"
                    ]
                }
            ],
            "source": [
                "# inspect a chunk \n",
                "print(chunks[0].page_content)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "f2df3c75",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "976\n"
                    ]
                }
            ],
            "source": [
                "print(len(chunks[0].page_content))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7a04768",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 5: Create Embeddings and Vector Store\n",
                "\n",
                "### What are Embeddings?\n",
                "\n",
                "**Embeddings** are numerical representations (vectors) of text that capture semantic meaning. Texts with similar meanings will have vectors that are close together in the embedding space.\n",
                "\n",
                "### What is a Vector Store?\n",
                "\n",
                "A **Vector Store** is a specialized database optimized for:\n",
                "- Storing high-dimensional vectors\n",
                "- Performing fast similarity searches\n",
                "- Enabling \"semantic search\" (finding text by meaning, not just keywords)\n",
                "\n",
                "### Our Setup\n",
                "\n",
                "- **`OpenAIEmbeddings`**: Uses OpenAI's `text-embedding-3-small` model (fast and cost-effective)\n",
                "- **`Chroma`**: Open-source vector database that persists to disk\n",
                "\n",
                "> üí° **Tip**: The embeddings are stored locally, so subsequent runs will be faster as you won't need to re-embed documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "23b3974e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Vector store created with 688 documents\n"
                    ]
                }
            ],
            "source": [
                "# Initialize embedding model\n",
                "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
                "\n",
                "# Clean up existing database if it exists (to ensure fresh data)\n",
                "if os.path.exists(db_name):\n",
                "    import shutil\n",
                "    shutil.rmtree(db_name)\n",
                "    print(f\"üóëÔ∏è Removed existing vector store: {db_name}\")\n",
                "\n",
                "# Create vector store from document chunks\n",
                "vectorstore = Chroma.from_documents(\n",
                "    documents=chunks,\n",
                "    embedding=embeddings,\n",
                "    persist_directory=db_name\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Vector store created with {vectorstore._collection.count()} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "7adb3ee0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Document: arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "L¬¥ aszl¬¥ o Kozma and Johannes Voderholzer\n",
                        "Institut f¬® ur Informatik, Freie Universit¬® at Berlin, Germany\n",
                        "Abstract\n",
                        "Byte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\n",
                        "grammar-based text compression. It is employed in a variety of lang uage processing tasks such\n",
                        "as machine translation or large language model (LLM) pretraining, t o create a token dictionary\n",
                        "of a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\n",
                        "practical performance are not well understood.\n",
                        "In this paper we focus on the optimization problem underlying BPE: Ô¨Ån ding a pair encoding\n",
                        "that achieves optimal compression utility. We show that this problem is APX-complete, indi-\n",
                        "cating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\n",
                        "stronger form, a question recently raised by Zouhar et al. [ ZMG+23].\n",
                        "Score: 0.638746976852417\n",
                        "\n",
                        "Document: encoding (BPE), which lets us learn a vocabulary\n",
                        "that provides a good compression rate of the text.\n",
                        "3.2 Byte Pair Encoding (BPE)\n",
                        "Byte Pair Encoding (BPE) (Gage, 1994) is a sim-\n",
                        "ple data compression technique that iteratively re-\n",
                        "places the most frequent pair of bytes in a se-\n",
                        "quence with a single, unused byte. We adapt this\n",
                        "algorithm for word segmentation. Instead of merg-\n",
                        "ing frequent pairs of bytes, we merge characters or\n",
                        "character sequences.\n",
                        "Firstly, we initialize the symbol vocabulary with\n",
                        "the character vocabulary, and represent each word\n",
                        "as a sequence of characters, plus a special end-of-\n",
                        "word symbol ‚Äò ¬∑‚Äô, which allows us to restore the\n",
                        "original tokenization after translation. We itera-\n",
                        "tively count all symbol pairs and replace each oc-\n",
                        "currence of the most frequent pair (‚ÄòA‚Äô, ‚ÄòB‚Äô) with\n",
                        "a new symbol ‚ÄòAB‚Äô. Each merge operation pro-\n",
                        "duces a new symbol which represents a charac-\n",
                        "ter n-gram. Frequent character n-grams (or whole\n",
                        "words) are eventually merged into a single sym-\n",
                        "Score: 0.6799839735031128\n",
                        "\n",
                        "Document: input (we precisely deÔ¨Åne this problem ‚Äì optimal pair encoding ‚Äì later in this section).\n",
                        "The problem formulation we use closely resembles the one rec ently introduced by Zouhar et\n",
                        "al. [ ZMG+23] for the same task. This abstract setting presents a challen ging algorithm design\n",
                        "problem of independent interest and allows a clean theoreti cal analysis of BPE. Note however,\n",
                        "that we necessarily ignore some practical aspects and optim izations of BPE-variants (e.g., special\n",
                        "treatment of whitespace and punctuation or language-speci Ô¨Åc rules [RWC+19, AFT+23]).\n",
                        "An algorithm A for optimal pair encoding has approximation ratio Œ± ‚â§ 1, if the compression\n",
                        "utility of A is at least Œ± times the optimum for all inputs ( s, k). The greedy step of BPE is locally\n",
                        "optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\n",
                        "show that BPE may not achieve optimal compression (see Figur e 1).\n",
                        "Score: 0.8841273784637451\n",
                        "\n",
                        "Document: to directly optimize for, tokenization is usually solved he uristically, or formulated as a diÔ¨Äerent\n",
                        "but closely related task: data compression. Indeed, the dictionary-encoding of tokens reduces text\n",
                        "length; the amount of compression is easy to measure, and was found to be a good predictor of\n",
                        "the quality of tokenization for downstream tasks, e.g., for translation accuracy [ Gal19]. It is thus\n",
                        "natural to study tokenization with the proxy optimization g oal of compression utility .\n",
                        "Byte-Pair Encoding (BPE), introduced by Gage in 1994 [Gag94], is a commonly used heuristic for\n",
                        "tokenization. It proceeds by repeatedly identifying the most frequently occurring pair of symbols\n",
                        "and replacing all occurrences of this pair with a new symbol, thereby shortening the text. The\n",
                        "new symbols, together with the pairs they replace, are store d in a lookup-table, which allows the\n",
                        "reconstruction of the original text. In typical applicatio ns, the number of new symbols (and thus\n",
                        "Score: 0.93475341796875\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# test chroma db for similarity search \n",
                "query = \"What is Byte Pair Encoding?\"\n",
                "docs_with_scores = vectorstore.similarity_search_with_score(query)\n",
                "for doc, score in docs_with_scores:\n",
                "    print(f\"Document: {doc.page_content}\\nScore: {score}\\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "228c56ab",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 6: Build the RAG Chain\n",
                "\n",
                "Now we create the complete RAG pipeline using **LangChain Expression Language (LCEL)**. The chain consists of two main components:\n",
                "\n",
                "### 1. History-Aware Retriever\n",
                "\n",
                "This component reformulates the user's question to be **standalone** (understandable without context). \n",
                "\n",
                "**Example:**\n",
                "- Chat history: \"Tell me about SecLM\"\n",
                "- Follow-up: \"What are its main features?\"\n",
                "- Reformulated: \"What are the main features of SecLM?\"\n",
                "\n",
                "### 2. Question-Answer Chain\n",
                "\n",
                "This component:\n",
                "1. Takes the retrieved documents and the question\n",
                "2. \"Stuffs\" the documents into the prompt as context\n",
                "3. Generates a grounded answer using the LLM\n",
                "\n",
                "### The Complete Flow\n",
                "\n",
                "```\n",
                "User Question ‚Üí Contextualize ‚Üí Retrieve ‚Üí Generate Answer\n",
                "      ‚Üë              ‚Üì            ‚Üì            ‚Üì\n",
                "  Chat History    Standalone    Relevant    Final\n",
                "                   Question     Documents   Response\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "7655086e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize the LLM\n",
                "llm = ChatOpenAI(temperature=0, model_name=MODEL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "d31270eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Create a retriever from the vector store\n",
                "# k=5 means we retrieve the top 5 most relevant chunks\n",
                "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "32d1a82f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieved 5 documents\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "5"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "query = \"What is Byte Pair Encoding?\"\n",
                "\n",
                "# Just check what the retriever returns (raw documents)\n",
                "docs = retriever.invoke(query)\n",
                "print(f\"Retrieved {len(docs)} documents\")\n",
                "len(docs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "b68e83cd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "L¬¥ aszl¬¥ o Kozma and Johannes Voderholzer\n",
                        "Institut f¬® ur Informatik, Freie Universit¬® at Berlin, Germany\n",
                        "Abstract\n",
                        "Byte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\n",
                        "grammar-based text compression. It is employed in a variety of lang uage processing tasks such\n",
                        "as machine translation or large language model (LLM) pretraining, t o create a token dictionary\n",
                        "of a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\n",
                        "practical performance are not well understood.\n",
                        "In this paper we focus on the optimization problem underlying BPE: Ô¨Ån ding a pair encoding\n",
                        "that achieves optimal compression utility. We show that this problem is APX-complete, indi-\n",
                        "cating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\n",
                        "stronger form, a question recently raised by Zouhar et al. [ ZMG+23].\n"
                    ]
                }
            ],
            "source": [
                "print(docs[0].page_content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "id": "bc64f3ca",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "------------------------------------------------------------\n",
                        "=== Document 1 sourced from 2411.08671v1.pdf page 0 ===\n",
                        "=== Content of Document 1 ===\n",
                        "arXiv:2411.08671v1  [cs.DS]  13 Nov 2024\n",
                        "Theoretical Analysis of Byte-Pair Encoding\n",
                        "L¬¥ aszl¬¥ o Kozma and Johannes Voderholzer\n",
                        "Institut f¬® ur Informatik, Freie Universit¬® at Berlin, Germany\n",
                        "Abstract\n",
                        "Byte-Pair Encoding (BPE) is a widely used method for subword token ization, with origins in\n",
                        "grammar-based text compression. It is employed in a variety of lang uage processing tasks such\n",
                        "as machine translation or large language model (LLM) pretraining, t o create a token dictionary\n",
                        "of a prescribed size. Most evaluations of BPE to date are empirical, a nd the reasons for its good\n",
                        "practical performance are not well understood.\n",
                        "In this paper we focus on the optimization problem underlying BPE: Ô¨Ån ding a pair encoding\n",
                        "that achieves optimal compression utility. We show that this problem is APX-complete, indi-\n",
                        "cating that it is unlikely to admit a polynomial-time approximation scheme . This answers, in a\n",
                        "stronger form, a question recently raised by Zouhar et al. [ ZMG+23].\n",
                        "------------------------------------------------------------\n",
                        "=== Document 2 sourced from 1508.07909v5.pdf page 2 ===\n",
                        "=== Content of Document 2 ===\n",
                        "encoding (BPE), which lets us learn a vocabulary\n",
                        "that provides a good compression rate of the text.\n",
                        "3.2 Byte Pair Encoding (BPE)\n",
                        "Byte Pair Encoding (BPE) (Gage, 1994) is a sim-\n",
                        "ple data compression technique that iteratively re-\n",
                        "places the most frequent pair of bytes in a se-\n",
                        "quence with a single, unused byte. We adapt this\n",
                        "algorithm for word segmentation. Instead of merg-\n",
                        "ing frequent pairs of bytes, we merge characters or\n",
                        "character sequences.\n",
                        "Firstly, we initialize the symbol vocabulary with\n",
                        "the character vocabulary, and represent each word\n",
                        "as a sequence of characters, plus a special end-of-\n",
                        "word symbol ‚Äò ¬∑‚Äô, which allows us to restore the\n",
                        "original tokenization after translation. We itera-\n",
                        "tively count all symbol pairs and replace each oc-\n",
                        "currence of the most frequent pair (‚ÄòA‚Äô, ‚ÄòB‚Äô) with\n",
                        "a new symbol ‚ÄòAB‚Äô. Each merge operation pro-\n",
                        "duces a new symbol which represents a charac-\n",
                        "ter n-gram. Frequent character n-grams (or whole\n",
                        "words) are eventually merged into a single sym-\n",
                        "------------------------------------------------------------\n",
                        "=== Document 3 sourced from 2411.08671v1.pdf page 1 ===\n",
                        "=== Content of Document 3 ===\n",
                        "input (we precisely deÔ¨Åne this problem ‚Äì optimal pair encoding ‚Äì later in this section).\n",
                        "The problem formulation we use closely resembles the one rec ently introduced by Zouhar et\n",
                        "al. [ ZMG+23] for the same task. This abstract setting presents a challen ging algorithm design\n",
                        "problem of independent interest and allows a clean theoreti cal analysis of BPE. Note however,\n",
                        "that we necessarily ignore some practical aspects and optim izations of BPE-variants (e.g., special\n",
                        "treatment of whitespace and punctuation or language-speci Ô¨Åc rules [RWC+19, AFT+23]).\n",
                        "An algorithm A for optimal pair encoding has approximation ratio Œ± ‚â§ 1, if the compression\n",
                        "utility of A is at least Œ± times the optimum for all inputs ( s, k). The greedy step of BPE is locally\n",
                        "optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\n",
                        "show that BPE may not achieve optimal compression (see Figur e 1).\n",
                        "------------------------------------------------------------\n",
                        "=== Document 4 sourced from 2411.08671v1.pdf page 0 ===\n",
                        "=== Content of Document 4 ===\n",
                        "to directly optimize for, tokenization is usually solved he uristically, or formulated as a diÔ¨Äerent\n",
                        "but closely related task: data compression. Indeed, the dictionary-encoding of tokens reduces text\n",
                        "length; the amount of compression is easy to measure, and was found to be a good predictor of\n",
                        "the quality of tokenization for downstream tasks, e.g., for translation accuracy [ Gal19]. It is thus\n",
                        "natural to study tokenization with the proxy optimization g oal of compression utility .\n",
                        "Byte-Pair Encoding (BPE), introduced by Gage in 1994 [Gag94], is a commonly used heuristic for\n",
                        "tokenization. It proceeds by repeatedly identifying the most frequently occurring pair of symbols\n",
                        "and replacing all occurrences of this pair with a new symbol, thereby shortening the text. The\n",
                        "new symbols, together with the pairs they replace, are store d in a lookup-table, which allows the\n",
                        "reconstruction of the original text. In typical applicatio ns, the number of new symbols (and thus\n",
                        "------------------------------------------------------------\n",
                        "=== Document 5 sourced from 2411.08671v1.pdf page 1 ===\n",
                        "=== Content of Document 5 ===\n",
                        "optimal, and thus, for k = 1 it achieves optimal compression. For k > 1, however, simple examples\n",
                        "show that BPE may not achieve optimal compression (see Figur e 1).\n",
                        "As our main complexity-result, we show that optimal pair encoding is APX-complete. This means\n",
                        "(informally) that no polynomial-time algorithm can approx imate it to a factor arbitrarily close to\n",
                        "1, unless P=NP. On the positive side, we show that BPE achieve s an approximation ratio Œ± , with\n",
                        "0.333 < Œ± ‚â§ 0.625. We note that previously no constant-approximation gua rantee was known\n",
                        "for BPE or other algorithms. The question of whether optimal pair encoding is NP-complete was\n",
                        "raised recently by Zouhar et al. [ ZMG+23]; our result settles this question in a stronger form.\n",
                        "Before precisely stating our results, we review some furthe r related work and give a formal\n",
                        "deÔ¨Ånition of the problem and the algorithms that we study.\n",
                        "Related work. BPE has its origins in text compression, in particular, the c lass of grammar-based\n"
                    ]
                }
            ],
            "source": [
                "for i, doc in enumerate(docs):\n",
                "    print(f'---'*20)\n",
                "    print(f'=== Document {i+1} sourced from {doc.metadata[\"source_file\"]} page {doc.metadata[\"page\"]} ===')\n",
                "    print(f'=== Content of Document {i+1} ===')\n",
                "    print(doc.page_content)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "id": "9bde396d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "------------------------------------------------------------\n",
                        "=== Document 1 sourced from 1301.3781v3.pdf page 4 ===\n",
                        "=== Content of Document 1 ===\n",
                        "resulting vectors can be used to answer very subtle semantic relationships between words, such as\n",
                        "a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\n",
                        "with such semantic relationships could be used to improve many existing NLP applications, such\n",
                        "as machine translation, information retrieval and question answering systems, and may enable other\n",
                        "future applications yet to be invented.\n",
                        "5\n",
                        "------------------------------------------------------------\n",
                        "=== Document 2 sourced from 1301.3781v3.pdf page 4 ===\n",
                        "=== Content of Document 2 ===\n",
                        "Somewhat surprisingly, these questions can be answered by performing simple algebraic operations\n",
                        "with the vector representation of words. To Ô¨Ånd a word that is similar to small in the same sense as\n",
                        "biggest is similar to big, we can simply compute vectorX = vector(‚Äùbiggest‚Äù) ‚àívector(‚Äùbig‚Äù) +\n",
                        "vector(‚Äùsmall‚Äù). Then, we search in the vector space for the word closest toX measured by cosine\n",
                        "distance, and use it as the answer to the question (we discard the input question words during this\n",
                        "search). When the word vectors are well trained, it is possible to Ô¨Ånd the correct answer (word\n",
                        "smallest) using this method.\n",
                        "Finally, we found that when we train high dimensional word vectors on a large amount of data, the\n",
                        "resulting vectors can be used to answer very subtle semantic relationships between words, such as\n",
                        "a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\n",
                        "------------------------------------------------------------\n",
                        "=== Document 3 sourced from 1301.3781v3.pdf page 4 ===\n",
                        "=== Content of Document 3 ===\n",
                        "showing example words and their most similar words, and understand them intuitively. Although\n",
                        "it is easy to show that word France is similar to Italy and perhaps some other countries, it is much\n",
                        "more challenging when subjecting those vectors in a more complex similarity task, as follows. We\n",
                        "follow previous observation that there can be many different types of similarities between words, for\n",
                        "example, word big is similar to bigger in the same sense that small is similar to smaller. Example\n",
                        "of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\n",
                        "denote two pairs of words with the same relationship as a question, as we can ask: ‚ÄùWhat is the\n",
                        "word that is similar to small in the same sense as biggest is similar to big?‚Äù\n",
                        "Somewhat surprisingly, these questions can be answered by performing simple algebraic operations\n",
                        "with the vector representation of words. To Ô¨Ånd a word that is similar to small in the same sense as\n",
                        "------------------------------------------------------------\n",
                        "=== Document 4 sourced from 1301.3781v3.pdf page 9 ===\n",
                        "=== Content of Document 4 ===\n",
                        "Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\n",
                        "gram model trained on 783M words with 300 dimensionality).\n",
                        "Relationship Example 1 Example 2 Example 3\n",
                        "France - Paris Italy: Rome Japan: Tokyo Florida: Tallahassee\n",
                        "big - bigger small: larger cold: colder quick: quicker\n",
                        "Miami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii\n",
                        "Einstein - scientist Messi: midÔ¨Åelder Mozart: violinist Picasso: painter\n",
                        "Sarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japan\n",
                        "copper - Cu zinc: Zn gold: Au uranium: plutonium\n",
                        "Berlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack\n",
                        "Microsoft - Windows Google: Android IBM: Linux Apple: iPhone\n",
                        "Microsoft - Ballmer Google: Yahoo IBM: McNealy Apple: Jobs\n",
                        "Japan - sushi Germany: bratwurst France: tapas USA: pizza\n",
                        "assumes exact match, the results in Table 8 would score only about 60%). We believe that word\n",
                        "------------------------------------------------------------\n",
                        "=== Document 5 sourced from 1508.07909v5.pdf page 8 ===\n",
                        "=== Content of Document 5 ===\n",
                        "Beijing, China.\n",
                        "Rohan Chitnis and John DeNero. 2015. Variable-\n",
                        "Length Word Encodings for Neural Translation\n",
                        "Models. In Proceedings of the 2015 Conference on\n",
                        "Empirical Methods in Natural Language Processing\n",
                        "(EMNLP).\n"
                    ]
                }
            ],
            "source": [
                "query = \"What is the capital of France?\"\n",
                "# This query might not be in the documents, so retrieval might return irrelevant info\n",
                "docs = retriever.invoke(query)\n",
                "for i, doc in enumerate(docs):\n",
                "    print(f'---'*20)\n",
                "    print(f'=== Document {i+1} sourced from {doc.metadata[\"source_file\"]} page {doc.metadata[\"page\"]} ===')\n",
                "    print(f'=== Content of Document {i+1} ===')\n",
                "    print(doc.page_content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "id": "e196646c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "llm_basic = ChatOpenAI(model_name=MODEL, temperature=0)\n",
                "# 3. Define the contextualization prompt\n",
                "# This prompt helps reformulate questions based on chat history\n",
                "contextualize_q_system_prompt = (\n",
                "    \"Given a chat history and the latest user question \"\n",
                "    \"which might reference context in the chat history, \"\n",
                "    \"formulate a standalone question which can be understood \"\n",
                "    \"without the chat history. Do NOT answer the question, \"\n",
                "    \"just reformulate it if needed and otherwise return it as is.\"\n",
                ")\n",
                "\n",
                "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", contextualize_q_system_prompt),\n",
                "    MessagesPlaceholder(\"chat_history\"),\n",
                "    (\"human\", \"{input}\"),\n",
                "])\n",
                "\n",
                "# Create the history-aware retriever\n",
                "history_aware_retriever = create_history_aware_retriever(\n",
                "    llm, retriever, contextualize_q_prompt\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39ee7429",
            "metadata": {},
            "source": [
                "__What contextualize_q_system_prompt does__\n",
                "* This prompt is used only by create_history_aware_retriever.‚Äã\n",
                "* It tells the LLM: ‚ÄúGiven chat history + latest user input, rewrite the question so it‚Äôs standalone; don‚Äôt answer it.‚Äù That rewritten question is then sent to the retriever.‚Äã\n",
                "* You need this only if:\n",
                "    * You want follow‚Äëup questions like ‚ÄúWhat about its limitations?‚Äù to still retrieve the right chunks, and\n",
                "    * You are using create_history_aware_retriever (or an equivalent ‚Äúconversational retriever‚Äù).\n",
                "\n",
                "If you don‚Äôt care about multi‚Äëturn context in retrieval, you can skip the history‚Äëaware retriever entirely and just use retriever = vectorstore.as_retriever(...) as you did with RetrievalQA. In that case, contextualize_q_system_prompt and its prompt are not needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "id": "2d569ed2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define the QA prompt\n",
                "# This prompt instructs the LLM how to use the retrieved context\n",
                "qa_system_prompt = (\n",
                "    \"You are an assistant for question-answering tasks. \"\n",
                "    \"Use the following pieces of retrieved context to answer \"\n",
                "    \"the question. If you don't know the answer, just say that you \"\n",
                "    \"don't know. Use three sentences maximum and keep the \"\n",
                "    \"answer concise.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "qa_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", qa_system_prompt),\n",
                "    MessagesPlaceholder(\"chat_history\"),\n",
                "    (\"human\", \"{input}\"),\n",
                "])\n",
                "\n",
                "# Create the question-answer chain\n",
                "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f9bc762",
            "metadata": {},
            "source": [
                "__What qa_system_prompt does__\n",
                "* This is the system prompt used by the answer‚Äëgeneration step (create_stuff_documents_chain).‚Äã\n",
                "* It controls how the LLM:\n",
                "    * Uses {context} (retrieved docs),\n",
                "    * Handles ‚ÄúI don‚Äôt know‚Äù cases,\n",
                "    * Constrains length and style of answers\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "id": "cb04d5ce",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ RAG chain created successfully!\n"
                    ]
                }
            ],
            "source": [
                "# 5. Combine into the final RAG chain\n",
                "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
                "\n",
                "print(\"‚úÖ RAG chain created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f48e93f6",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 7: Test the RAG Chain\n",
                "\n",
                "Let's test our RAG chain with a simple query. The chain will:\n",
                "\n",
                "1. Take the user's question\n",
                "2. Retrieve relevant document chunks from the vector store\n",
                "3. Generate a response based on the retrieved context\n",
                "\n",
                "The response object contains:\n",
                "- **`answer`**: The generated response\n",
                "- **`context`**: The retrieved document chunks used to generate the answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "id": "5e931695",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ùì Question: What is the main topic of these documents?\n",
                        "\n",
                        "üí¨ Answer: The main topic of these documents is the development and evaluation of universal text embeddings, which are models designed to perform well across a variety of natural language processing tasks. They discuss the challenges of creating effective embeddings, the importance of diverse and high-quality datasets, and recent advancements in the field. Additionally, they mention benchmarks like the Massive Text Embedding Benchmark (MTEB) that assess the performance of these models across multiple languages and tasks.\n",
                        "\n",
                        "‚úÖ Chat history updated. You can now ask follow-up questions!\n"
                    ]
                }
            ],
            "source": [
                "# Initialize empty chat history\n",
                "chat_history = []\n",
                "\n",
                "# Ask a question\n",
                "query = \"What is the main topic of these documents?\"\n",
                "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
                "\n",
                "print(\"‚ùì Question:\", query)\n",
                "print(\"\\nüí¨ Answer:\", response[\"answer\"])\n",
                "\n",
                "# Update chat history for follow-up questions\n",
                "chat_history.extend([\n",
                "    HumanMessage(content=query),\n",
                "    AIMessage(content=response[\"answer\"])\n",
                "])\n",
                "\n",
                "print(\"\\n‚úÖ Chat history updated. You can now ask follow-up questions!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "id": "ad84af45",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"repeat the answer but this time in bullet points please\"\n",
                "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "id": "bbc39e1e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ùì Question: repeat the answer but this time in bullet points please\n",
                        "\n",
                        "üí¨ Answer: - The main topic is the development and evaluation of universal text embeddings for natural language processing tasks.\n",
                        "- It discusses challenges in creating effective embeddings and the importance of diverse, high-quality datasets.\n",
                        "- Recent advancements in the field and benchmarks like the Massive Text Embedding Benchmark (MTEB) are highlighted, assessing model performance across multiple languages and tasks.\n"
                    ]
                }
            ],
            "source": [
                "print(\"‚ùì Question:\", query)\n",
                "print(\"\\nüí¨ Answer:\", response[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "id": "f57a5f40",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ùì Question: What is byte pair encoding?\n",
                        "\n",
                        "üí¨ Answer: Byte Pair Encoding (BPE) is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. It is adapted for word segmentation by merging characters or character sequences instead of bytes, creating a token dictionary of a prescribed size. BPE is commonly used in natural language processing tasks, such as machine translation, to improve tokenization and compression utility.\n"
                    ]
                }
            ],
            "source": [
                "query = \"What is byte pair encoding?\"\n",
                "response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
                "print(\"‚ùì Question:\", query)\n",
                "print(\"\\nüí¨ Answer:\", response[\"answer\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32017bdc",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 8: Interactive Chat with Gradio\n",
                "\n",
                "We'll create a user-friendly chat interface using **Gradio**. This allows for:\n",
                "\n",
                "- **Continuous conversation**: The chat history is maintained across turns\n",
                "- **Shareable link**: Gradio provides a public URL (if `share=True`)\n",
                "- **Easy testing**: No code needed to interact with your RAG application\n",
                "\n",
                "### How It Works\n",
                "\n",
                "1. The `predict` function converts Gradio's chat format to LangChain's message format\n",
                "2. It invokes the RAG chain with the message and history\n",
                "3. The response is returned to the Gradio interface"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "id": "6174aa7f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "* Running on local URL:  http://127.0.0.1:7860\n",
                        "* Running on public URL: https://e327e5a06577a278ad.gradio.live\n",
                        "\n",
                        "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div><iframe src=\"https://e327e5a06577a278ad.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Keyboard interruption in main thread... closing server.\n",
                        "Killing tunnel 127.0.0.1:7860 <> https://e327e5a06577a278ad.gradio.live\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": []
                    },
                    "execution_count": 73,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def predict(message, history):\n",
                "    \"\"\"\n",
                "    Process a user message and return a response from the RAG chain.\n",
                "    \n",
                "    Args:\n",
                "        message: The user's current message\n",
                "        history: List of message dictionaries from Gradio 4.x+ (role/content format)\n",
                "    \n",
                "    Returns:\n",
                "        The AI's response string\n",
                "    \"\"\"\n",
                "    # Convert Gradio history to LangChain message format\n",
                "    # Gradio 4.x+ uses [{'role': 'user'/'assistant', 'content': '...'}] format\n",
                "    history_langchain_format = []\n",
                "    for msg in history:\n",
                "        if msg['role'] == 'user':\n",
                "            history_langchain_format.append(HumanMessage(content=msg['content']))\n",
                "        elif msg['role'] == 'assistant':\n",
                "            history_langchain_format.append(AIMessage(content=msg['content']))\n",
                "    \n",
                "    # Invoke the RAG chain\n",
                "    response = rag_chain.invoke({\n",
                "        \"input\": message, \n",
                "        \"chat_history\": history_langchain_format\n",
                "    })\n",
                "    \n",
                "    return response[\"answer\"]\n",
                "\n",
                "# Create and launch the Gradio interface\n",
                "gr.ChatInterface(\n",
                "    predict,\n",
                "    title=\"üìö RAG Chat with PDF\",\n",
                "    description=\"Ask questions about your PDF documents. The AI will use the document content to provide accurate answers.\",\n",
                "    examples=[\n",
                "        \"Summarize the documents\",\n",
                "        \"What are the key findings?\",\n",
                "        \"What is the main topic?\"\n",
                "    ],\n",
                ").launch(share=True, debug=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "db18c7d3",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: Chat with SQL Database\n",
                "\n",
                "In this section, we explore how to use LangChain to interact with a **SQL database** using natural language. This is another powerful application of LLMs: translating human questions into SQL queries.\n",
                "\n",
                "## Modern SQL Agent with LangChain v1.0+\n",
                "\n",
                "LangChain v1.0+ introduces a modernized agent architecture using **`create_agent`** from `langchain.agents`. Key features:\n",
                "\n",
                "| Feature | Description |\n",
                "|---------|-------------|\n",
                "| **Message-based interface** | Consistent API across all agent types |\n",
                "| **System prompts** | Fine-grained control over agent behavior |\n",
                "| **Built on LangGraph** | Enables persistence, human-in-the-loop, and advanced orchestration |\n",
                "| **ReAct pattern** | Iterative reasoning and acting until a solution is found |\n",
                "\n",
                "## How SQL Agents Work\n",
                "\n",
                "```\n",
                "User Question ‚Üí Agent Reasoning ‚Üí SQL Generation ‚Üí Execute Query ‚Üí Format Answer\n",
                "                     ‚Üì                  ‚Üì              ‚Üì\n",
                "              \"What tables exist?\"  \"SELECT...\"   \"8 employees\"\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sql-setup-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 9: Set Up the SQL Agent\n",
                "\n",
                "We'll use the **Chinook database**, a sample database representing a digital music store. It contains tables for:\n",
                "- Artists, Albums, Tracks\n",
                "- Customers, Employees, Invoices\n",
                "\n",
                "### Components Used\n",
                "\n",
                "- **`SQLDatabase`**: Connects to and inspects the database\n",
                "- **`SQLDatabaseToolkit`**: Provides tools for the agent to interact with SQL\n",
                "- **`create_agent`**: Creates a ReAct agent with the SQL tools\n",
                "\n",
                "> ‚ö†Ô∏è **Security Note**: SQL agents execute model-generated queries. In production, use read-only database connections and implement proper query validation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c1c1987",
            "metadata": {},
            "source": [
                "### `create_agent` + toolkit tools\n",
                "* __Purpose__: Generic agent builder that works with any tools, not just SQL; it builds a graph-based agent runtime under the hood\n",
                "* __Interface__: Message-centric. You pass `{\"messages\": [...]}` and it returns a messages list; you control the system prompt explicitly.\n",
                "* __Flexibility__: Very configurable (middleware, custom graph nodes, more complex orchestration) and is the forward-looking v1 way to build agents.\n",
                "* __SQL behavior__: You are manually giving it the SQL tools from `SQLDatabaseToolkit.get_tools()`, so the ‚ÄúSQL-ness‚Äù is just a set of generic tools plus your system prompt; there is no extra SQL-specific prompt engineering or guardrails baked in for you.‚Äã\n",
                "* __Conceptually__: use this when you want a general agent framework that happens to have SQL tools, and you are comfortable owning the prompt/spec and any extra behaviors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "id": "dd0c1f09",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìä Available tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
                    ]
                }
            ],
            "source": [
                "import requests\n",
                "from langchain_community.utilities import SQLDatabase\n",
                "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
                "from langchain.agents import create_agent\n",
                "\n",
                "# Download the Chinook sample database if it doesn't exist\n",
                "db_url = \"https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/refs/heads/master/db_samples/Chinook.db\"\n",
                "if not os.path.exists(\"Chinook.db\"):\n",
                "    print(\"üì• Downloading Chinook database...\")\n",
                "    response = requests.get(db_url)\n",
                "    if response.status_code == 200:\n",
                "        with open(\"Chinook.db\", \"wb\") as file:\n",
                "            file.write(response.content)\n",
                "        print(\"‚úÖ Chinook.db downloaded successfully!\")\n",
                "    else:\n",
                "        print(f\"‚ùå Failed to download database (status: {response.status_code})\")\n",
                "\n",
                "# Connect to the database\n",
                "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
                "print(f\"\\nüìä Available tables: {db.get_usable_table_names()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "id": "439572cd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[(1, 'For Those About To Rock We Salute You', 1), (2, 'Balls to the Wall', 2), (3, 'Restless and Wild', 2), (4, 'Let There Be Rock', 1), (5, 'Big Ones', 3), (6, 'Jagged Little Pill', 4), (7, 'Facelift', 5), (8, 'Warner 25 Anos', 6), (9, 'Plays Metallica By Four Cellos', 7), (10, 'Audioslave', 8)]\n"
                    ]
                }
            ],
            "source": [
                "result = db.run(\"SELECT * FROM ALBUM LIMIT 10\")\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "id": "68c82eac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n"
                    ]
                }
            ],
            "source": [
                "result = db.run(\"SELECT * FROM ARTIST LIMIT 10\")\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "id": "dcde7013",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ SQL Agent created successfully!\n"
                    ]
                }
            ],
            "source": [
                "# Create the SQL toolkit with tools for the agent\n",
                "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
                "\n",
                "# Define the agent's system prompt\n",
                "system_prompt = (\n",
                "    \"You are an agent designed to interact with a SQL database. \"\n",
                "    \"Given an input question, create a syntactically correct SQLite query to run, \"\n",
                "    \"then look at the results of the query and return the answer. \"\n",
                "    \"You can order the results by a relevant column to return the most interesting examples. \"\n",
                "    \"Never query for all the columns from a specific table, only ask for the relevant columns. \"\n",
                "    \"You have access to tools for interacting with the database. \"\n",
                "    \"Only use the information returned by these tools to construct your final answer. \"\n",
                "    \"If you get an error while executing a query, rewrite the query and try again.\"\n",
                ")\n",
                "\n",
                "# Create the agent using LangChain v1.0+ API\n",
                "agent = create_agent(\n",
                "    model=llm,\n",
                "    tools=toolkit.get_tools(),\n",
                "    system_prompt=system_prompt\n",
                ")\n",
                "\n",
                "print(\"‚úÖ SQL Agent created successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sql-test-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 10: Test the SQL Agent\n",
                "\n",
                "Let's test our SQL agent with some natural language queries. The agent will:\n",
                "\n",
                "1. Understand the question\n",
                "2. Explore the database schema if needed\n",
                "3. Generate and execute SQL queries\n",
                "4. Interpret results and provide a human-readable answer\n",
                "\n",
                "Notice how we use a **message-based interface** with the agent - this is the modern LangChain v1.0+ pattern."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "id": "test-sql-agent",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ùì Question: How many employees are there?\n",
                        "\n",
                        "üí¨ Answer: There are 8 employees.\n"
                    ]
                }
            ],
            "source": [
                "# Test the SQL agent with a simple question\n",
                "question = \"How many employees are there?\"\n",
                "\n",
                "print(f\"‚ùì Question: {question}\\n\")\n",
                "\n",
                "# Invoke the agent with the message-based interface\n",
                "response = agent.invoke(\n",
                "    {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
                ")\n",
                "\n",
                "# Extract the final answer from the response\n",
                "print(f\"üí¨ Answer: {response['messages'][-1].content}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sql-advanced-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 11: Try More Complex Queries\n",
                "\n",
                "The SQL agent can handle more sophisticated questions that require:\n",
                "- Joining multiple tables\n",
                "- Aggregations and grouping\n",
                "- Filtering and sorting\n",
                "\n",
                "Try some of these example queries!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "id": "advanced-sql-queries",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ùì Question: What are the top 3 genres by number of tracks?\n",
                        "\n",
                        "üí¨ Answer: The top 3 genres by number of tracks are:\n",
                        "\n",
                        "1. **Rock** - 1297 tracks\n",
                        "2. **Latin** - 579 tracks\n",
                        "3. **Metal** - 374 tracks\n"
                    ]
                }
            ],
            "source": [
                "# Example complex queries to try\n",
                "complex_queries = [\n",
                "    \"What are the top 5 best-selling artists by total sales?\",\n",
                "    \"How many tracks are in each genre?\",\n",
                "    \"Who are the customers from the USA?\",\n",
                "]\n",
                "\n",
                "# Test with the first complex query\n",
                "query = \"What are the top 3 genres by number of tracks?\"\n",
                "\n",
                "print(f\"‚ùì Question: {query}\\n\")\n",
                "\n",
                "response = agent.invoke(\n",
                "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
                ")\n",
                "\n",
                "print(f\"üí¨ Answer: {response['messages'][-1].content}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eec0f605",
            "metadata": {},
            "source": [
                "### `create_sql_agent` AgentExecutor helper \n",
                "* __Purpose__: A convenience helper that specializes an agent for SQL: it wires tools, default prompts, and an agent loop tuned for querying databases.\n",
                "* __SQL behavior__: It automatically builds the SQL toolkit (if you just pass db=) or uses your toolkit, attaches the right tools, and uses a prebuilt prompt for SQL Q&A (including instructions like only selecting relevant columns, etc.)\n",
                "* __Status__: This is ‚ÄúLangChain Classic‚Äù style but still supported; it is simpler for pure SQL use cases and is what many SQL tutorials still use\n",
                "* __Conceptually__: use this when you primarily want a database Q&A agent and prefer a one-call setup with built-in SQL prompts, easy verbosity, and AgentExecutor semantics"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae901d6e",
            "metadata": {},
            "source": [
                "### üõ†Ô∏è Understanding the SQL Components\n",
                "\n",
                "- **`SQLDatabase`**: Validates your connection string and inspects the database tables/schema automatically.\n",
                "- **`SQLDatabaseToolkit`**: A bundle of \"tools\" that the agent can use. It gives the agent the ability to: `list_tables_sql_db`, `schema_sql_db`, and `query_sql_db`.\n",
                "- **`create_sql_agent`**: A factory function that wires up the LLM, the toolkit, and a specific system prompt designed for SQL generation. It handles the \"loop\" of: Thought -> Action (Query) -> Observation (Result) -> Answer.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "id": "0f566cbf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
                        "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables  \n",
                        "Action Input: \"\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\u001b[0m\u001b[32;1m\u001b[1;3mThe \"Employee\" table seems to be the most relevant for finding the number of employees. I will check the schema of the \"Employee\" table to understand its structure.  \n",
                        "Action: sql_db_schema  \n",
                        "Action Input: \"Employee\"  \u001b[0m\u001b[33;1m\u001b[1;3m\n",
                        "CREATE TABLE \"Employee\" (\n",
                        "\t\"EmployeeId\" INTEGER NOT NULL, \n",
                        "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
                        "\t\"FirstName\" NVARCHAR(20) NOT NULL, \n",
                        "\t\"Title\" NVARCHAR(30), \n",
                        "\t\"ReportsTo\" INTEGER, \n",
                        "\t\"BirthDate\" DATETIME, \n",
                        "\t\"HireDate\" DATETIME, \n",
                        "\t\"Address\" NVARCHAR(70), \n",
                        "\t\"City\" NVARCHAR(40), \n",
                        "\t\"State\" NVARCHAR(40), \n",
                        "\t\"Country\" NVARCHAR(40), \n",
                        "\t\"PostalCode\" NVARCHAR(10), \n",
                        "\t\"Phone\" NVARCHAR(24), \n",
                        "\t\"Fax\" NVARCHAR(24), \n",
                        "\t\"Email\" NVARCHAR(60), \n",
                        "\tPRIMARY KEY (\"EmployeeId\"), \n",
                        "\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
                        ")\n",
                        "\n",
                        "/*\n",
                        "3 rows from Employee table:\n",
                        "EmployeeId\tLastName\tFirstName\tTitle\tReportsTo\tBirthDate\tHireDate\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\n",
                        "1\tAdams\tAndrew\tGeneral Manager\tNone\t1962-02-18 00:00:00\t2002-08-14 00:00:00\t11120 Jasper Ave NW\tEdmonton\tAB\tCanada\tT5K 2N1\t+1 (780) 428-9482\t+1 (780) 428-3457\tandrew@chinookcorp.com\n",
                        "2\tEdwards\tNancy\tSales Manager\t1\t1958-12-08 00:00:00\t2002-05-01 00:00:00\t825 8 Ave SW\tCalgary\tAB\tCanada\tT2P 2T3\t+1 (403) 262-3443\t+1 (403) 262-3322\tnancy@chinookcorp.com\n",
                        "3\tPeacock\tJane\tSales Support Agent\t2\t1973-08-29 00:00:00\t2002-04-01 00:00:00\t1111 6 Ave SW\tCalgary\tAB\tCanada\tT2P 5M5\t+1 (403) 262-3443\t+1 (403) 262-6712\tjane@chinookcorp.com\n",
                        "*/\u001b[0m\u001b[32;1m\u001b[1;3mTo find the number of employees, I can count the number of rows in the \"Employee\" table. I will construct a query to do that.\n",
                        "\n",
                        "Action: sql_db_query_checker  \n",
                        "Action Input: \"SELECT COUNT(*) AS EmployeeCount FROM Employee;\"  \u001b[0m\u001b[36;1m\u001b[1;3m```sql\n",
                        "SELECT COUNT(*) AS EmployeeCount FROM Employee;\n",
                        "```\u001b[0m\u001b[32;1m\u001b[1;3mThe query is correct. I will now execute it to get the number of employees.\n",
                        "\n",
                        "Action: sql_db_query  \n",
                        "Action Input: \"SELECT COUNT(*) AS EmployeeCount FROM Employee;\"  \u001b[0m\u001b[36;1m\u001b[1;3m[(8,)]\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
                        "Final Answer: There are 8 employees.\u001b[0m\n",
                        "\n",
                        "\u001b[1m> Finished chain.\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "from langchain_community.agent_toolkits import create_sql_agent\n",
                "\n",
                "agent = create_sql_agent(\n",
                "    llm=llm,\n",
                "    toolkit=toolkit,\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "result = agent.invoke({\"input\": \"How many employees are there?\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "id": "f2ae4461",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'input': 'How many employees are there?', 'output': 'There are 8 employees.'}"
                        ]
                    },
                    "execution_count": 83,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "id": "4e6e4c2d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
                        "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables  \n",
                        "Action Input: \"\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\u001b[0m\u001b[32;1m\u001b[1;3mI need to check the schema of the \"Track\" and \"Genre\" tables to understand how to count the number of tracks per genre.  \n",
                        "Action: sql_db_schema  \n",
                        "Action Input: \"Track, Genre\"  \u001b[0m\u001b[33;1m\u001b[1;3m\n",
                        "CREATE TABLE \"Genre\" (\n",
                        "\t\"GenreId\" INTEGER NOT NULL, \n",
                        "\t\"Name\" NVARCHAR(120), \n",
                        "\tPRIMARY KEY (\"GenreId\")\n",
                        ")\n",
                        "\n",
                        "/*\n",
                        "3 rows from Genre table:\n",
                        "GenreId\tName\n",
                        "1\tRock\n",
                        "2\tJazz\n",
                        "3\tMetal\n",
                        "*/\n",
                        "\n",
                        "\n",
                        "CREATE TABLE \"Track\" (\n",
                        "\t\"TrackId\" INTEGER NOT NULL, \n",
                        "\t\"Name\" NVARCHAR(200) NOT NULL, \n",
                        "\t\"AlbumId\" INTEGER, \n",
                        "\t\"MediaTypeId\" INTEGER NOT NULL, \n",
                        "\t\"GenreId\" INTEGER, \n",
                        "\t\"Composer\" NVARCHAR(220), \n",
                        "\t\"Milliseconds\" INTEGER NOT NULL, \n",
                        "\t\"Bytes\" INTEGER, \n",
                        "\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n",
                        "\tPRIMARY KEY (\"TrackId\"), \n",
                        "\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \n",
                        "\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \n",
                        "\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n",
                        ")\n",
                        "\n",
                        "/*\n",
                        "3 rows from Track table:\n",
                        "TrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n",
                        "1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n",
                        "2\tBalls to the Wall\t2\t2\t1\tNone\t342562\t5510424\t0.99\n",
                        "3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n",
                        "*/\u001b[0m\u001b[32;1m\u001b[1;3mTo find the top genres by the number of tracks, I will need to join the \"Track\" and \"Genre\" tables on the GenreId, count the number of tracks for each genre, and then order the results by the count in descending order. I will limit the results to the top 3 genres.\n",
                        "\n",
                        "Action: sql_db_query_checker  \n",
                        "Action Input: \"SELECT g.Name, COUNT(t.TrackId) AS TrackCount FROM Genre g JOIN Track t ON g.GenreId = t.GenreId GROUP BY g.GenreId ORDER BY TrackCount DESC LIMIT 3;\"  \u001b[0m\u001b[36;1m\u001b[1;3m```sql\n",
                        "SELECT g.Name, COUNT(t.TrackId) AS TrackCount \n",
                        "FROM Genre g \n",
                        "JOIN Track t ON g.GenreId = t.GenreId \n",
                        "GROUP BY g.GenreId \n",
                        "ORDER BY TrackCount DESC \n",
                        "LIMIT 3;\n",
                        "```\u001b[0m\u001b[32;1m\u001b[1;3mThe query is syntactically correct. I will now execute it to get the results.\n",
                        "\n",
                        "Action: sql_db_query  \n",
                        "Action Input: \"SELECT g.Name, COUNT(t.TrackId) AS TrackCount FROM Genre g JOIN Track t ON g.GenreId = t.GenreId GROUP BY g.GenreId ORDER BY TrackCount DESC LIMIT 3;\"  \u001b[0m\u001b[36;1m\u001b[1;3m[('Rock', 1297), ('Latin', 579), ('Metal', 374)]\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
                        "Final Answer: The top 3 genres by number of tracks are Rock (1297 tracks), Latin (579 tracks), and Metal (374 tracks).\u001b[0m\n",
                        "\n",
                        "\u001b[1m> Finished chain.\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "result = agent.invoke({\"input\": \"What are the top 3 genres by number of tracks?\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "id": "95395abc",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'input': 'What are the top 3 genres by number of tracks?',\n",
                            " 'output': 'The top 3 genres by number of tracks are Rock (1297 tracks), Latin (579 tracks), and Metal (374 tracks).'}"
                        ]
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "result"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusion-md",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Summary\n",
                "\n",
                "In this notebook, you learned how to build two powerful AI applications using **LangChain v1.0+**:\n",
                "\n",
                "## Part 1: RAG Application\n",
                "- üìÑ **Document Loading**: Load and parse PDF files\n",
                "- ‚úÇÔ∏è **Text Splitting**: Break documents into semantic chunks\n",
                "- üî¢ **Embeddings**: Convert text to vector representations\n",
                "- üóÑÔ∏è **Vector Store**: Store and search vectors with Chroma\n",
                "- üîó **RAG Chain**: Combine retrieval and generation\n",
                "- üí¨ **Chat Interface**: Build an interactive UI with Gradio\n",
                "\n",
                "## Part 2: SQL Agent\n",
                "- üóÉÔ∏è **Database Connection**: Connect to SQLite databases\n",
                "- ü§ñ **Agent Creation**: Build a ReAct agent with SQL tools\n",
                "- üîç **Natural Language Queries**: Translate questions to SQL\n",
                "\n",
                "## Key LangChain v1.0+ Features Used\n",
                "- `create_agent` for building agents (replaces older patterns)\n",
                "- Message-based interface for consistent API\n",
                "- Built on LangGraph for advanced orchestration\n",
                "\n",
                "## Next Steps\n",
                "- Try adding more documents to your RAG application\n",
                "- Experiment with different chunk sizes and overlap values\n",
                "- Connect to your own SQL databases\n",
                "- Explore LangChain's other agent types and tools"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
