{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction to Model Optimization\n",
                "\n",
                "**Module 03 | Notebook 1 of 5**\n",
                "\n",
                "This notebook introduces the key techniques for making LLMs faster, smaller, and cheaper to deploy.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Understand why optimization is essential for production\n",
                "2. Compare distillation, pruning, and quantization techniques\n",
                "3. Choose the right optimization strategy for your use case\n",
                "4. Calculate cost-benefit trade-offs\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers torch matplotlib pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModel, AutoModelForSequenceClassification\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1ï¸âƒ£ Why Optimize LLMs?\n",
                "\n",
                "### The Production Challenge\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚                     The LLM Trilemma                            â”‚\n",
                "â”‚                                                                 â”‚\n",
                "â”‚                        ACCURACY                                 â”‚\n",
                "â”‚                           â–²                                     â”‚\n",
                "â”‚                          / \\                                    â”‚\n",
                "â”‚                         /   \\                                   â”‚\n",
                "â”‚                        /     \\                                  â”‚\n",
                "â”‚              SPEED â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º COST                            â”‚\n",
                "â”‚                                                                 â”‚\n",
                "â”‚   Large models: High accuracy, slow, expensive                  â”‚\n",
                "â”‚   Small models: Fast, cheap, but less accurate                  â”‚\n",
                "â”‚                                                                 â”‚\n",
                "â”‚   ğŸ¯ Goal: Get the best of all three with optimization!        â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "### Real-World Constraints\n",
                "\n",
                "| Constraint | Impact |\n",
                "|------------|--------|\n",
                "| **Latency** | Users expect <100ms response times |\n",
                "| **Memory** | Edge devices have limited RAM |\n",
                "| **Cost** | GPU compute is expensive at scale |\n",
                "| **Energy** | Large models consume significant power |\n",
                "| **Throughput** | Need to serve thousands of requests/sec |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare model sizes\n",
                "models = [\n",
                "    (\"distilbert-base-uncased\", \"DistilBERT\"),\n",
                "    (\"bert-base-uncased\", \"BERT-base\"),\n",
                "    (\"bert-large-uncased\", \"BERT-large\"),\n",
                "]\n",
                "\n",
                "data = []\n",
                "for model_name, label in models:\n",
                "    print(f\"Loading {label}...\")\n",
                "    model = AutoModel.from_pretrained(model_name)\n",
                "    params = sum(p.numel() for p in model.parameters())\n",
                "    size_mb = params * 4 / (1024**2)  # FP32\n",
                "    data.append({\"Model\": label, \"Parameters (M)\": params/1e6, \"Size (MB)\": size_mb})\n",
                "    del model\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(\"\\nModel Comparison:\")\n",
                "print(df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the difference\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "colors = ['#2ecc71', '#3498db', '#9b59b6']\n",
                "\n",
                "axes[0].barh(df['Model'], df['Parameters (M)'], color=colors)\n",
                "axes[0].set_xlabel('Parameters (Millions)')\n",
                "axes[0].set_title('Model Size Comparison')\n",
                "\n",
                "axes[1].barh(df['Model'], df['Size (MB)'], color=colors)\n",
                "axes[1].set_xlabel('Memory (MB)')\n",
                "axes[1].set_title('Memory Footprint (FP32)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate savings\n",
                "bert_params = df[df['Model']=='BERT-base']['Parameters (M)'].values[0]\n",
                "distil_params = df[df['Model']=='DistilBERT']['Parameters (M)'].values[0]\n",
                "print(f\"\\nDistilBERT is {bert_params/distil_params:.1f}x smaller than BERT-base\")\n",
                "print(f\"That's a {(1-distil_params/bert_params)*100:.0f}% reduction!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2ï¸âƒ£ The Three Pillars of Optimization\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚                    Model Optimization Techniques                        â”‚\n",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "â”‚     DISTILLATION      â”‚       PRUNING         â”‚      QUANTIZATION       â”‚\n",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "â”‚                       â”‚                       â”‚                         â”‚\n",
                "â”‚  Teacher â†’ Student    â”‚  Remove connections   â”‚  Reduce precision       â”‚\n",
                "â”‚                       â”‚                       â”‚                         â”‚\n",
                "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â—â—â—â—â—â—  â†’  â—â—‹â—â—‹â—â—   â”‚  32-bit â†’ 8-bit         â”‚\n",
                "â”‚  â”‚ Large Model   â”‚    â”‚  â—â—â—â—â—â—     â—â—â—‹â—‹â—â—   â”‚                         â”‚\n",
                "â”‚  â”‚   (Teacher)   â”‚    â”‚  â—â—â—â—â—â—     â—‹â—â—â—â—‹â—   â”‚  1.234567890            â”‚\n",
                "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                       â”‚       â†“                 â”‚\n",
                "â”‚          â”‚ teaches    â”‚                       â”‚    1.23                 â”‚\n",
                "â”‚          â–¼            â”‚                       â”‚                         â”‚\n",
                "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                       â”‚                         â”‚\n",
                "â”‚  â”‚ Small Model   â”‚    â”‚                       â”‚                         â”‚\n",
                "â”‚  â”‚   (Student)   â”‚    â”‚                       â”‚                         â”‚\n",
                "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                       â”‚                         â”‚\n",
                "â”‚                       â”‚                       â”‚                         â”‚\n",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "â”‚ Size: 2-10x smaller   â”‚ Size: 50-90% sparse   â”‚ Size: 2-4x smaller      â”‚\n",
                "â”‚ Speed: 2-3x faster    â”‚ Speed: 1.5-3x faster  â”‚ Speed: 2-4x faster      â”‚\n",
                "â”‚ Accuracy: 95-99%      â”‚ Accuracy: 90-99%      â”‚ Accuracy: 95-99%        â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Technique Comparison\n",
                "\n",
                "| Technique | How It Works | Pros | Cons |\n",
                "|-----------|--------------|------|------|\n",
                "| **Distillation** | Train small model to mimic large model | Most accuracy retention | Requires training |\n",
                "| **Pruning** | Remove unimportant weights | No retraining needed | Sparse operations support varies |\n",
                "| **Quantization** | Reduce numerical precision | Easy to apply, big speedups | Some accuracy loss |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3ï¸âƒ£ Quick Demo: Quantization in Action"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Load a small model for demo\n",
                "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
                "\n",
                "# Model size in FP32\n",
                "params = sum(p.numel() for p in model_fp32.parameters())\n",
                "fp32_size = params * 4  # 4 bytes per float32\n",
                "\n",
                "print(f\"Model: {model_name}\")\n",
                "print(f\"Parameters: {params:,}\")\n",
                "print(f\"FP32 Size: {fp32_size / 1024**2:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply dynamic quantization (INT8)\n",
                "model_int8 = torch.quantization.quantize_dynamic(\n",
                "    model_fp32, \n",
                "    {torch.nn.Linear},  # Quantize Linear layers\n",
                "    dtype=torch.qint8\n",
                ")\n",
                "\n",
                "print(\"\\nâœ… Dynamic quantization applied!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare inference speed\n",
                "test_text = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
                "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
                "\n",
                "# Warmup\n",
                "with torch.no_grad():\n",
                "    _ = model_fp32(**inputs)\n",
                "    _ = model_int8(**inputs)\n",
                "\n",
                "# Benchmark FP32\n",
                "n_runs = 100\n",
                "start = time.time()\n",
                "for _ in range(n_runs):\n",
                "    with torch.no_grad():\n",
                "        _ = model_fp32(**inputs)\n",
                "fp32_time = (time.time() - start) / n_runs * 1000\n",
                "\n",
                "# Benchmark INT8\n",
                "start = time.time()\n",
                "for _ in range(n_runs):\n",
                "    with torch.no_grad():\n",
                "        _ = model_int8(**inputs)\n",
                "int8_time = (time.time() - start) / n_runs * 1000\n",
                "\n",
                "print(f\"Inference Latency (avg of {n_runs} runs):\")\n",
                "print(f\"  FP32: {fp32_time:.2f} ms\")\n",
                "print(f\"  INT8: {int8_time:.2f} ms\")\n",
                "print(f\"  Speedup: {fp32_time/int8_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify outputs are similar\n",
                "with torch.no_grad():\n",
                "    fp32_output = model_fp32(**inputs).logits\n",
                "    int8_output = model_int8(**inputs).logits\n",
                "\n",
                "fp32_probs = torch.softmax(fp32_output, dim=-1)\n",
                "int8_probs = torch.softmax(int8_output, dim=-1)\n",
                "\n",
                "print(\"\\nPrediction Comparison:\")\n",
                "print(f\"  FP32: POSITIVE={fp32_probs[0,1]:.4f}, NEGATIVE={fp32_probs[0,0]:.4f}\")\n",
                "print(f\"  INT8: POSITIVE={int8_probs[0,1]:.4f}, NEGATIVE={int8_probs[0,0]:.4f}\")\n",
                "print(f\"  Difference: {torch.abs(fp32_probs - int8_probs).max():.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4ï¸âƒ£ Decision Framework: Choosing the Right Technique\n",
                "\n",
                "```\n",
                "                    Which optimization to use?\n",
                "                            â”‚\n",
                "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "            â”‚                               â”‚\n",
                "    Need significant           Just need speed/\n",
                "    size reduction?            memory savings?\n",
                "            â”‚                               â”‚\n",
                "     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n",
                "     â”‚             â”‚                 â”‚             â”‚\n",
                "  Can train?  No training?      Accuracy      Speed is\n",
                "     â”‚             â”‚             critical?     priority?\n",
                "     â”‚             â”‚                 â”‚             â”‚\n",
                "     â–¼             â–¼                 â–¼             â–¼\n",
                " DISTILLATION   PRUNING       QUANTIZATION   QUANTIZATION\n",
                " (best quality) (quick)          (INT8)         (INT4)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision helper\n",
                "decision_matrix = pd.DataFrame({\n",
                "    'Technique': ['Distillation', 'Pruning', 'Quantization (INT8)', 'Quantization (INT4)'],\n",
                "    'Size Reduction': ['50-90%', '50-80%', '50%', '75%'],\n",
                "    'Speed Improvement': ['2-4x', '1.5-3x', '2-3x', '3-4x'],\n",
                "    'Accuracy Loss': ['1-5%', '1-10%', '<1%', '2-5%'],\n",
                "    'Requires Training': ['Yes', 'Optional', 'No', 'No'],\n",
                "    'Difficulty': ['High', 'Medium', 'Low', 'Low']\n",
                "})\n",
                "\n",
                "print(\"Optimization Decision Matrix:\")\n",
                "print(\"=\" * 90)\n",
                "print(decision_matrix.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5ï¸âƒ£ Cost Analysis\n",
                "\n",
                "Let's calculate the potential savings from optimization:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_savings(requests_per_day, cost_per_1k_tokens_original, optimization_speedup):\n",
                "    \"\"\"\n",
                "    Calculate monthly cost savings from optimization.\n",
                "    \"\"\"\n",
                "    tokens_per_request = 500  # Average\n",
                "    \n",
                "    # Original cost\n",
                "    daily_tokens = requests_per_day * tokens_per_request\n",
                "    daily_cost_original = (daily_tokens / 1000) * cost_per_1k_tokens_original\n",
                "    monthly_cost_original = daily_cost_original * 30\n",
                "    \n",
                "    # Optimized cost (assuming cost scales with compute)\n",
                "    monthly_cost_optimized = monthly_cost_original / optimization_speedup\n",
                "    \n",
                "    savings = monthly_cost_original - monthly_cost_optimized\n",
                "    \n",
                "    return {\n",
                "        'monthly_requests': requests_per_day * 30,\n",
                "        'original_cost': monthly_cost_original,\n",
                "        'optimized_cost': monthly_cost_optimized,\n",
                "        'monthly_savings': savings,\n",
                "        'annual_savings': savings * 12\n",
                "    }\n",
                "\n",
                "# Example scenarios\n",
                "scenarios = [\n",
                "    {\"name\": \"Small App\", \"requests\": 10000, \"cost\": 0.002, \"speedup\": 2},\n",
                "    {\"name\": \"Medium App\", \"requests\": 100000, \"cost\": 0.002, \"speedup\": 2},\n",
                "    {\"name\": \"Large App\", \"requests\": 1000000, \"cost\": 0.002, \"speedup\": 3},\n",
                "]\n",
                "\n",
                "print(\"Cost Savings Analysis:\")\n",
                "print(\"=\" * 70)\n",
                "for s in scenarios:\n",
                "    result = calculate_savings(s['requests'], s['cost'], s['speedup'])\n",
                "    print(f\"\\n{s['name']} ({s['requests']:,} requests/day, {s['speedup']}x speedup):\")\n",
                "    print(f\"  Monthly requests: {result['monthly_requests']:,}\")\n",
                "    print(f\"  Original cost: ${result['original_cost']:,.0f}/month\")\n",
                "    print(f\"  Optimized cost: ${result['optimized_cost']:,.0f}/month\")\n",
                "    print(f\"  Monthly savings: ${result['monthly_savings']:,.0f}\")\n",
                "    print(f\"  Annual savings: ${result['annual_savings']:,.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6ï¸âƒ£ Combining Techniques\n",
                "\n",
                "For maximum optimization, techniques can be combined:\n",
                "\n",
                "```\n",
                "Original Model (BERT-large, 340M params)\n",
                "            â”‚\n",
                "            â–¼ Distillation\n",
                "Distilled Model (DistilBERT, 66M params)  [5x smaller]\n",
                "            â”‚\n",
                "            â–¼ Pruning (50%)\n",
                "Pruned Model (33M effective params)       [10x smaller]\n",
                "            â”‚\n",
                "            â–¼ Quantization (INT8)\n",
                "Final Model (~16.5MB)                     [20x smaller!]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize combined optimization\n",
                "stages = ['BERT-large', 'Distilled', 'Pruned (50%)', 'Quantized (INT8)']\n",
                "sizes = [340, 66, 33, 16.5]  # Approximate sizes in millions/MB\n",
                "colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db']\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "bars = ax.bar(stages, sizes, color=colors, edgecolor='black')\n",
                "ax.set_ylabel('Size (MB / Million params)')\n",
                "ax.set_title('Combined Optimization: 20x Reduction!')\n",
                "\n",
                "# Add reduction labels\n",
                "for i, (bar, size) in enumerate(zip(bars, sizes)):\n",
                "    if i > 0:\n",
                "        reduction = (1 - size/sizes[0]) * 100\n",
                "        ax.annotate(f'-{reduction:.0f}%', \n",
                "                   xy=(bar.get_x() + bar.get_width()/2, size + 10),\n",
                "                   ha='center', fontsize=10, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ¯ Student Challenge\n",
                "\n",
                "### Challenge: Calculate Your Optimization Strategy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Given these requirements, recommend an optimization strategy:\n",
                "\n",
                "# Scenario:\n",
                "# - Current model: BERT-large (340M params)\n",
                "# - Target latency: <50ms (currently 200ms)\n",
                "# - Accuracy tolerance: Can accept up to 3% accuracy drop\n",
                "# - Training budget: No GPU hours available for training\n",
                "# - Deployment target: Cloud GPU\n",
                "\n",
                "# Questions:\n",
                "# 1. Which technique(s) would you recommend?\n",
                "# 2. What size reduction could you achieve?\n",
                "# 3. How much speedup would you expect?\n",
                "\n",
                "# Your answer:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“ Key Takeaways\n",
                "\n",
                "1. **Optimization is essential** for production deployments\n",
                "2. **Three main techniques**: Distillation, Pruning, Quantization\n",
                "3. **Trade-offs exist** between size, speed, and accuracy\n",
                "4. **Techniques can be combined** for maximum benefit\n",
                "5. **Cost savings** can be substantial at scale\n",
                "\n",
                "---\n",
                "\n",
                "## â¡ï¸ Next Steps\n",
                "\n",
                "Continue to `02_knowledge_distillation.ipynb` for hands-on distillation!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
