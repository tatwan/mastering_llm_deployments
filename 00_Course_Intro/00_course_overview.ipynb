{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mastering LLM Deployment\n",
                "\n",
                "**Welcome to the LLM Deployment Course!**\n",
                "\n",
                "This course will teach you how to deploy Large Language Models (LLMs) efficiently and cost-effectively. By the end, you'll have hands-on experience with model optimization techniques and cloud deployment.\n",
                "\n",
                "---\n",
                "\n",
                "## Course Modules\n",
                "\n",
                "| Module | Topic | What You'll Learn |\n",
                "|--------|-------|-------------------|\n",
                "| **01** | Foundations | Transformers, tokenization, model architecture |\n",
                "| **02** | Fine-Tuning | Transfer learning, sentiment analysis, summarization |\n",
                "| **03** | Optimization | Distillation, pruning, quantization, benchmarking |\n",
                "| **04** | Deployment | FastAPI, Gradio, Docker, AWS ECS |\n",
                "| **05** | Capstone | End-to-end project |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Learning Objectives\n",
                "\n",
                "By completing this course, you will be able to:\n",
                "\n",
                "1. **Load and use** pre-trained transformer models from Hugging Face\n",
                "2. **Fine-tune** models for classification and text generation tasks\n",
                "3. **Optimize** models using distillation, pruning, and quantization\n",
                "4. **Benchmark** model performance (latency, memory, accuracy)\n",
                "5. **Deploy** models using REST APIs, Gradio UIs, and Docker containers\n",
                "6. **Scale** deployments on AWS ECS with cost optimization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment Setup\n",
                "\n",
                "This course is designed to run on **Google Colab** for free GPU access.\n",
                "\n",
                "### Step 1: Enable GPU Runtime\n",
                "1. Go to **Runtime** → **Change runtime type**\n",
                "2. Select **T4 GPU** (or any available GPU)\n",
                "3. Click **Save**\n",
                "\n",
                "### Step 2: Verify GPU Access\n",
                "Run the cell below to confirm GPU is available:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.0+cu126\n",
                        "CUDA available: True\n",
                        "GPU: Tesla T4\n",
                        "GPU Memory: 15.8 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"⚠️ No GPU detected. Go to Runtime > Change runtime type > Select GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Install Dependencies\n",
                "\n",
                "Run this cell to install all required libraries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets accelerate evaluate rouge-score\n",
                "!pip install gradio fastapi uvicorn\n",
                "!pip install bitsandbytes\n",
                "\n",
                "print(\"✅ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Verify Transformers Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Transformers version: 4.57.3\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ac16f87009bc462b9339e08eb4b54e20",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e676fde7643a494394549bf1f4fbf8e2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0ca587902b2142e6adf5c03b8847397c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d671be479a144d74aa32b94903b0ce56",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "✅ Transformers library working correctly!\n"
                    ]
                }
            ],
            "source": [
                "import transformers\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "print(f\"Transformers version: {transformers.__version__}\")\n",
                "\n",
                "# Quick test: Load a small model\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
                "print(\"\\n✅ Transformers library working correctly!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Prerequisites Checklist\n",
                "\n",
                "Before starting, make sure you're comfortable with:\n",
                "\n",
                "- [ ] **Python basics**: functions, classes, list comprehensions\n",
                "- [ ] **NumPy/Pandas**: array operations, DataFrames\n",
                "- [ ] **Basic ML concepts**: training vs. inference, loss functions\n",
                "- [ ] **Neural networks**: layers, forward pass, backpropagation (conceptual)\n",
                "\n",
                "### Quick Self-Assessment\n",
                "\n",
                "Can you explain what this code does?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model architecture:\n",
                        "SimpleNet(\n",
                        "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
                        "  (relu): ReLU()\n",
                        "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "# Self-assessment: What does this code do?\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SimpleNet(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.fc1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Answer: This defines a simple 2-layer neural network with ReLU activation\n",
                "model = SimpleNet(10, 32, 2)\n",
                "print(f\"Model architecture:\\n{model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Course Navigation\n",
                "\n",
                "### Module 01: Foundations\n",
                "Start here to understand how transformers work:\n",
                "- `01_Foundations/01_transformers_basics.ipynb` - Model loading and tokenization\n",
                "- `01_Foundations/02_model_architecture.ipynb` - Understanding model internals\n",
                "\n",
                "### Module 02: Fine-Tuning\n",
                "Learn to adapt pre-trained models for specific tasks:\n",
                "- `02_Fine_Tuning/01_transfer_learning.ipynb` - Transfer learning concepts\n",
                "- `02_Fine_Tuning/02_sentiment_analysis.ipynb` - IMDB classification\n",
                "- `02_Fine_Tuning/03_summarization.ipynb` - Text summarization\n",
                "\n",
                "### Module 03: Optimization\n",
                "Make models faster and smaller:\n",
                "- `03_Model_Optimization/01_intro_to_optimization.ipynb` - Why optimize?\n",
                "- `03_Model_Optimization/02_knowledge_distillation.ipynb` - Teacher-student training\n",
                "- `03_Model_Optimization/03_pruning.ipynb` - Removing unnecessary weights\n",
                "- `03_Model_Optimization/04_quantization.ipynb` - Reducing precision\n",
                "- `03_Model_Optimization/05_benchmarking.ipynb` - Comparing techniques\n",
                "\n",
                "### Module 04: Deployment\n",
                "Put models into production:\n",
                "- `04_Deployment/01_local_serving.ipynb` - FastAPI endpoints\n",
                "- `04_Deployment/02_gradio_ui.ipynb` - Interactive demos\n",
                "- `04_Deployment/03_docker_packaging.md` - Containerization\n",
                "- `04_Deployment/04_aws_ecs_deployment.md` - Cloud deployment\n",
                "\n",
                "### Module 05: Capstone\n",
                "Bring it all together:\n",
                "- `05_Capstone/capstone_project.ipynb` - End-to-end project"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Quick Start: Your First Transformer\n",
                "\n",
                "Let's run a quick example to see transformers in action!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a344692200f548bd9164ceef870eedd4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7eeeaf55aeea4d7cb336dbce754c3f73",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "705ac989f98243f191375c828e43b527",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1540c4fbdcb140bea72c202558c00790",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "99f798dd78994991b92e4a0835772a71",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "128b46f1590f49899f8a3414ddbd856b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a08979a3e75549e7b9a57bfed068bdd4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n",
                        "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
                        "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Prompt: The future of AI is\n",
                        "\n",
                        "Generated: The future of AI is uncertain. In an interview with the Financial Times, Professor Michael Linden, the co-director of the Artificial Intelligence Laboratory at the University of California, Berkeley, pointed out that the next step is to get more people into the field.\n",
                        "\n",
                        "\"As we're building systems, we're going to have to see if there's a market for it,\" he said. \"If not, it might not be coming and we don't know how to get more people into it.\"\n",
                        "\n",
                        "Follow Stories Like This Get the Monitor stories you care about delivered to your inbox.\n",
                        "\n",
                        "Many AI researchers, however, are finding it hard to imagine a future where humans can do everything, from creating things like cars, to building robots, to making money.\n",
                        "\n",
                        "\"People are getting used to working with computers,\" said Dr. Tim S. Kappelhoff, a professor at the University of Arizona and a co-author of a new book on the subject. \"But they're beginning to get used to the idea that we can do this. We have to have the ability to understand what's going on around us, and we can build robots that can do that.\"\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Load a text generation pipeline\n",
                "generator = pipeline(\"text-generation\", model=\"gpt2\", device=0 if torch.cuda.is_available() else -1)\n",
                "\n",
                "# Generate text\n",
                "prompt = \"The future of AI is\"\n",
                "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
                "\n",
                "print(f\"Prompt: {prompt}\")\n",
                "print(f\"\\nGenerated: {result[0]['generated_text']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0ef1d6aae8534d33b82526faf88a406d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "44d7fb789703420193f2e096571ca1ba",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7586040c6f0846fe9d65df4e47760d5c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3a26619eaeeb4726a0aab6d8241bb72d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sentiment Analysis Results:\n",
                        "--------------------------------------------------\n",
                        "Text: I love this course! It's amazing!\n",
                        "  → POSITIVE (confidence: 99.99%)\n",
                        "\n",
                        "Text: This is confusing and frustrating.\n",
                        "  → NEGATIVE (confidence: 99.96%)\n",
                        "\n",
                        "Text: The weather is okay today.\n",
                        "  → POSITIVE (confidence: 99.98%)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Try sentiment analysis\n",
                "classifier = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
                "\n",
                "texts = [\n",
                "    \"I love this course! It's amazing!\",\n",
                "    \"This is confusing and frustrating.\",\n",
                "    \"The weather is okay today.\"\n",
                "]\n",
                "\n",
                "print(\"Sentiment Analysis Results:\")\n",
                "print(\"-\" * 50)\n",
                "for text in texts:\n",
                "    result = classifier(text)[0]\n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"  → {result['label']} (confidence: {result['score']:.2%})\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Ready to Start!\n",
                "\n",
                "You've successfully:\n",
                "- ✅ Set up your environment\n",
                "- ✅ Verified GPU access\n",
                "- ✅ Installed dependencies\n",
                "- ✅ Run your first transformer models\n",
                "\n",
                "**Next Step**: Open `01_Foundations/01_transformers_basics.ipynb` to begin Module 01!\n",
                "\n",
                "---\n",
                "\n",
                "## Getting Help\n",
                "\n",
                "If you encounter issues:\n",
                "1. **Runtime errors**: Restart the runtime (Runtime → Restart runtime)\n",
                "2. **Out of memory**: Use a smaller model or reduce batch size\n",
                "3. **Import errors**: Re-run the pip install cell\n",
                "4. **GPU unavailable**: Check you've enabled GPU in runtime settings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
