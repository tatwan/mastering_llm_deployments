{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83d\udd27 Transformers Basics - SOLUTIONS\n",
                "\n",
                "**Module 01 | Notebook 1 of 2**\n",
                "\n",
                "> \u26a0\ufe0f **Note**: This notebook contains solutions to the student challenges. Try to complete the challenges on your own first using `01_transformers_basics.ipynb`!\n",
                "\n",
                "In this notebook, you'll learn the fundamental building blocks of working with transformer models using the Hugging Face Transformers library.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will be able to:\n",
                "1. Load pre-trained models using `AutoModel` and `AutoTokenizer`\n",
                "2. Understand the tokenization process\n",
                "3. Perform model inference\n",
                "4. Use pipelines for common tasks\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install transformers datasets torch accelerate\n",
                "print(\"\u2705 Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
                "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding Tokenization\n",
                "\n",
                "### What is Tokenization?\n",
                "\n",
                "Tokenization is the process of converting text into smaller units (tokens) that the model can process. These tokens are then converted to numerical IDs.\n",
                "\n",
                "```\n",
                "Text: \"Hello, how are you?\"\n",
                "  \u2193 Tokenization\n",
                "Tokens: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"]\n",
                "  \u2193 Convert to IDs\n",
                "Token IDs: [7592, 1010, 2129, 2024, 2017, 1029]\n",
                "```\n",
                "\n",
                "### Why Tokenization Matters\n",
                "\n",
                "- Models can only process numbers, not text\n",
                "- Different models use different tokenization strategies\n",
                "- Token count affects memory usage and processing time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d010015d7d344cef98c5d8c62b34a855",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "30ab1b3f85e44a6aaba89e7a587606c4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "990bf7d901554d269c5d27ee576d2ca9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f14a7293fe6f4bfaa1ddbe99b3662c66",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original text: Hello, how are you doing today?\n",
                        "Tokens: ['hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
                        "Number of tokens: 8\n"
                    ]
                }
            ],
            "source": [
                "# Load a tokenizer\n",
                "model_name = \"bert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# Simple tokenization example\n",
                "text = \"Hello, how are you doing today?\"\n",
                "\n",
                "# Tokenize\n",
                "tokens = tokenizer.tokenize(text)\n",
                "print(f\"Original text: {text}\")\n",
                "print(f\"Tokens: {tokens}\")\n",
                "print(f\"Number of tokens: {len(tokens)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token IDs: [7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029]\n",
                        "Decoded tokens: ['hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
                    ]
                }
            ],
            "source": [
                "# Convert tokens to IDs\n",
                "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
                "print(f\"Token IDs: {token_ids}\")\n",
                "\n",
                "# We can also go back from IDs to tokens\n",
                "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
                "print(f\"Decoded tokens: {decoded_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Complete Tokenization Pipeline\n",
                "\n",
                "In practice, we use the tokenizer's `__call__` method which handles everything:\n",
                "\n",
                "The `__call__` method is the recommended way to tokenize text because it handles the complete tokenization pipeline in one step. When you call `tokenizer(text, return_tensors=\"pt\")`, it performs multiple operations: converting text to tokens, mapping tokens to IDs, adding special tokens (like `[CLS]` and `[SEP]`), generating the attention mask, and converting the output to PyTorch tensors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Encoded outputs:\n",
                        "  Keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
                        "  input_ids shape: torch.Size([1, 10])\n",
                        "  input_ids: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029,  102]])\n",
                        "  attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
                    ]
                }
            ],
            "source": [
                "# Complete tokenization with the __call__ method\n",
                "encoded = tokenizer(text, return_tensors=\"pt\")\n",
                "\n",
                "print(\"Encoded outputs:\")\n",
                "print(f\"  Keys: {list(encoded.keys())}\")\n",
                "print(f\"  input_ids shape: {encoded['input_ids'].shape}\")\n",
                "print(f\"  input_ids: {encoded['input_ids']}\")\n",
                "print(f\"  attention_mask: {encoded['attention_mask']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Outputs\n",
                "\n",
                "| Field | Description |\n",
                "|-------|-------------|\n",
                "| `input_ids` | Token IDs for the model |\n",
                "| `attention_mask` | 1s for real tokens, 0s for padding |\n",
                "| `token_type_ids` | Segment IDs (for sentence pairs) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token-by-token breakdown:\n",
                        "----------------------------------------\n",
                        "ID:   101 | Attention: 1 | Token: [CLS]\n",
                        "ID:  7592 | Attention: 1 | Token: hello\n",
                        "ID:  1010 | Attention: 1 | Token: ,\n",
                        "ID:  2129 | Attention: 1 | Token: how\n",
                        "ID:  2024 | Attention: 1 | Token: are\n",
                        "ID:  2017 | Attention: 1 | Token: you\n",
                        "ID:  2725 | Attention: 1 | Token: doing\n",
                        "ID:  2651 | Attention: 1 | Token: today\n",
                        "ID:  1029 | Attention: 1 | Token: ?\n",
                        "ID:   102 | Attention: 1 | Token: [SEP]\n"
                    ]
                }
            ],
            "source": [
                "# Visualize the tokenization\n",
                "print(\"Token-by-token breakdown:\")\n",
                "print(\"-\" * 40)\n",
                "for token_id, attention in zip(encoded['input_ids'][0], encoded['attention_mask'][0]):\n",
                "    token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
                "    print(f\"ID: {token_id.item():5d} | Attention: {attention.item()} | Token: {token}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Special Tokens\n",
                "\n",
                "Models use special tokens to mark the beginning/end of sequences:\n",
                "\n",
                "* __[CLS] (Classification Token)__ is automatically placed at the beginning of every input sequence and serves as an aggregate representation of the entire sequence. During training, the model learns to encode information from all tokens into this first position through bidirectional attention, making it ideal for sequence-level classification tasks where a single vector representation is needed.\u200b\n",
                "\n",
                "* __[SEP] (Separator Token)__ marks boundaries between different segments in the input. For single sentences, it appears at the end (`[CLS] sentence [SEP]`), while for sentence pairs it separates both sequences (`[CLS] sentence1 [SEP] sentence2 [SEP]`). This is essential for tasks like question answering or sentence similarity where the model needs to distinguish between multiple text segments.\u200b\n",
                "\n",
                "* __[PAD] (Padding Token)__ is used when batching sequences of different lengths to ensure uniform dimensions. Since transformer models process batches efficiently, shorter sequences are padded to match the longest sequence in the batch, and the attention mask ensures these padding tokens are ignored during computation.\u200b\n",
                "\n",
                "* __[UNK] (Unknown Token)__ represents any word or subword that doesn't exist in the model's vocabulary. When the tokenizer encounters an out-of-vocabulary term, it substitutes this token, though modern subword tokenization methods like WordPiece minimize this occurrence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Special tokens:\n",
                        "  [CLS] token: [CLS] (ID: 101)\n",
                        "  [SEP] token: [SEP] (ID: 102)\n",
                        "  [PAD] token: [PAD] (ID: 0)\n",
                        "  [UNK] token: [UNK] (ID: 100)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Special tokens:\")\n",
                "print(f\"  [CLS] token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
                "print(f\"  [SEP] token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
                "print(f\"  [PAD] token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
                "print(f\"  [UNK] token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Loading Pre-trained Models\n",
                "\n",
                "### The Auto Classes\n",
                "\n",
                "When you call AutoModel.from_pretrained(model_name), the Auto class examines the model_type property in the model's configuration file and automatically selects the appropriate specific model class (e.g., BERT, GPT-2, RoBERTa). This pattern matching makes your code model-agnostic, allowing you to easily switch between different architectures by simply changing the model name without modifying any other code.\n",
                "\n",
                "Hugging Face provides `Auto` classes that automatically detect the correct model architecture:\n",
                "\n",
                "| Class | Use Case |\n",
                "|-------|----------|\n",
                "| `AutoModel` | Base model (embeddings only) |\n",
                "| `AutoModelForSequenceClassification` | Text classification |\n",
                "| `AutoModelForSeq2SeqLM` | Sequence-to-sequence (summarization, translation) |\n",
                "| `AutoModelForCausalLM` | Text generation (GPT-style) |\n",
                "| `AutoModelForQuestionAnswering` | Extractive QA |\n",
                "\n",
                "\n",
                "* `AutoModel`: Returns the base transformer model without any task-specific head, outputting raw hidden states and embeddings useful for feature extraction or custom downstream tasks\n",
                "\n",
                "* `AutoModelForSequenceClassification`: Adds a classification head on top of the base model for tasks like sentiment analysis, spam detection, or multi-class categorization\n",
                "\n",
                "* `AutoModelForSeq2SeqLM`: Designed for encoder-decoder architectures like T5 or BART, handling sequence-to-sequence tasks such as translation, summarization, or paraphrasing\n",
                "\n",
                "* `AutoModelForCausalLM`: Used for autoregressive language models like GPT that generate text by predicting the next token, ideal for text completion and generation\n",
                "\n",
                "* `AutoModelForQuestionAnswering`: Specialized for extractive question answering tasks where the model identifies answer spans within a given context passage\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1c7ce761a88a4d3d9770e16a1cc10c6b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model type: BertModel\n",
                        "Number of parameters: 109,482,240\n"
                    ]
                }
            ],
            "source": [
                "# Load the base BERT model\n",
                "model = AutoModel.from_pretrained(model_name)\n",
                "\n",
                "print(f\"Model type: {type(model).__name__}\")\n",
                "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model configuration:\n",
                        "  Hidden size: 768\n",
                        "  Number of layers: 12\n",
                        "  Number of attention heads: 12\n",
                        "  Vocabulary size: 30522\n",
                        "  Max position embeddings: 512\n"
                    ]
                }
            ],
            "source": [
                "# Get model configuration\n",
                "config = model.config\n",
                "\n",
                "print(\"Model configuration:\")\n",
                "print(f\"  Hidden size: {config.hidden_size}\")\n",
                "print(f\"  Number of layers: {config.num_hidden_layers}\")\n",
                "print(f\"  Number of attention heads: {config.num_attention_heads}\")\n",
                "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
                "print(f\"  Max position embeddings: {config.max_position_embeddings}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model Inference\n",
                "\n",
                "Let's pass our tokenized text through the model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output keys: ['last_hidden_state', 'pooler_output']\n",
                        "Last hidden state shape: torch.Size([1, 10, 768])\n",
                        "  - Batch size: 1\n",
                        "  - Sequence length: 10\n",
                        "  - Hidden size: 768\n"
                    ]
                }
            ],
            "source": [
                "# Move model to device\n",
                "model = model.to(device)\n",
                "\n",
                "# Prepare inputs\n",
                "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
                "\n",
                "# Run inference (no gradient computation needed)\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "\n",
                "print(f\"Output keys: {list(outputs.keys())}\")\n",
                "print(f\"Last hidden state shape: {outputs.last_hidden_state.shape}\")\n",
                "print(f\"  - Batch size: {outputs.last_hidden_state.shape[0]}\")\n",
                "print(f\"  - Sequence length: {outputs.last_hidden_state.shape[1]}\")\n",
                "print(f\"  - Hidden size: {outputs.last_hidden_state.shape[2]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Output\n",
                "\n",
                "The last_hidden_state is the output of the final transformer layer and contains contextualized embeddings for every token in the input sequence. Unlike static word embeddings, these representations are context-aware, meaning the same word gets different embeddings depending on its surrounding context. The shape `[batch_size, sequence_length, hidden_size]` corresponds to `[1, 10, 768`] where 768 is BERT-base's hidden dimension.\n",
                "\n",
                "The `last_hidden_state` contains embeddings for each token:\n",
                "\n",
                "```\n",
                "Shape: [batch_size, sequence_length, hidden_size]\n",
                "       [1,          10,              768]\n",
                "```\n",
                "\n",
                "Each token is now represented as a 768-dimensional vector that captures its meaning in context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CLS embedding shape: torch.Size([768])\n",
                        "CLS embedding (first 10 values): tensor([ 0.0051, -0.0445, -0.2543, -0.1362, -0.0878, -0.4347,  0.5267,  0.4450,\n",
                        "         0.1334, -0.1693], device='cuda:0')\n"
                    ]
                }
            ],
            "source": [
                "# Extract the [CLS] token embedding (often used for classification)\n",
                "cls_embedding = outputs.last_hidden_state[0, 0, :]  # First token of first batch\n",
                "print(f\"CLS embedding shape: {cls_embedding.shape}\")\n",
                "print(f\"CLS embedding (first 10 values): {cls_embedding[:10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Task-Specific Models\n",
                "\n",
                "For specific tasks, use the appropriate model class:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "344e67afce66403c99d0657836a14034",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "194527970ee741738d1fef1a97c8e295",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a6e918c8efe54bd8a6d63a9a7150eb0b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3c1b39a219204acca737db1d39f34276",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of labels: 2\n",
                        "Label mapping: {0: 'NEGATIVE', 1: 'POSITIVE'}\n"
                    ]
                }
            ],
            "source": [
                "# Load a classification model\n",
                "classifier = AutoModelForSequenceClassification.from_pretrained(\n",
                "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                ").to(device)\n",
                "\n",
                "classifier_tokenizer = AutoTokenizer.from_pretrained(\n",
                "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
                ")\n",
                "\n",
                "print(f\"Number of labels: {classifier.config.num_labels}\")\n",
                "print(f\"Label mapping: {classifier.config.id2label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text: I absolutely love this product!\n",
                        "  \u2192 POSITIVE (99.99%)\n",
                        "\n",
                        "Text: This is the worst experience ever.\n",
                        "  \u2192 NEGATIVE (99.98%)\n",
                        "\n",
                        "Text: It's okay, nothing special.\n",
                        "  \u2192 NEGATIVE (81.90%)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run classification\n",
                "test_texts = [\n",
                "    \"I absolutely love this product!\",\n",
                "    \"This is the worst experience ever.\",\n",
                "    \"It's okay, nothing special.\"\n",
                "]\n",
                "\n",
                "for text in test_texts:\n",
                "    inputs = classifier_tokenizer(text, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = classifier(**inputs)\n",
                "    \n",
                "    # Get probabilities\n",
                "    probs = torch.softmax(outputs.logits, dim=-1)\n",
                "    predicted_class = torch.argmax(probs).item()\n",
                "    confidence = probs[0, predicted_class].item()\n",
                "    \n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"  \u2192 {classifier.config.id2label[predicted_class]} ({confidence:.2%})\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Using Pipelines\n",
                "\n",
                "For quick prototyping, use the `pipeline` API which abstracts away tokenization and post-processing:\n",
                "\n",
                "The pipeline handles three critical steps automatically: preprocessing (tokenization), model inference (forward pass), and postprocessing (converting model outputs to human-readable results). You simply pass raw text, images, or audio as input, and receive task-specific outputs. For example, `pipeline(\"sentiment-analysis\")` automatically selects a default model, loads the appropriate tokenizer, and returns sentiment labels with confidence scores.\n",
                "\n",
                "__Using Pipeline__\n",
                "You can instantiate a pipeline in two ways:\u200b\n",
                "* __By task__: `pipeline(task=\"text-generation\")` uses the default model for that task\n",
                "* __By model__: `pipeline(model=\"bert-base-uncased\")` automatically detects the task from the model's configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "150b18c0fc8648a3a07a632625cb8d5e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fbe7643781d44be4ac3ad9ab576e959e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1d6644295b6d490387dd1990430f7851",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "245cea4b79b444caa2b159bd2ab1a5f1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Pipeline Results:\n",
                        "I absolutely love this product!\n",
                        "  \u2192 POSITIVE (99.99%)\n",
                        "\n",
                        "This is the worst experience ever.\n",
                        "  \u2192 NEGATIVE (99.98%)\n",
                        "\n",
                        "It's okay, nothing special.\n",
                        "  \u2192 NEGATIVE (81.90%)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Sentiment analysis pipeline\n",
                "sentiment_pipeline = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "results = sentiment_pipeline(test_texts)\n",
                "\n",
                "print(\"Pipeline Results:\")\n",
                "for text, result in zip(test_texts, results):\n",
                "    print(f\"{text}\")\n",
                "    print(f\"  \u2192 {result['label']} ({result['score']:.2%})\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a5e3fea1e4594249a00f8d798985b1e4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "64ac0907f51d4e0d8e0cc7420374cf97",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a911368b4268429cafde187f90a01bf7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4b54d5bd306c449a8f8f2899bd8c97ab",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0685dd6bce344ffdb16260b606a3ca8e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e731da8e9acb49d781a14b6972e842e5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original length: 96 words\n",
                        "Summary: The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest that covers most of the Amazon basin of South America. This region includes territory belonging to nine nations and 3,344 formally acknowledged indigenous territories.\n"
                    ]
                }
            ],
            "source": [
                "# Summarization pipeline\n",
                "summarizer = pipeline(\n",
                "    \"summarization\",\n",
                "    model=\"facebook/bart-large-cnn\",\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "long_text = \"\"\"\n",
                "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
                "in the Amazon biome that covers most of the Amazon basin of South America. This basin \n",
                "encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) \n",
                "are covered by the rainforest. This region includes territory belonging to nine nations \n",
                "and 3,344 formally acknowledged indigenous territories. The majority of the forest is \n",
                "contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia \n",
                "with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, \n",
                "and Venezuela.\n",
                "\"\"\"\n",
                "\n",
                "summary = summarizer(long_text, max_length=50, min_length=20, do_sample=False)\n",
                "print(f\"Original length: {len(long_text.split())} words\")\n",
                "print(f\"Summary: {summary[0]['summary_text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Available Pipelines\n",
                "\n",
                "| Pipeline | Task |\n",
                "|----------|------|\n",
                "| `text-classification` | Sentiment, topic classification |\n",
                "| `token-classification` | NER, POS tagging |\n",
                "| `question-answering` | Extractive QA |\n",
                "| `summarization` | Text summarization |\n",
                "| `translation` | Machine translation |\n",
                "| `text-generation` | GPT-style generation |\n",
                "| `fill-mask` | Masked language modeling |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Batch Processing\n",
                "\n",
                "For efficiency, process multiple inputs at once:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Batch shape: torch.Size([3, 17])\n",
                        "\n",
                        "Input IDs:\n",
                        "tensor([[  101,  2460,  3793,  1012,   102,     0,     0,     0,     0,     0,\n",
                        "             0,     0,     0,     0,     0,     0,     0],\n",
                        "        [  101,  2023,  2003,  1037,  5396,  3091,  6251,  1012,   102,     0,\n",
                        "             0,     0,     0,     0,     0,     0,     0],\n",
                        "        [  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  3397,  2116,\n",
                        "          2062,  2616,  1998, 19204,  2015,  1012,   102]])\n",
                        "\n",
                        "Attention Mask (0 = padding):\n",
                        "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
                    ]
                }
            ],
            "source": [
                "# Batch tokenization with padding\n",
                "texts = [\n",
                "    \"Short text.\",\n",
                "    \"This is a medium length sentence.\",\n",
                "    \"This is a much longer sentence that contains many more words and tokens.\"\n",
                "]\n",
                "\n",
                "# Tokenize with padding\n",
                "batch_encoded = tokenizer(\n",
                "    texts,\n",
                "    padding=True,           # Pad to longest in batch\n",
                "    truncation=True,        # Truncate if too long\n",
                "    max_length=32,          # Maximum length\n",
                "    return_tensors=\"pt\"     # Return PyTorch tensors\n",
                ")\n",
                "\n",
                "print(f\"Batch shape: {batch_encoded['input_ids'].shape}\")\n",
                "print(f\"\\nInput IDs:\")\n",
                "print(batch_encoded['input_ids'])\n",
                "print(f\"\\nAttention Mask (0 = padding):\")\n",
                "print(batch_encoded['attention_mask'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udfaf Student Challenge\n",
                "\n",
                "Now it's your turn! Complete the following exercises:\n",
                "\n",
                "### Challenge 1: Compare Tokenizers\n",
                "Load tokenizers for `bert-base-uncased` and `gpt2`, then compare how they tokenize the same sentence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Solution: Compare Tokenizers\n",
                "\n",
                "# 1. Load both tokenizers\n",
                "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "# 2. Tokenize the test sentence\n",
                "test_sentence = \"The transformer architecture revolutionized natural language processing.\"\n",
                "\n",
                "bert_tokens = bert_tokenizer.tokenize(test_sentence)\n",
                "gpt2_tokens = gpt2_tokenizer.tokenize(test_sentence)\n",
                "\n",
                "# 3. Print the tokens and token counts for each\n",
                "print(\"=\" * 60)\n",
                "print(\"BERT Tokenizer (WordPiece)\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Tokens: {bert_tokens}\")\n",
                "print(f\"Token count: {len(bert_tokens)}\")\n",
                "print()\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"GPT-2 Tokenizer (BPE)\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Tokens: {gpt2_tokens}\")\n",
                "print(f\"Token count: {len(gpt2_tokens)}\")\n",
                "print()\n",
                "\n",
                "# Bonus: Show the difference in tokenization strategies\n",
                "print(\"=\" * 60)\n",
                "print(\"Key Observations\")\n",
                "print(\"=\" * 60)\n",
                "print(\"\u2022 BERT uses WordPiece tokenization (## prefix for subwords)\")\n",
                "print(\"\u2022 GPT-2 uses Byte-Pair Encoding (\u0120 prefix for space before token)\")\n",
                "print(f\"\u2022 BERT produced {len(bert_tokens)} tokens\")\n",
                "print(f\"\u2022 GPT-2 produced {len(gpt2_tokens)} tokens\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Challenge 2: Model Size Comparison\n",
                "Load `distilbert-base-uncased` and `bert-base-uncased`, then compare their parameter counts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Solution: Model Size Comparison\n",
                "\n",
                "# 1. Load both models\n",
                "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
                "distilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
                "\n",
                "# 2. Count parameters for each\n",
                "bert_params = sum(p.numel() for p in bert_model.parameters())\n",
                "distilbert_params = sum(p.numel() for p in distilbert_model.parameters())\n",
                "\n",
                "# 3. Calculate the size reduction percentage\n",
                "reduction = (bert_params - distilbert_params) / bert_params * 100\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"Model Size Comparison\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"BERT-base parameters:       {bert_params:>15,}\")\n",
                "print(f\"DistilBERT parameters:      {distilbert_params:>15,}\")\n",
                "print(f\"Parameter reduction:        {reduction:>14.1f}%\")\n",
                "print()\n",
                "\n",
                "# Bonus: Compare model configurations\n",
                "print(\"=\" * 60)\n",
                "print(\"Model Architecture Comparison\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"{'Attribute':<25} {'BERT':>12} {'DistilBERT':>12}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'Hidden Size':<25} {bert_model.config.hidden_size:>12} {distilbert_model.config.hidden_size:>12}\")\n",
                "print(f\"{'Hidden Layers':<25} {bert_model.config.num_hidden_layers:>12} {distilbert_model.config.num_hidden_layers:>12}\")\n",
                "print(f\"{'Attention Heads':<25} {bert_model.config.num_attention_heads:>12} {distilbert_model.config.num_attention_heads:>12}\")\n",
                "print()\n",
                "print(\"Key Insight: DistilBERT has half the layers (6 vs 12) while\")\n",
                "print(\"retaining 97% of BERT's language understanding capabilities.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **Tokenization** converts text to numerical IDs that models can process\n",
                "2. **Auto classes** automatically detect the right model architecture\n",
                "3. **Task-specific models** add appropriate heads for classification, generation, etc.\n",
                "4. **Pipelines** provide a high-level API for quick prototyping\n",
                "5. **Batch processing** with padding improves efficiency\n",
                "\n",
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "Continue to `02_model_architecture.ipynb` to learn about:\n",
                "- Encoder vs. Decoder architectures\n",
                "- Attention mechanism visualization\n",
                "- Memory and compute requirements"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}